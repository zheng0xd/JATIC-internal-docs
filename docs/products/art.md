# Adversarial Robustness Toolbox (ART)

Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security.

ART provides tools that enable developers and researchers to defend and evaluate Machine Learning models and
applications against the adversarial threats of Evasion, Poisoning, Extraction, and Inference. ART supports all popular
machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.),
all data types (images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, speech
recognition, generation, certification, etc.). ART has a JATIC-customized extension library called HEART (Hardened
Extension Adversarial Robustness Toolbox) that builds on the Adversarial Robustness algorithms in ART.

- [ART github](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
- [ART documentation](https://adversarial-robustness-toolbox.readthedocs.io/)

```python
pip install adversarial-robustness-toolbox
```
## Contributing and Feedback
End-user experience and internal testing are valuable input for optimization of this tool.
- If you have found a bug or have an idea for an improvement, please create a GitLab issue on the (internal) HEART repo. This
allows the discussion to be captured in an easy-to-find location, refer to lines of code easily, etc.
- If you have a question or want to discuss the tool, please reach out in via http://lfai-art.slack.com/ .
