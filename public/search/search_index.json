{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"org-process/","title":"Organization and Processes","text":"<p>This repo will be used to store living documents pertaining to the organization and processes within the JATIC program. </p> <p>For non-markdown documents (e.g., .pptx and .xlsx files), PDFs are created for easier viewing in gitlab.</p> <p>POC: Jason Gaulin @jason.c.gaulin.civ</p> <p>Deputy POC: David Jin @djin</p>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/","title":"Agile Roles and Responsibilities","text":""},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#purpose","title":"Purpose","text":"<p>This document describes the key roles and their responsibilities within the Agile scrum methodology.</p> <p>Details about agile ceremonies and meetings can be found in <code>Calendar</code>.</p> <p>All of the blocks are quotes from the reference below.</p>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#roles","title":"Roles","text":""},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#development-team-member","title":"Development team member","text":"<p>The development team can be comprised of all kinds of people including designers, writers, programmers, etc.  </p> <p>You can think of it in the same way as when you have a house project and you hire a developer. They develop the project and do the work. Yes, this might mean they lay bricks, do plumbing, even dig holes, but the person is known as a developer. So, that means the \u2018developer\u2019 role in scrum means a team member who has the right skills, as part of the team to do the work.</p> <p>The development team should be able to self-organize so they can make decisions to get work done. Think of a development team as similar to a production support team that is called in during the night because something has gone wrong. The development team, like the production support team, can make decisions and deliver the fix/value for the problem at hand. Self-organization isn\u2019t about disrespecting the organization, but rather about empowering the people closest to the work to do what\u2019s needed to solve the problem.   </p> <p>The development team\u2019s responsibilities include:</p> <ul> <li>Delivering the work through the sprint.</li> <li>To ensure transparency during the sprint they meet at standup, which provides a dedicated place for team members to seek help, talk about success and highlight issues and blockers. The scrum master might facilitate the daily scrum, but ultimately it is the responsibility of the development team to run this meeting. It is their meeting to help them, as a group, to inspect and adapt the work they are doing and work in a more effective way.</li> </ul>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#development-team-lead","title":"Development team lead","text":"<p>The development team lead is not a traditional agile role, but is relevant in the context when we have product owners from the government and development teams from industry. In this case, the development team lead is the techncial leader and main POC of the industry team.</p> <p>Responsibilities of the development team lead include:  - Technical leadership of the development team and guidance to the government product owner - Close coordination with the government product owner to achieve shared product vision  - Communicating programmtic information and shared vision with the development team to ensure situational awareness - Day-to-day management of development team</p>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#product-owner","title":"Product owner","text":"<p>Scrum product owners understand the customer and business requirements, then create and manage the product backlog based on those requirements. Since agile teams are, by design, flexible and responsive, it is the responsibility of the product owner to ensure that they are delivering the most value. The business is represented by the product owner who tells the development team what is important to deliver. Trust between these two roles is crucial.</p> <p>The product owner should not only understand the customer but also have a vision for the value the scrum team is delivering to the customer. The product owner also balances the needs of other stakeholders in the organization.  </p> <p>So the product owner must take all these inputs and prioritize the work. This is probably their most important responsibility because conflicting priorities and unclear directions will not only reduce the effectiveness of the team but also could break the important trust relationship that the business has with the development team.</p> <p>The Scrum Guide defines the product owner's responsibilities as:</p> <ul> <li>Managing the scrum backlog - This does not mean that they are the only one putting in new product backlog Items into the backlog. But ultimately they are responsible for the backlog that the development team pulls to deliver from. That means the product owner should know about everything that is in the backlog and other people that add items to the product backlog should ensure that they communicate with the product owner. </li> <li>Release management - The sprint is not a release cycle, but instead a planning cycle. That means that scrum teams can deliver at any time. Ideally, they would deliver frequently throughout the sprint allowing the sprint review to review real customer usage and feedback. However continuous delivery is not always possible and other release models are required. It is important for the product owner to know when things can and should be released.</li> <li>Stakeholder management - Any product will have many stakeholders involved ranging from users, customers, governance and organizational leadership. The product owner will have to work with all these people to effectively ensure that the development team is delivering value. That can mean a large amount of stakeholder management and communication.</li> </ul> <p>It is ultimately the product owner's responsibility to have a clear vision for the product and manage its delivery of value to customers. This vision should be developed alongside the development team lead, building a shared vision for the product.</p> <p>Some emphasized or additional product owner responsibilities within our context:</p> <ul> <li>Roadmap creation: In addition to managing the scrum backlog for the short term, the product owner is also responsible for creating epics, together constituting a roadmap, for the longer term development of the product. This roadmap and long term vision for the product must be clearly communicated with the product team. </li> <li>Stakeholder coordination: The DOD has a huge number of stakeholders, many who are working in adjacent or overlapping spaces. In addition to stakeholder management to set priorities, the product owner must actively engage, coordinate, and collaborate with others working in the space to achieve adoption of our products.</li> <li>Product owners should talk with their product team and with the program management team if they would like help in discovering or pursuing external engagements.</li> <li>Programmatic coordination: Since the JATIC program has multiple loosely coupled components, product owner is responsible for coordinating with the program manager and other product owners and managing the backlog to align their development to the broader vision. </li> </ul> <p>There are a few specific requirements for Product Owners necessary for appropriate execution of their actions. These are specific to the role within JATIC:</p> <p>Day-to-day tasks: - Perform backlog grooming to prioritize items in the backlog. - Work with the team in refining Issue tickets. - Verify and ensure satisfaction of program requirements by the team.</p> <p>Weekly/Bi-weekly tasks: - Participate in sprint planning meetings with the team, representing the user and program.  - Participate in sprint review and retrospective meetings with the team, representing the user and program. - Participate in weekly program-wide Product Owner Sync, representing the team.</p> <p>Quarterly tasks: - Participate in program increment planning sessions and help set objectives for the team.   - Give the business value rating of team increment objectives (with discussion from team), both up-front during the increment planning as well as in review after the increment.</p>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#technical-sme","title":"Technical SME","text":"<p>The Technical SME is not a traditional agile role, but serves a unique and extremely important role within our program's context. For a given product, the Technical SME has a deep understanding of both the technical subject matter and the mission specific context in order to advise the product owner and development team. The role includes a variety of activities, including aiding in development and integration of the product, supporting deployment to DoD environments, and acting as the deputy product owner when necessary.</p>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#scrum-master","title":"Scrum Master","text":"<p>The scrum master is the role responsible for gluing everything together and ensuring that scrum is being done well. The scrum master is a servant leader which not only describes a supportive style of leadership but describes what they do on a day-to-day basis.  They serve the product owner by helping them better understand and communicate value, to manage the backlog, help them plan the work with the team and break down that work to deliver the most effective learning. Serving the development team, the scrum master helps them self-organize, focus on outcomes, get to a \u201cdone increment,\u201d and manage blockers. The scrum master also serves the organization at large, helping them understand what scrum is and create an environment that supports scrum.</p> <p>The scrum master focuses on:</p> <ul> <li>Transparency - To effectively inspect and adapt it is important that the right people can see what is going on. But this is actually much harder than it looks. The scrum master is tasked with ensuring that the scrum team works in a transparent way. Examples include creating story maps and updating Confluence pages with retrospective ideas.</li> <li>Empiricism - A fundamental for scrum and agile approaches the idea that the best way of planning is to do work and learn from it. The empirical process is not easy and requires the scrum master to coach the scrum team on breaking down work, describing clear outcomes, and reviewing those outcomes.</li> <li>Self-organization - Telling a development team they can self-organize does mean that the team will self-organize. In fact, self-organization comes over time and requires help and support. The scrum master will encourage team members to step outside their comfort zone and try different things and use practices such as \u2018delegation poker\u2019 to expose and challenge predefined ideas about role boundaries and responsibilities.</li> <li>Values - Scrum defines 5 values of courage, focus, commitment, respect, and openness not because they are nice to have, but because they create an environment of physiological safety and trust. Following the values is the responsibility of everyone in the scrum team, but the scrum master takes an active role in encouraging and reminding everyone of the importance of those values.</li> </ul>"},{"location":"org-process/Agile%20Roles%20and%20Responsibilities/#references","title":"References","text":"<ol> <li>https://www.atlassian.com/agile/scrum/roles</li> </ol>"},{"location":"org-process/Gitlab%20Organization/","title":"Gitlab Organization","text":""},{"location":"org-process/Gitlab%20Organization/#purpose","title":"Purpose","text":"<p>This document specifies the usage of Gitlab concepts for organization.</p>"},{"location":"org-process/Gitlab%20Organization/#primer","title":"Primer","text":"<p>For those who are unfamiliar with gitlab, we highly recommend Gitlab Project Management and Comparing Gitlab and Bitbucket terms for background on groups, projects, issues, epics, milestones, and iterations.</p>"},{"location":"org-process/Gitlab%20Organization/#groups-subgroups-and-projects","title":"Groups, subgroups, and projects","text":"<p>The government team and FFRDC support are direct members of the top-level JATIC group. The subgroups of the JATIC group correspond to the different organizations who are working on the program. For example:</p> <ul> <li>CDAO - the CDAO Gov team, as well as FFRDC support are direct members.</li> <li>Kitware - the Kitware team are direct members.</li> <li>TwoSix - the TwoSix team are direct members.</li> </ul> <p>Direct members in the top-level JATIC group will inherit permissions on all subgroups and projects within the subgroups. In general, they will have the developer role in all repositories within the namespace. Teams will be have the ability to set roles within their subgroup, which will be inherited by all projects within the group.</p> <p>Projects will generally live within the subgroup corresponding to the organization who is leading development. Some projects, which are large and cross-organizational, may live in the top-level JATIC group.</p> <p>By default, it is recommended that groups, subgroups, and the projects within are set to the \"internal\" visibility, meaning that they can be discovered and cloned by any signed in users. All groups are able to create \"private\" visibility repos as well if needed.</p>"},{"location":"org-process/Gitlab%20Organization/#gitlab-project-points-of-contact-poc-and-roles","title":"Gitlab Project Points of Contact (POC) and Roles","text":"<p>Each project is responsible for managing roles within the project with respect to management of the Gitlab repository. The projects should set appropriate roles for project members to fill the following needs. Each of the below role also can fulfil the tasks of any subsequent role. E.g., the POC should also be maintainers on the repo and can perform mantainer tasks (fyi, maintainer is a user role defined in/by Gitlab). Maintainers also can do development, but cannot approve their own MR. </p> <ol> <li>POC: A role for responding to questions regarding to the repository. The POCs should be noted in the README.md of the repository. The POC is essentially a lead maintainer of the repo. All POCs should be assigned maintainer (or owner) roles of the repo in their repo's user rules on Github. Each project/repo should have either one POC and one deputy POC or two POCs assigned.</li> <li>Maintainer: A role responsible for managing the repository. Approval from a maintainer is required for merge requests. Reviews can be largely delegated, but a maintainer is responsible for every change to the protected branches of the repository (e.g., main and dev). E.g., the main review might be done by a developer assigned by a maintainer and the maintainer then check the review and MR before approving. Maintainer are responsible for ensuring that the appropriate maintainer is providing approval of the merge request (there will be multiple maintainers for a repo), that appropriate reviews (by appropriate reviewers) take place, and that appropriate processes have been followed in the merge request. Maintainers should all have the \"maintainer\" user role defined in Gitlab.</li> <li>Developer: A role for doing development and reviews. Developers can create issues, update issues, create branches, create merge requests, and provide feedback/review on merge requests.  </li> </ol> <p>Other roles that exist in Gitlab are \"reporter\", \"guest\", and \"minimal access\". These roles are largely unused, in favor of the above. </p> <p>Upon creation of a new Gitlab Project, the above roles should be assigned for the project. Roles should be maintained.</p> <p>Details on access by role can be found here https://gitlab.jatic.net/help/user/permissions</p>"},{"location":"org-process/Gitlab%20Organization/#checking-roles","title":"Checking Roles","text":"<p>POC and DPOC should be noted in the README.md in the top level of the repo. Maintainers and developer roles can be found under Project Information -&gt; Members.</p>"},{"location":"org-process/Gitlab%20Organization/#example","title":"Example","text":""},{"location":"org-process/Gitlab%20Organization/#example-project","title":"Example project","text":"<p>...</p> <p>...</p> <p>POC: David Jin @djin </p> <p>DPOC: Ari Kapusta @akapusta</p>"},{"location":"org-process/Gitlab%20for%20Agile/","title":"Gitlab for Agile","text":""},{"location":"org-process/Gitlab%20for%20Agile/#purpose","title":"Purpose","text":"<p>This document specifies the usage of Gitlab concepts, such as labels, issues, and epics, for our Agile software development processes.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#primer","title":"Primer","text":"<p>For those who are unfamiliar with gitlab, we highly recommend Gitlab Project Management and Comparing Gitlab and Bitbucket terms for background on groups, projects, issues, epics, milestones, and iterations.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#labels","title":"Labels","text":"<p>Labels are tags which can be attached to issues, epics, and merge requests. The labels with <code>::</code> are \"scoped labels\", meaning that only one with the corresponding key may be added to a given issue.</p> <p>At the <code>jatic</code> group level, labels the following labels currently exist:</p> <ul> <li>a labels for each organization involved in work</li> <li><code>epic::program</code>, for program level epics. See Epics</li> <li><code>epic::feature</code>, for feature level epics. See Epics</li> <li><code>status::grooming</code>, for work items that are of near-term interest, but need to be decomposed or groomed before being added to an iteration</li> <li><code>status::ready to start</code>, for work items that are fully refined, prioritized, and ready to be assigned</li> <li><code>status::in progress</code>, for work items that are currently being worked on</li> <li><code>status::in review</code>, for work items that have opened a merge request currently in review</li> </ul> <p>Projects and groups may add labels to best suit their needs. </p>"},{"location":"org-process/Gitlab%20for%20Agile/#helpful-gitlab-usage-tips","title":"Helpful Gitlab usage tips","text":""},{"location":"org-process/Gitlab%20for%20Agile/#viewing-issues","title":"Viewing issues","text":"<p>Issue boards are generally recommended for viewing issues. </p> <p>Each project and group will have a list of issues. A group's issues will show all of the issues contained within its subgroups and subprojects. Thus, boards at higher level groups will often contain a large number of issues from a large number of disparate projects. For example, see the JATIC board.</p> <p>Therefore, if one is only interested in issues related to a specific project or group, viewing issues at that project or group level is recommended for more precision. For example, compare the SDP board with the JATIC board above. </p> <p>Labels are tags which can be attached to issues, epics, and merge requests. The labels with <code>::</code> are \"scoped labels\", meaning that only one with the corresponding key may be added to a given issue.</p> <p>At the <code>jatic</code> group level, labels the following labels currently exist:</p> <ul> <li>a labels for each organization involved in work</li> <li><code>epic::program</code>, for program level epics. See Epics</li> <li><code>epic::feature</code>, for feature level epics. See Epics</li> <li><code>status::grooming</code>, for work items that are of near-term interest, but need to be decomposed or groomed before being added to an iteration</li> <li><code>status::ready to start</code>, for work items that are fully refined, prioritized, and ready to be assigned</li> <li><code>status::in progress</code>, for work items that are currently being worked on</li> <li><code>status::in review</code>, for work items that have opened a merge request currently in review</li> </ul> <p>Projects and groups may add labels to best suit their needs. </p>"},{"location":"org-process/Gitlab%20for%20Agile/#helpful-gitlab-usage-tips_1","title":"Helpful Gitlab usage tips","text":""},{"location":"org-process/Gitlab%20for%20Agile/#viewing-issues_1","title":"Viewing issues","text":"<p>Issue boards are generally recommended for viewing issues. </p> <p>Each project and group will have a list of issues. A group's issues will show all of the issues contained within its subgroups and subprojects. Thus, boards at higher level groups will often contain a large number of issues from a large number of disparate projects. For example, see the JATIC board.</p> <p>Therefore, if one is only interested in issues related to a specific project or group, viewing issues at that project or group level is recommended for more precision. For example, compare the SDP board with the JATIC board above. </p>"},{"location":"org-process/Gitlab%20for%20Agile/#viewing-epics","title":"Viewing epics","text":"<p>Epics can be viewed with epic boards, as well as on the roadmap. For example, see the JATIC epic board and JATIC roadmap.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#issues","title":"Issues","text":"<p>Issues should be populated with a description of the problem statement, the desired behavior, and the definition of done. </p> <p>Generally, the product owner should create these, sometimes with assistance from the product technical SME. The work to be completed from a given issue should be understandable on its own. </p> <p>Issues will go through multiple stages of refinement, from being added to the backlog as a work item, to refined during backlog refinement, to work-in-progress during a sprint, to completion. As an issue moves through these stages, the information which is provided with the issue should also mature.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#adding-issues","title":"Adding issues","text":"<p>Any one may add issues to the product backlog which correspond to work items, desired improvements / features, etc. The product owner is responsible for both adding issues and prioritizing all of the backlog issues.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#refining-issues","title":"Refining issues","text":"<p>During backlog refinement (aka backlog grooming), the product owner, along with technical SMEs, will refine work items in the backlog, adding clarity to issues, resolving dependencies, and setting priorities. Once an issue is refined, the following properties should exist for an issue:</p> <ol> <li>A populated description, with a problem statement, desired behavior, and definition of done (potentially as subtasks)</li> <li>An associated epic / larger effort that the issue falls into (if applicable)</li> <li>A weight, according to its number of story points, preferrably using Agile Planning Poker</li> <li>The <code>status::ready to start</code> label should be added to the issue</li> </ol> <p>If an issue requires one to open a merge request, this should be noted. If an issue will require technical review it should be noted as a requirement within the issue.  In this case, it is recommended that appropriate reviewers / approvers for the MR be explicitly listed in the issue.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#issue-weights","title":"Issue weights","text":"<p>Weights are standardized across all teams and follow the Fibonacci sequence. Story points correspond to the complexity of a task, amount of work, and risk or uncertainty. However, by popular demand, the below is a mapping to the approximate hours of each.</p> <p>Weight / Approximate hours - 1 / 0-4 hours - 2 / 4-8 hours - 3 / 8-16 hours - 5 / 16-32 hours - 8 / 32-64 hours</p>"},{"location":"org-process/Gitlab%20for%20Agile/#committing-to-an-issue-for-an-iteration","title":"Committing to an issue for an iteration","text":"<p>During sprint planning, the team will commit to issues that they will complete during a sprint. Once an issue is committed to, the following properties should exist for an issue:</p> <ol> <li>The current iteration for which planning is occurring</li> <li>Assignment to a person or multiple people who will work on the task</li> </ol>"},{"location":"org-process/Gitlab%20for%20Agile/#issue-in-progress","title":"Issue in progress","text":"<p>During a sprint, when an issue is being actively worked, the assignee should add the <code>status::in progress</code> label to the issue.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#merge-requests","title":"Merge requests","text":"<p>Many issues will create changes or artifacts which the assignee must incorporate into a larger project. To incorporate changes from a source branch to a target branch, you use a Merge Request (MR). </p> <p>When attempting to address an issue with a merge request, the issue should not be considered resolved until the merge request is merged. Merely creating a merge request should not complete the issue.  </p> <p>Merge requests include: - A description of the content or changes to be incorporated - Code changes and inline code reviews - CI/CD information - Commit history</p> <p>In general, a MR should be approved by the project maintainer or their delegated reviewers before being incorporated into the larger project's main branch. </p>"},{"location":"org-process/Gitlab%20for%20Agile/#merge-request-reviews","title":"Merge request reviews","text":"<p>Before incorporating changes or artifacts from an MR into the larger project, a review of the content is typically necessary. Merge Request Reviews provide mechanisms for the team to leave comments or make suggestions on the MR, giving evidence of the review and naturally documenting the relevant discussion, considerations, suggestions, and decision. </p> <p>When an issue has a related MR under review, it should be labeled with the <code>status::in review</code> label. </p> <p>As a rule, MR reviews should be only be performed by those who did not make any commits in the MR. In other words, reviewers only review, make comments, and discussion with the people who suggested the changes. This is slower, but cleaner and less susceptible to bias. </p> <p>The appropriate reviewer for a given artifact will depend on the specific project or repository that the artifact falls within. Different projects may have different acceptance criteria for a merge request, which will be documented in their project README. For instance, a MR into infrastructure configuration files may require a different review process than an MR into design documentation. </p> <p>Reviews, especially merge request reviews, need not be completed within the same sprint that an issue was committed to. Review requests do not show up formally in a particular user's weight. If it is the case that an iteration is particularly heavy with review requests, please note this to the team to adjust the workload accordingly.</p> <p>The Merge Request should remain assigned to the author while reviewers are assigned as reviewers. It is important for reviews to be completed in a timely fashion. Reviewers should consider it a priority to review, otherwise the process grinds to a halt. Also, the Merge Request author should poke reviewers (early and often) to encourage and remind them to review. </p>"},{"location":"org-process/Gitlab%20for%20Agile/#closing-an-issue","title":"Closing an issue","text":"<p>Issues can be closed when bugs are fixed or requested features have been developed and approved. Usually, this means that an issue can be closed when the merge request which addresses it is merged into the main branch. </p> <p>Issue can be made to close automatically when related merge request are merged into the default branch, and the commit message or merge request description containes keywoards from a default closing pattern. See closing issues automatically for more details.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#epics","title":"Epics","text":""},{"location":"org-process/Gitlab%20for%20Agile/#epics-background-best-practices","title":"Epics Background &amp; Best Practices","text":"<p>Epics are collections of issues and other epics which are used to group collections of work. Epics live at the group-level, not the project-level. Epics can be nested within other epics.</p> <ol> <li>Epics should be created at the lowest group possible. For example, if an epic only involves work involving one organization, it should be creating in that organization's subgroup.</li> <li>Epics should be labelled with the organizations (i.e., sub-groups) that are involved in the effort. This allows easy tracking and filtering at the top-level group.</li> <li>We will track two types of epics: Program Epics and Feature Epics. All epics should be labelled with either <code>epic::program</code> or <code>epic::feature</code>.</li> <li>Epics should be labeled with their work status, either <code>status::ready to start</code>, or <code>status::in progress</code>. This helps differentiating between epics which are actively being worked compared to epics which are used for longer term planning.</li> <li>Epics which are actively being worked (i.e., <code>status::ready to start</code> or <code>status::in progress</code>) should always be created with start and due dates. The start date should reflect when the epic was actually started, generally the first day of the sprint. The larger the epic, the more squishy the due date can be.</li> </ol>"},{"location":"org-process/Gitlab%20for%20Agile/#program-epics","title":"Program Epics","text":"<p>The purpose of program epics are to:</p> <ol> <li>Set the major goals for the associated product during a quarter or a longer time-period</li> <li>Enable tracking, communication, consistency, and clarity of high-level goals within that time-period</li> <li>Help group lower-level child epics</li> </ol> <p>Program epics should be created for each product, by the associated product owner during quarterly planning. They should be created with feedback and discussion with the program manager and other product owners.</p> <p>Within the description field, a program epic should contain:</p> <ul> <li>a high-level description of the goals for the product for the epic</li> <li>the dependencies for the goals within the epic</li> <li>the risk items across the epics</li> <li>the mitigations for those risk items</li> <li>the content which will be demoed the end of an increment (each quarter).</li> </ul> <p>A common usage of a program epic may be as a Quarterly Epic, especially when a product has a release within the quarter. Then, this quarterly program epic would specify the features and goals to be delivered within that quarterly release. However, not all program epics need to be quarterly. For instance, a three month contracting effort starting mid-quarter should likely have an associated program epic.</p> <p>If an effort is expected to take three sprints or more, a program epic should be made, and the task should be decomposed into smaller child epics.</p>"},{"location":"org-process/Gitlab%20for%20Agile/#feature-epics","title":"Feature Epics","text":"<p>The purpose of feature epics are to:</p> <ol> <li>Set the goals for a larger effort or a sprint</li> <li>For a set of related issues, provide a higher-level picture of what the issues should come together to achieve</li> <li>Enable tracking and communication of the lines of effort at a higher level, for instance, at scrum of scrums sessions</li> </ol> <p>The following provide some guidelines on when it may be appropriate to make an epic:</p> <ul> <li>If an effort spans more than 5 issues, the product owner should probably make an epic (especially if the issues interact in a complex way)</li> <li>If an effort is made up of multiple issues, none of which give a full picture of the effort, the product owner should probably make an epic to provide insight into the larger objective.</li> <li>If an effort involves substantial work across multiple projects (i.e., repositories), the product owner should probably make an epic.</li> <li>If an effort involves substantial work across multiple organizations (i.e., subgroups), the product owner should DEFINITELY make an epic.</li> </ul> <p>These epics should be created for each product routinely by the product owner before sprint planning. The epics should be shared with the team during sprint planning as overarching sprint themes or goals.</p> <p>Within the description field, an epic should contain:</p> <ul> <li>A high-level description of the goal of the epic, and</li> <li>The completion criteria for the epic.</li> </ul> <p>Epics should not go on forever! They should be closed when the completion criteria is reached. Timeframes for completion and scope of epics may vary somewhat better teams. Some larger teams may complete one or more epics per week. For teams working multiple efforts in parallel, they have multiple epics open at the same time for multiple sprints.\u00a0However, if an effort is expected to take three sprints or more, a program epic should be made, and the task should be decomposed into smaller child epics.</p>"},{"location":"org-process/Onboarding/","title":"Onboarding","text":"<p>For all onboarding guidance, see the Program Wiki.</p>"},{"location":"org-process/Organization%20Chart/","title":"Organization Chart","text":""},{"location":"org-process/Organization%20Chart/#purpose","title":"Purpose","text":"<p>This is a living document to track those who are currently working on the JATIC program.</p> <p>A <code>*</code> next to a name indicates that the team member is consulted or informed, but does not work on the program day-to-day.</p>"},{"location":"org-process/Organization%20Chart/#pm-team","title":"PM team","text":"<p>The PM team is the core management team for the JATIC program. </p> <ul> <li>The CDAO Chief of T&amp;E leads the CDAO's AI T&amp;E policy, tools development, and execution. </li> <li>The Chief Product Owner (CPO) leads the product owners group and coordinates program-wide strategy, plans, and epics. This is primarily at a high-level - the vast majority of epics, features, and their prioritization will be provided by the product owners of the individual products.</li> <li>The Chief Engineer facilitates program level processes and execution, drives continuous value delivery, and ensures the technical integration between products. They lead integrated efforts such as the increment demo. They manage the Software Development Plan (SDP) and provide guidance and input to the Systems Engineering team, on topics such as the SDP, DevSecOps, security, infrastructure, and product testing.</li> <li>The Scrum of Scrums Master is the scrum master for the scrum of scrums team, facilitating the prioritization and removal of impediments across teams, and continuously improving the effectiveness of the Scrum of Scrums team.</li> <li>The AI T&amp;E Expert has a deep technical understanding of the subject matter and domain in order to advise and support the product owners and development teams. Their primary role is the application of the software developed within the program to actual DoD AI algorithms to provide feedback for the team. They will also aid in the development, integration, and deployment of the products as necessary.</li> <li>The User Engagement Coordinator helps run and coordinate engagements between the program and users or user stakeholders. Their focus is on program-level engagements, but also aids in coordination of product-level and distributed/democratized engagements.</li> </ul> Name Gitlab Email Org Role Jon Elliott* @jbelliott jonathan.b.elliott2.civ@mail.mil CDAO Director of Assessment &amp; Assurance David Jin @djin david.jin5.civ@mail.mil CDAO PM &amp; Chief Product Owner Ari Kapusta @akapusta akapusta@mitre.org MITRE Chief Engineer Michael Ide @michaelide mide@mitre.org MITRE Scrum of Scrums Master Jason Gaulin @jason.c.gaulin.civ jason.c.gaulin.civ@mail.mil CDAO AI T&amp;E Expert Susan Urban @surban surban@mitre.org MITRE User Engagement Coordinator Tabitha Colter @tcolter tcolter@mitre.org MITRE Strategic Outreach &amp; Communication Lead Kevin McCleary @kmccleary kmccleary@mitre.org MITRE product owner"},{"location":"org-process/Organization%20Chart/#ml-te","title":"ML T&amp;E","text":"<p>MIT/LL, working as the \"ML T&amp;E team\", is building the JATIC toolbox, a set of common types, protocols, utilities, and tooling to support and facilitate other tools development within the AI Assurance Toolbox.</p> <p>This board can be used track ongoing work by this team. The label in Gitlab associated with this team is <code>ML T&amp;E</code>.</p> <p>This product can be found internally at <code>jatic/cdao/jatic-toolbox</code>.</p> Name Gitlab Email Org Role Michael Yee @myee myee@ll.mit.edu MIT/LL Product Owner Lei Hamilton @lei.hamilton lei.hamilton@ll.mit.edu MIT/LL Deputy Product Owner Justin Goodwin @jgoodwin jgoodwin@ll.mit.edu MIT/LL Garrett Botkin @garrett.botkin garrett.botkin@ll.mit.edu MIT/LL Olivia Brown @olivia.brown olivia.brown@ll.mit.edu MIT/LL Manasi Sharma @msharma manasi.sharma@ll.mit.edu MIT/LL Jeffrey Arena @jarena jeffrey.arena@ll.mit.edu MIT/LL Sanjeev Mohindra @smohindra smohindra@ll.mit.edu MIT/LL Vince Mancuso* @vincent.mancuso vincent.mancuso@ll.mit.edu MIT/LL LL PM"},{"location":"org-process/Organization%20Chart/#systems-engineering-systemsdevsecops-platformsystems-engineering","title":"Systems Engineering (Systems/DevSecOps Platform/Systems Engineering)","text":"<p>MITRE is the lead systems integrator for the JATIC program and is working as the \"Systems team\" from the Scaled Agile Framework, but labeled as SysEng within JATIC. This covers the Software Development Plan (SDP), DevSecOps, security, infrastructure, product testing, product integration, and onboarding. </p> <p>The issues from the SysEng team are spread throughout many repos. This board can be used to track ongoing work by this team. The label in Gitlab associated with this team is <code>SysEng</code>.</p> <p>The SDP can be found internally at <code>jatic/docs/sdp</code>. </p> Name Gitlab Email Org Role Shane Ficorilli @sficorilli sficorilli@mitre.org MITRE Development team lead Tony Rice @arice arice@mitre.org MITRE Development team member Chris Dillon @cdillon cdillon@mitre.org MITRE Development team member Craig Sims @csims-syseng csims@mitre.org MITRE Product owner"},{"location":"org-process/Organization%20Chart/#kitware","title":"Kitware","text":"<p>The Kitware team is building XAITK Saliency, an open-source python library for visual saliency algorithm interfaces and implementations to support explainable AI. </p> <p>This board can be used track ongoing work by this team. </p> <p>This product can be found internally at <code>jatic/kitware/xaitk-cdao</code> and on GitHub at <code>XAITK/xaitk-saliency</code>.</p> Name Gitlab Email Org Role Brian Hu @brian.hu brian.hu@kitware.com Kitware Principal Investigator Paul Tunison @paul.tunison paul.tunison@kitware.com Kitware Scrum Master Alex Lynch @alexander.lynch alexander.lynch@kitware.com Kitware Emily Veenhuis @emily.veenhuis emily.veenhuis@kitware.com Kitware Barry Ravichandran @bharadwaj.ravichandran bharadwaj.ravichandran@kitware.com Kitware Brandon Richardwebster @b.richardwebster b.richardwebster@kitware.com Kitware Stephen Crowell @crowelsl stephen.crowell@kitware.com Kitware Mary Elise Dedicke @mdedicke maryelise.dedicke@kitware.com Kitware Technical writer/documentation expert Austin Whitesell @awhitesell awhitesell@mitre.org MITRE Product Owner"},{"location":"org-process/Organization%20Chart/#two-six-technologies","title":"Two Six Technologies","text":"<p>The Two Six team is building Armory, a testbed for running scalable evaluations of adversarial defenses. </p> <p>This board can be used track ongoing work by this team. </p> <p>This product can be found internally at <code>jatic/twosix/armory</code> and on GitHub at <code>twosixlabs/armory</code>.</p> Name Gitlab Email Org Role Matt Wartell @matt.wartell matt.wartell@twosixtech.com TwoSix Scrum Master Christopher Woodall @christopher.woodall christopher.woodall@twosixtech.com TwoSix Software Engineer Kyle Treubig @kyle.treubig kyle.treubig@twosixtech.com TwoSix Software Engineer Hamza Mahmood @hamza.mahmood hamza.mahmood@twosixtech.com TwoSix Machine Learning Engineer Christopher Honaker @honaker christopher.honaker@twosixtech.com TwoSix Machine Learning Engineer Rohitha Bhushan @rohithabhushan rohithabhushan@datamachines.io Data Machines Data Scientist Victoria Nagy @victorianagy victorianagy@datamachines.io Data Machines Data Scientist Matthew Donizetti* matthew.donizetti@twosixtech.com TwoSix Principal Investigator Josh Harguess @jharguess jharguess@mitre.org MITRE Product Owner Jeff Gross @jtgross jtgross@mitre.org MITRE Technical SME"},{"location":"org-process/Organization%20Chart/#team-metrostar","title":"Team MetroStar","text":"<p>Team MetroStar is building an AI T&amp;E platform prototype composed almost entirely of existing open-source technologies.</p> <p>This board can be used track ongoing work by this team.</p> Name Gitlab Email Org Role Joe Early @jearlyMSS jearly@metrostar.com MetroStar Scrum Master &amp; MetroStar PM Eric Kelly @ericdatakelly edkelly@metrostar.com MetroStar Pr. Data Scientist Scott Blair @sblair sblair@metrostar.com MetroStar Pr. DevSecOps Engineer Wilson Rodden @wrodden wrodden@metrostar.com MetroStar Sr. Data Scientist Ken Foster @kenafoster kfoster@metrostar.com MetroStar Pr. Systems Engineer Jesse Scearce @jscearce jscearce@metrostar.com MetroStar ML Engineer Vy Truong* @IAMVY vtruong@metrostar.com MetroStar CINO Dharhas Pothina @dharhas dharhas@quansight.com Quansight Quansight PM Andrew James @amjames ajames@quansight.com Quansight OSS PyTorch SME Kim Pevey @kcpevey kcpevey@quansight.com Quansight Full Stack Dev Eskild Eriksen @iameskild eeriksen@quansight.com Quansight DevSecOps Christopher Ostrouchov @costrouc costrouchov@quansight.com Quansight DevSecOps Amit Kumar @akumar akumar@quansight.com Quansight DevSecOps Pavithra Eswaramoorthy @peswaramoorthy peswaramoorthy@quansight.com Quansight OSS SME and Developer Advocate Jen Bishop @jbishop jbishop@openteams.com Quansight Project Manager / Project coordination Smera Goel @sgoel sgoel@quansight.com Quansight UI/UX Designer Jonathan Velando @jvelando jvelando@metrostar.com Quansight Pr. Software Engineer / Platform and OSS Software Engineer Jonathan Bouder @jbouder jbouder@metrostar.com Quansight Pr. Software Engineer / Full-Stack Developer (UI/UX) Sanjeev Mohindra @smohindra smohindra@ll.mit.edu MIT/LL Product Owner"},{"location":"org-process/Organization%20Chart/#aria","title":"ARiA","text":"<p>This board can be used track ongoing work by this team.</p> Name Gitlab Email Org Role Jason Summers @jason.e.summers jason.e.summers@ariacoustics.com ARiA Lead Scientist Scott Swan @scott.swan scott.swan@ariacoustics.com ARiA Scrum Master Jonathan Botts @bottsj jonathan.botts@ariacoustics.com ARiA SME Max Bright @max.bright max.bright@ariacoustics.com ARiA SME Jonathan Christian @jchristian jonathan.christian@ariacoustics.com ARiA SME James Gleeson @jcgleeson1 james.gleeson@ariacoustics.com ARiA Developer Robert Jullens @shaun.jullens shaun.jullens@ariacoustics.com ARiA Developer Ed Schwab @eschwab ed.schwab@ariacoustics.com ARiA IT Andrew Weng @aweng andrew.weng@ariacoustics.com ARiA Developer Justin McMillan @jmcmillan justin.mcmillan@ariacoustics.com ARiA SME Jason Inman @jinman jason.inman@ariacoustics.com ARiA Developer Thayer Fisher @tfisher thayer.fisher@ariacoustics.com ARiA Developer Bill Peria @bperia bill.peria@ariacoustics.com ARiA Developer Jason Gaulin @jason.c.gaulin.civ jason.c.gaulin.civ@mail.mil CDAO Product Owner"},{"location":"org-process/Organization%20Chart/#ibm","title":"IBM","text":"Name Gitlab Email Org Role Jae Ro @jaero jaero@ibm.com IBM Technical lead Matt Tilley @matt.tilley matt.tilley@ibm.com IBM Mark Baker @mark-baker mark.baker@ibm.com IBM Jackson Lee @jackson.lee jackson.lee@ibm.com IBM Adam Lockwood @lockwooda adam@nyla.io IBM Pin-Yu Chen @pinyuchen-ibm pin-yu.chen@ibm.com IBM Kieran Fraser @kieranfraser kieran.fraser@ibm.com IBM Beat Buesser @beat-buesser beat.buesser@ibm.com IBM ART Maintainer Hannah Jung* @mhjung mhjung@ibm.com IBM Project executive Mark Fisk* @fiskm fiskm@us.ibm.com IBM David Yu @dyu syu@mitre.org MITRE Product Owner"},{"location":"org-process/Organization%20Chart/#morse","title":"MORSE","text":"Name Gitlab Email Org Role Peter Wein @pwein pwein@morsecorp.com MORSE Team lead Nigel Mathes @nmathes nmathes@morsecorp.com MORSE Michael Nishida @mnishida mnishida@morsecorp.com MORSE Brian Lee @blee blee@morsecorp.com MORSE Andrew Briasco-Stewart @abriasco-stewart abriasco-stewart@morsecorp.com MORSE George Bikhazi @gbikhazi gbikhazi@morsecorp.com MORSE Matthew Larsen @mlarsen mlarsen@morsecorp.com MORSE Ziyang Huang @zhuang zhuang@morsecorp.com MORSE Joel Sullivan @jsullivan jsullivan@morsecorp.com MORSE Eric Nelson* @enelsonatmorse enelson@morsecorp.com MORSE Eoin Daly @Edaly Edaly@morsecorp.com MORSE Dan Gombos @dgombos DGombos@morsecorp.com MORSE Sarah Lipstone @slipstone slipstone@morsecorp.com MORSE Rachel Rajaseelan @rachel.rajaseelan rachel.m.rajaseelan.civ@mail.mil CDAO Product Owner Andreas Elterich @andreas.g.elterich.ctr andreas.g.elterich.ctr@mail.mil CDAO Technical SME"},{"location":"org-process/SAFe%20Events/","title":"Introduction","text":"<p>The JATIC Program follows a basic Scaled Agile approach (Essential SAFe). The defining attribute of SAFe is a  reliance on a series of rolling-wave, short-term commitments from Teams that are delivered upon and revised in an iterative cycle called a Program Increment (PI).</p> <p>There are Program-level events that take place over the course of the PI. There are also individual Team-level events that take place within the PI. These two levels of events are complimentary and non-duplicative. For instance, Program-level planning events may inform, but do not replace, Team-level planning events (and vice versa).</p> <p>The Program follows a 2-week Team Sprint cadence, with Sprints starting on Wednesday morning and ending 14 days later on Tuesday night. There are 6 Sprints, plus one smaller Innovation and Planning (IP) Sprint, in each PI.</p>"},{"location":"org-process/SAFe%20Events/#team-level-events","title":"Team-Level Events","text":"<p>The following events are minimum expectations for each Team's internal Sprint cycles. There will be no micromanaging of Team activities by Program Management; Team leadership is expected to ensure that the events occur with frequencies and timeboxes that best suit the Team.</p>"},{"location":"org-process/SAFe%20Events/#sprint-planning-day-1","title":"Sprint Planning (Day 1)","text":"<ul> <li>Default Attendees: Product Owner, Scrum Master, Technical SMEs, Development Team</li> <li>Inputs: Team Objectives, Filled and Prioritized Issue Tickets, Feedback from latest Sprint Review</li> <li>Outputs: Committed set of Issues</li> </ul> <p>At the beginning of the Sprint, the Team comes together to decide on the Sprint's activities. The quantity of activities is based on the Team's capacity. The subject matter of activities is informed by the Team's top prioritized Issues. Issue priorites are based on a combination of the Team Objectives committed to at the start of the PI, feedback from the previous Sprint Review, and the Product Vision maintained by the Product Owner.</p> <p>Tips 1. A well-maintained Backlog resulting from active Product Ownership can ensure this event is quick and easy. If the session tends to take more than an hour, more proactive Backlog Grooming is likely needed. 2. Rather than \"pushing\" Issues onto Development Team members with direct assignments, consider queing up Issues for the Sprint without an assignee and letting the Development Team members \"pull\" in Issues as they are available. More detail on this approach can be found here. 3. Any activity that takes away from the Development Team's capacity should be captured as an Issue with an appropriate Weight, including technical debt or general program support.</p> <p>Read more about Sprint Planning.</p>"},{"location":"org-process/SAFe%20Events/#daily-stand-up-day-2-13","title":"Daily Stand-up (Day 2-13)","text":"<p>Since this is a large project and few if any are fully assigned to it, its unlikely we will actually have daily standups at the JATIC or CDAO/Vendor collaboration levels.</p> <p>Typically, this happens every day during the sprint, preferably in the morning. The team members come together to discuss their progress since the last stand-up, their plans for the day, and any obstacles they're facing. This meeting should be brief, typically lasting no longer than 15 minutes. </p>"},{"location":"org-process/SAFe%20Events/#sprint-review-day-14","title":"Sprint Review (Day 14)","text":"<p>Happens at the end of the sprint, on Tuesday. The team presents the work they've completed during the sprint to stakeholders, who can provide feedback. The team discusses what went well, what didn't go well, and what they can improve in the next sprint.</p>"},{"location":"org-process/SAFe%20Events/#sprint-retrospective-day-14","title":"Sprint Retrospective (Day 14)","text":"<p>Occurs after the Sprint Review. The team comes together to reflect on the sprint and identify areas for improvement. The team discusses what went well, what didn't go well, and what they can do differently in the next sprint.</p>"},{"location":"org-process/SAFe%20Events/#backlog-refinement-day-7","title":"Backlog Refinement (Day 7+)","text":"<p>An ongoing process throughout the sprint, where the team reviews and updates the product backlog. The product owner and team collaborate to refine and prioritize the backlog items based on the feedback received from the stakeholders during the sprint review. </p> <p>This may be a formalized meeting.</p>"},{"location":"org-process/SAFe%20Events/#integration","title":"Integration","text":"<p>A key event that focuses on integrating various components or features of a software project. It typically occurs towards the end of a sprint or release cycle and involves a series of tests, such as integration testing, system testing, and acceptance testing, to ensure that the software meets the desired quality standards. The team responsible for integrating different pieces of the software work together to ensure that everything is working as expected and that the final product meets the requirements of the stakeholders. The Integration Event is led by the Scrum Master or an Integration Engineer who coordinates the efforts of the team.</p>"},{"location":"org-process/SAFe%20Events/#scrum-of-scrums","title":"Scrum of Scrums","text":"<p>An important event that enables effective communication and collaboration among multiple Scrum teams working on a large-scale project. It typically takes place at regular intervals, such as weekly or bi-weekly, and is led by a designated Scrum Master. During the meeting, representatives from each team gather to discuss their progress, identify dependencies, and share information on potential roadblocks and solutions. The Scrum of Scrums provides an opportunity for teams to collaborate and coordinate their work effectively, ensuring that all members are aligned towards the same goal and that the project is progressing in the right direction. It also helps to identify and mitigate any potential issues early on, reducing the risk of delays or quality issues further down the line.</p>"},{"location":"org-process/Slack%20Usage/","title":"Slack Usage","text":""},{"location":"org-process/Slack%20Usage/#background","title":"Background","text":"<p>The JATIC program has a indepedent slack workspace, hosted by MITRE. All members of the workspace are related to or supporting the CDAO T&amp;E Division.</p>"},{"location":"org-process/Slack%20Usage/#impact-level","title":"Impact level","text":"<p>The slack workspace is NOT approved for CUI. Please do not post CUI in the slack workspace.</p>"},{"location":"org-process/Slack%20Usage/#joining-the-workspace","title":"Joining the workspace","text":"<p>Users can join the workspace in two ways.</p> <ol> <li>If you have an existing slack account, you can be invited to the CDAO T&amp;E workspace through that email. This will add the CDAO T&amp;E workspace to your existing account.</li> <li>If you do not have an existing slack account, you can be invited to create a MITRE Partnership Account. After creating this account, you can be invited to the CDAO T&amp;E workspace through that email. The MPN account will serve as the single-sign-on for Slack.</li> </ol>"},{"location":"org-process/Slack%20Usage/#channels","title":"Channels","text":"<p>Anyone can create channels, both public or private. Feel free to do so for your team or working groups. </p> <p>Anyone can join any public channel. If you make a public channel, you should expect this.</p>"},{"location":"org-process/Slack%20Usage/#direct-messages","title":"Direct messages","text":"<p>Please restrict direct messages to those within the CDAO T&amp;E workspace. </p> <p>Because the workspace is hosted by MITRE, members are able send messages to people on MITRE Slack outside of the program. Please do NOT do that, or InfoSec will get upset. Be careful that you only message who you are trying to message. If you are not sure whether someone is in the program, check out the org chart!</p>"},{"location":"org-process/Slack%20Usage/#status","title":"Status","text":"<p>Using slack status messages is a very lightweight way to increase situational awareness of team status for cross-team collaboration. </p> <p>A few recommendations for usage:</p> <ul> <li>While you are on leave, set your slack status to :palm_tree: On vacation, or something similar. </li> <li>When you have planned upcoming leave, set your slack status to :calendar: Planned leave from ___ to ___</li> </ul>"},{"location":"org-process/Slack%20Usage/#automated-workflows","title":"Automated workflows","text":"<p>Our Slack workspace has automated workflows for asychronous meeting updates. On an automated schedule, the workflow sends a message, inviting those to reply with their updates.</p>"},{"location":"org-process/Slack%20Usage/#for-cdao-scrum-of-scrums","title":"For #cdao-scrum-of-scrums","text":"<p>This workflow is currently set to message at 12:00 PM EST every Thursday in the #cdao-scrum-of-scrums channel.</p> <p>Each product team should provide an update within the automated thread.</p>"},{"location":"org-process/shared-principles-general/","title":"CDAO A2 Division Shared Principles","text":""},{"location":"org-process/shared-principles-general/#introduction","title":"Introduction","text":""},{"location":"org-process/shared-principles-general/#purpose","title":"Purpose","text":"<p>The CDAO Assessment &amp; Assurance (A2) Division is constituted of a large number of diverse organizations. In order to facilitate productive collaboration, this document establishes a set of shared principles for all work within our multi-organizational team. </p> <p>In addition to these general principles, we have also developed a set of principles that specifically apply to software development projects, in Shared Principles for Software Development.</p>"},{"location":"org-process/shared-principles-general/#request","title":"Request","text":"<p>All teams supporting CDAO A2 will review this document and raise any concerns, questions, or comments with the division as necessary. After discussion, revision, and/or establishment of further clarifying procedures, organizations will commit to following these common principles in their work with the division.</p>"},{"location":"org-process/shared-principles-general/#shared-principles-for-a2-projects","title":"Shared principles for A2 projects","text":""},{"location":"org-process/shared-principles-general/#inter-organization-collaboration","title":"Inter-organization collaboration","text":"<p>Projects within the A2 division will involve multiple teams, each with different areas of expertise and focus. </p> <p>While the government will delineate areas of focus for each organization, there will often still exist areas of overlap and ambiguity between efforts, as well as areas for synergy and collaboration. </p> <p>Teams will work to proactively identify these areas and directly connect with each other in order to share progress, find areas of synergy, and collaborate on products. In particular, teams will not let the government to become the bottleneck in communication or collaboration with other teams. </p> <p>In the situation when focus areas or tasks must be clarified or delineated in order to move forward, teams will proactively escalate to the government for a decision.</p>"},{"location":"org-process/shared-principles-general/#organize-around-iterative-value-delivery","title":"Organize around iterative value delivery","text":"<p>Products should not be made for their own sake; rather, they should be aimed at delivering value by meeting the needs and requirements of the end customer. Teams will organize their work around value delivery and impact towards the end customer. </p> <p>Iterative delivery presents opportunities for discovery of and reprioritization to customer requirements, as well as opportunities for inter-team collaboration. Teams will deliver in-progress work items iteratively, in order to receive feedback from the customer, government, and larger division, as well as to allow greater opportunity for collaboration and coordination with other teams. While there is no universal rule for delivery cadence, teams will strive to reduce their cycle time for value delivery.</p>"},{"location":"org-process/shared-principles-general/#use-of-common-platforms","title":"Use of common platforms","text":"<p>Shared platforms with collective adoption and usage are critical for facilitating collaboration, coordination, information sharing, and situational awareness across a large, multi-organizational team.</p> <p>The preferred communication platform for the division is Slack. Teams will ensure that they are quickly reachable within Slack and will use it for division communications as much as is securely possible. The preferred platform for information sharing, intra-division planning, and collaboration is Gitlab. Teams will use Gitlab for division information sharing, planning, and collaboration as much as is securely possible. </p> <p>Project-specific communications and collaboration platforms should default to these, but may deviate when necessary to accommodate project-specific needs, such as security.</p>"},{"location":"org-process/shared-principles-general/#use-of-common-processes-and-standards","title":"Use of common processes and standards","text":"<p>Similar to shared platforms, the use of common processes and standards are critical for facilitating collaboration, coordination, information sharing, and situational awareness.</p> <p>The government will also specify common processes and standards across the division. These will include processes for higher-level strategic alignment, such as a synchronized portfolio planning / review process, as well as mundane processes for daily tasks, such as common processes and standards for meeting scheduling, support tickets, out-of-office indicators, or information organization. </p> <p>Teams will align with these processes and standards. In addition (and more importantly), teams will work to provide feedback and continuously improve on these common division processes and standards in order to become a more effective team.</p>"},{"location":"org-process/shared-principles-general/#transparency-of-work","title":"Transparency of work","text":"<p>Transparency of work is critical to facilitating information discovery and situational awareness across a large, multi-organizational team.</p> <p>Teams will make artifacts, code, and documentation transparent to the entire A2 division as much as is securely possible. Teams will also make plans and ongoing work priorities, including roadmaps, epics, and issues, transparent to the entire A2 division as much as is securely possible. </p> <p>Information that is made available to other teams should be visible, accessible, and understandable - meaning that other team members can locate it, access it, and recognize the content, context, and applicability of it. </p>"},{"location":"org-process/shared-principles-software/","title":"CDAO A2 Division Shared Principles for Software Development","text":""},{"location":"org-process/shared-principles-software/#introduction","title":"Introduction","text":""},{"location":"org-process/shared-principles-software/#purpose","title":"Purpose","text":"<p>The CDAO Assessment &amp; Assurance (A2) Division creates many different software products across its large multi-organizational team. In order to develop interoperable softwfare and facilitate productive collaboration, this document establishes a set of shared principles for software development within the division. </p> <p>This document is an amendment to the A2 Division Shared Principles. </p>"},{"location":"org-process/shared-principles-software/#request","title":"Request","text":"<p>All teams supporting CDAO A2 in software development efforts will review this document and raise any concerns, questions, or comments with the division as necessary. After discussion, revision, and/or establishment of further clarifying procedures, organizations will commit to following these common principles in their work with the division.</p>"},{"location":"org-process/shared-principles-software/#shared-principles-for-a2-software-development-projects","title":"Shared principles for A2 software development projects","text":""},{"location":"org-process/shared-principles-software/#interoperability-of-software-products","title":"Interoperability of software products","text":"<p>Within software development efforts, teams will need to work closely with other teams and the government in order to ensure interoperable usage of their products. Teams will work collaboratively with other teams and the government to design intelligent interfaces and APIs to enable interoperability with their products, as well as other AI/ML tools. In order to ensure interoperability of products, the government will have the right to mandate the use of certain interfaces or APIs.</p>"},{"location":"org-process/shared-principles-software/#agile-development-methodology","title":"Agile development methodology","text":"<p>Software development efforts will be organized using the Scaled Agile Framework software development methodology. </p> <p>All teams will operate on synchronized two-week sprints set by the government. Releases for each product will be held quarterly.</p> <p>To each product team, the government will assign product owner(s), who ensure that the product direction is aligned with stakeholder needs. In particular, the product owner will continuously prioritize the backlog and create larger epics for the product, in consultation with the team and technical subject matter experts.</p> <p>Each team is expected to have a team member to fulfill the \u201cscrum master\u201d role for the team. They need not be a certified scrum master but should be familiar with the principles of scrum and their responsibilities as a scrum master. </p> <p>The team should be familiar (or willing to learn) about the principles and processes of scrum. </p> <p>The product team will hold standard agile ceremonies, such as standup, sprint planning, backlog refinement, sprint demonstrations, and sprint retrospectives. The product owner will lead some of these meetings, such as sprint planning and backlog refinement. The government team will also lead a scrum-of-scrums, which a representative from each product team will attend. The government will engage end users for their feedback as often as possible, inviting them to sprint demos and other sprint meetings when appropriate. </p> <p>All work items, including items in the backlog, active work in progress items, and larger epics, will be tracked in the government Gitlab instance. All types of technical work, including code, documentation creation, research, integrations, etc., will be planned in sprint planning and tracked in the government Gitlab instance.</p>"},{"location":"org-process/shared-principles-software/#software-delivery-cicd","title":"Software delivery &amp; CI/CD","text":"<p>All code and associated documentation will be delivered to the government Gitlab instance. Software delivery will follow DevSecOps best-practices. Code, corresponding to the work items completed, will be delivered by teams at least once each sprint. It will be automatically testing in the government\u2019s CI/CD pipeline for unit and security tests. These results, as well as the tests themselves, will be easily viewable to teams for quick iteration. CI/CD and automatic testing will be performed within the government Gitlab instance. </p>"},{"location":"org-process/shared-principles-software/#software-development-plan","title":"Software development plan","text":"<p>Software development will follow the government\u2019s Software Development Plan (SDP). This document broadly specifies the style, dependency management, testing plan, and common tools in order to ensure a baseline consistency between different teams\u2019 code. </p> <p>The government will accept feedback and inputs on the SDP and make updates regularly in order to ensure best software development practices. The SDP is designed to simplify the process of creating high-quality  software and reduce variability between projects in terms of their structure, design, testing, packaging, and documentation. It is designed to be reasonable and not impose heavy burdens.</p> <p>Each team will provide a sub-document to the government\u2019s SDP which provides further details on the software development practices, style, and assumptions for their projects. </p>"},{"location":"program-strategy/","title":"Program Strategy","text":"<p>This project outlines the high-level vision and strategy for our program. </p> <ul> <li>Capability Description provides a overview of the scope and vision of JATIC for FY23.</li> <li>Design Principles is a lengthier exploration for some of the design principles for JATIC tools.</li> <li>Programmatic Docs primiarly contains documents for the Software Acquisition Pathway</li> </ul> <p>POC: @djin</p>"},{"location":"program-strategy/Agile%20Development%20Approach/","title":"Agile Development Approach","text":"<p>Section to be completed: software project management plan.</p>"},{"location":"program-strategy/Agile%20Development%20Approach/#agile-software-development","title":"Agile software development","text":"<p>JATIC software will be developed with an Agile approach. The following principles will govern the high-level software development framework:</p> <ol> <li>Constant focus on delivery of value to users</li> <li>Continual re-evaluation of direction to best deliver value, with regular re-alignment of tasks with capability need</li> <li>Rapid and regular delivery of working code</li> <li>Transparent, open, and frequent communication between all teams working on program, including government, FFRDCs, and vendors</li> <li>Open access to code between all teams working on program, including all vendors</li> <li>Use of modern software development methodologies (e.g., Scrum, DevSecOps) and tools (e.g., Slack, Confluence, Kanban, Git, Jenkins)</li> </ol> <p>All vendors will utilize an Agile software development approach, with ability to tailor to particular preferences in methodology and experience. All JATIC vendors will employ agile development methods such as:</p> <ul> <li>All vendors will operate on two week sprints, with all vendors delivering code on the same week.</li> <li>daily collaboration between Government and contractor teams</li> <li>mutually agreed-upon definition of \u201cdone\u201d for each sprint/release cycle</li> <li>metrics informing the Government about the quality of capability developed and delivered</li> </ul> <p>The government, in consultation with government sponsors through the User Agreements, will have full control of prioritizing the product backlog and managing the development process to ensure software aligns with architecture, vision, and other development activities.</p>"},{"location":"program-strategy/Agile%20Development%20Approach/#iterative-value-delivery-cycle","title":"Iterative value delivery cycle","text":"<p>The iterative value delivery cycle operates on multiple time frames at once. We will establish the short-, medium-, and long-term time frames as &lt; 1 month, multiple months, &gt; 1 year, respectively. The types of value delivery are referenced from Value Statement from Capability Descripition.</p> <p>On the medium- to long-term time frame, software capabilities may be created or extended to support new T&amp;E dimensions, AI tasks, or mission use cases. These tasks can greatly vary in scope and the amount of work needed. As an example lower bound, it may be take a few weeks or a month to extend a T&amp;E capability from regular imagery to satellite imagery formats, which include multiple magnifications. As an example upper bound, it may take a year or longer to create capabilities for adversarial AI T&amp;E of natural language processing (NLP) algorithms, because there may be almost no overlap with the CV counterpart.</p> <p>Also on the medium- to long-term time frame, software capabilities may be integrated with large technologies and deployed on large, complex DoD environments. Technologies requiring extensive integration may be large platforms such as Sagemaker or Databricks, or frameworks such as TensorFlow or Pytorch. Large DoD environments include enterprise solutions such as ADVANA or integration into sensitive environments.</p> <p>On the short- to medium-term time frame, software capabilities may be extended with smaller T&amp;E functions, or integrated into smaller technologies.</p> <p>Continuously, software will be pushed to increase ease of use, user experience, performance, cybersecurity, and productization.</p> <p>Within the Agile time frames, Initiatives will be defined as a set of long- and medium-term objectives. An Epic will focus on the delivery of a major milestone or component of a long- or medium-term objective, or the delivery of a short-term objective.</p> <p>The long-term objectives, such as the AI task, T&amp;E dimension, and the environments / large technologies with which to integrate, will be determined as high-level requirements to vendors before beginning work. Objectives on shorter time frames will have more flexibility, with greater ability to re-prioritize to meet user or mission needs, or to implement the best solution.</p>"},{"location":"program-strategy/Capability%20Description/","title":"Capability Descripition","text":""},{"location":"program-strategy/Capability%20Description/#introduction","title":"Introduction","text":"<p>This document provides a technical overview of the JATIC Capability, focusing on the MVP to be developed in FY23.</p>"},{"location":"program-strategy/Capability%20Description/#jatic-te-tools-vision","title":"JATIC T&amp;E Tools Vision","text":"<p>Develop AI T&amp;E tools that are widely adopted by DoD programs to increase the rigor of their missions and by the open-source community</p>"},{"location":"program-strategy/Capability%20Description/#scope","title":"Scope","text":"<p>For this MVP, JATIC aims to develop T&amp;E capabilities to test AI/ML models for Computer Vision (CV) classification and object detection (OD).</p> <p>While other AI modalities besides CV, such as autonomous agents and natural language processing also require T&amp;E capabilities, they are out of scope for the current JATIC MVP.</p> <p>CV classification and object detection was chosen as the AI modality for JATIC's MVP due to its wide use across the DoD in many operational contexts. CV models, often of very similar underlying architectures, are used across diverse tasks such as automatic target recognition, satellite imagery classification, medical imagery diagnosis, and object detection on UAVs, UUVs, and autonomous vehicles. Thus, T&amp;E capabilities which support CV classification and object detection models may have broad applicability across the Department. Future AI modalities (e.g. natural language processing, full motion video, image segmentation, etc.) will be addressed in future capabilities releases of JATIC. However, the ordering and priority of each of these modalities is yet to be firmly determined and will evolve as CDAO T&amp;E comes to better understand the prevalence and demand for T&amp;E capabilities within each one of these areas.</p> <p>While other areas of T&amp;E, such as systems integration, human-machine, and operational T&amp;E are complex issues with capabilities gaps, they are out of scope for JATIC.</p> <p>Other areas of testing, such as systems integration testing, are far more difficult to develop generalizable T&amp;E capabilities for. For example, while the same T&amp;E software may be able to test a CV OD model for radiology and a CV OD model for vehicle detection on UAVs, the required systems integration testing for these two capabilities looks completely different. Thus, we believe that capabilities for systems integration, human system interaction, and operational testing should be left to individual organizations, agencies, and services to develop, who have the best knowledge about their operational mission and systems.</p> <p>While operational monitoring of AI models and synthetic data generation are capabilities areas which are critical for AI/ML, they are out of scope for JATIC. This is because there are other programs within the CDAO which will address these capability gaps.</p>"},{"location":"program-strategy/Capability%20Description/#problem-statement","title":"Problem statement","text":"<p>There is widespread interest throughout the DoD for enterprise-level T&amp;E capabilities to address the novel challenges posed by the T&amp;E of AI. While many T&amp;E capabilities have been developed by previous DoD AI Programs to meet the specific needs of those programs, their usage and distribution throughout the Department has been limited by several key factors, including:</p> <ol> <li>Lack of capabilities for advanced AI T&amp;E functions, such as robustness testing.</li> <li>Lack of maturity, scalability, reproducibility, ease of use, or cyber-hardening of capabilities.</li> <li>Difficulty deploying and using capabilities within DoD environments.</li> <li>Difficulty using capabilities within ML pipelines and technology stacks.</li> <li>Difficulty using capabilities in an integrated T&amp;E pipeline, as each individual capability often requires custom model formats or data wrangling.</li> <li>Inability for capabilities to evolve with developments in AI research.</li> </ol> <p>These limitations, in addition to other factors, such as a lack of common standards and best practices for AI T&amp;E, have inhibited the ability for DoD AI programs to perform comprehensive T&amp;E of their AI models.</p> <p>A further discussion of previous AI T&amp;E capabilities can be found in JATIC Design Principles.</p> <p>In addition previous capabilities were also greatly limited by the T&amp;E functions that they enabled users to perform and were primarily limited to producing summary metrics of the performance of an AI model over an entire dataset. With only these functions it is difficult to understand the functioning of an AI model in operation.</p> <p>It is not clear that a test dataset is well representative of an operational dataset, or that there are no biases in the test dataset. Even if an AI model does well overall across a test dataset, that does not indicate that it is free of biases or performance variations across the subclasses. It also does not indicate that it is robust to operationally realistic perturbations or adversarial attacks. Finally, understanding the overall performance of a model does not provide sufficient intuition for what features a model finds important or what criteria it is using to make its decision. Possessing these additional capabilities and more greatly strengthens the ability for a tester or analyst to understand the performance of an AI model.</p>"},{"location":"program-strategy/Capability%20Description/#ai-te-functionality","title":"AI T&amp;E Functionality","text":"<p>Within its minimum viable product (MVP), JATIC seeks to develop T&amp;E capabilities within the following dimensions of AI T&amp;E:</p>"},{"location":"program-strategy/Capability%20Description/#dataset-analysis","title":"Dataset Analysis","text":"<p>Dataset analysis capabilities allow T&amp;E stakeholders to understand held out T&amp;E datasets, including their quality and similarity to operational data. This dimension is focused on assessing properties of a dataset that are independent of a particular AI model under test. Example functionalities include (but are not limited to) the computation of:</p> <ul> <li>Dataset quality (e.g., label errors, missing data)</li> <li>Dataset sufficiency for a given test (e.g., number of samples, variation)</li> <li>Biases, outliers, and anomalies in the dataset, which may be naturally occurring or intentionally inserted (i.e., data poisoning)</li> <li>Comparison of two datasets (e.g., divergence of dataset distributions)</li> </ul>"},{"location":"program-strategy/Capability%20Description/#model-performance","title":"Model Performance","text":"<p>Model performance capabilities allow T&amp;E stakeholders to assess how well an AI model performs on a labeled dataset and across its population subclasses. Example functionalities include (but are not limited to):</p> <ul> <li>A comprehensive set of well-established CV metrics (e.g., precision, recall, mean average precision)</li> <li>Metrics to assess probability calibration, i.e., the reliability of the model's measure of predictive uncertainty (e.g., expected calibration error, entropy, reliability diagrams)</li> <li>Metrics to assess fairness and bias in model output across the test dataset (e.g., statistical parity)</li> </ul> <p>In addition to pre-defined metrics, capabilities will support the creation of custom, user-defined metrics.</p>"},{"location":"program-strategy/Capability%20Description/#model-analysis","title":"Model Analysis","text":"<p>Model analysis capabilities expand upon the model performance dimension by allowing T&amp;E stakeholders to gain a deeper understanding of a model's capabilities and limitations by uncovering hidden trends, patterns, or insights across the input space. Example functionalities include (but are not limited to) algorithms that:</p> <ul> <li>Detect trends in model performance across the entire dataset, such as distinct types of model errors</li> <li>Identify clusters of the model input space based on model predictions, feature values, network activations, etc.</li> <li>Identify potentially mislabeled ground truth data based on sets of model predictions</li> <li>Determine under-tested or high-value regions of the input space (for the model under test) to inform future test data collection or labeling</li> </ul>"},{"location":"program-strategy/Capability%20Description/#natural-robustness","title":"Natural Robustness","text":"<p>Natural robustness capabilities enable T&amp;E stakeholders to determine how natural corruptions in data can impact model performance. These corruptions emulate realistic noise that may be encountered within the operational deployment environment. Example corruptions include (but are not limited to):</p> <ul> <li>Pre-sensor, environmental or physical corruptions (e.g., fog, snow, rain, changes in target shape or dimensions)</li> <li>Sensory corruptions (e.g., out-of-focus, glare, blur)</li> <li>Post-sensor, in-silico corruptions (e.g., Gaussian noise, digital compression)</li> </ul> <p>Capabilities for creating these corruptions may leverage synthetic data generation techniques. Capabilities will provide users with the ability to control the severity level of the corruption, so that performance can be reported as a function of severity level and T&amp;E stakeholders can use these results to set or assess requirements on performance and robustness.</p>"},{"location":"program-strategy/Capability%20Description/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>Adversarial robustness capabilities enable T&amp;E stakeholders to assess how adversarial corruptions on data inputs may impact model performance. The primary focus of this dimension will be on evasion-style attacks (i.e., attacks in which the adversary aims to manipulate the input data to produce an error in the model output, often covertly). Example types of attack methods include (but are not limited to):</p> <ul> <li>White-box and black-box methods</li> <li>Mathematical (e.g., Lp norm-constrained) and physically realizable (e.g., patch) attacks</li> <li>Empirical and certified attacks</li> </ul> <p>In addition to pre-defined attacks, capabilities will provide lower-level building blocks to enable users to create their own adaptive attacks that can be customized to the specific AI model under test. Capabilities will provide users with the ability to control the attack severity and assess performance given varying assumptions on adversary knowledge and adversary capabilities.</p>"},{"location":"program-strategy/Capability%20Description/#model-cards-data-cards-and-te-reports","title":"Model Cards, Data Cards, and T&amp;E Reports","text":"<p>Model cards, data cards, and T&amp;E reports provide summarized information about a model or datasets performance for end-user consumption. </p> <p>Example functionalities include:</p> <ul> <li>Ingestion of metrics and results from other tools</li> <li>Machine readable formats, compatible with experiment tracking and dashboarding tools</li> <li>Human readable formats, e.g., powerpoint report generation</li> </ul>"},{"location":"program-strategy/Capability%20Description/#ai-assurance-toolbox","title":"AI Assurance Toolbox","text":"<p>Most of the software developed within the JATIC program in FY23 will fall within the AI Assurance Toolbox.</p> <p>This toolbox will be a set of python libraries, mutually compatable, built on a set of shared design principles, with a narrowly scoped set of functionality specific to AI T&amp;E.</p> <p>Tools within the AI Assurance Toolbox will generally adhere to the following principles:</p> <ul> <li>Will be usable as independent but mutually compatabible python libraries</li> <li>Will prefer a narrow scope of functionality constrained to AI T&amp;E, making use of existing open-source solutions for common ML tasks like data loading, experiment tracking, visualization, model inference, etc.</li> <li>Will implement open-source standard APIs for models, datasets, etc.</li> <li>Will build upon existing open-source libraries (especially when the open-source components are widely used and recognized), increasing their maturity, compatibility, or relevancy for DoD usage</li> <li>Will (where appropriate) publically release or open-source capabilities</li> <li>Will be designed to enable straightforward contributions from developers in the community</li> <li>Will be designed to enable continual evolution with the state-of-the-art in AI T&amp;E and updates in ML technologies</li> <li>Will allow users to add functions (e.g., custom metrics, perturbations, adversarial attacks) in an extensible way</li> <li>Will be lightweight and straightforward to run on a wide range of deployment environments, including all major operating systems, cloud deployments, etc.</li> <li>Will be comprhensively supported with examples, tutorials, and documentation</li> </ul> <p>By limiting the software form factor to python libraries, a narrow scope of functionality, and heavy re-use of open-source, this toolbox will be sufficiently flexible to be easily usable within many different T&amp;E workflows, ML technology stacks, and compute environments. For instance, individual libraries within the AI Assurance Toolbox will be usable independently within Jupyter Notebooks to enable quick model testing and metrics visualization (e.g., gradio, streamlit), and also within production-level ML pipelines - leveraging features such as test automation, experiment tracking, and visualization dashboards - to enable powerful new insights into model performance.</p> <p>In addition, while JATIC is primarily focused on supporting T&amp;E of models after training, the libraries within the AI Assurance Toolbox will be applicable to many other phases of the AI lifecycle, such as model development or post-deployment monitoring.</p>"},{"location":"program-strategy/Capability%20Description/#ai-te-platform","title":"AI T&amp;E platform","text":"<p>We noted above how libraries within the AI Assurance Toolbox are sufficiently flexible to be used within many AI/ML platforms, given their lightweight form factor and narrow scope. However, some common MLOp capabilities such as automation, tracking, and visualization are critical components for any AI T&amp;E pipeline. By not providing this functionality in the AI Assurance Toolbox (as to not duplicate effort which exists in many MLOps platforms), it becomes very hard for users without an existing platform or pipeline (which we call \"platform-free\" users in the Design Principles) to use these tools to their full potential - the libraries will seem fragmented and disconnected.</p> <p>For these users, the AI Assurance Toolbox may be delivered altogether, bundled together with open-source, industry-standard tools for workflow automation, experiment tracking, object storage, visualization, etc. This will provide a comprehensive and opinionated platform for AI T&amp;E, which may either be adopted wholesale or used as a reference architecture. We call this platform the AI T&amp;E Platform. </p> <p>The AI T&amp;E Platform will be deployable to a Kubernetes cluster on commercial cloud (AWS, Azure primary targets) or on-prem. Additionally, it will be deployable to local machines as a set of dockerized micro-services. It will enable the use of infrastructure as code to quickly be deployable to a diverse set of envrionments.</p>"},{"location":"program-strategy/Capability%20Description/#areas-of-focus","title":"Areas of focus","text":"<p>JATIC is a program that aims to develop mature software which can be leveraged widely across the DoD. The following are the areas of focus within JATIC work.</p>"},{"location":"program-strategy/Capability%20Description/#integrations","title":"Integrations","text":"<p>Four types of integration are critical: 1. Easy deployments of tools into varied DoD environments 2. Easy integration of tools into varied MLOps platforms and stacks 3. Easy integration of tools with common ML model and dataset formats 4. Integration of tools with each other</p> <p>We hope that the python library modality of tools within the AI Assurance Toolbox makes 1. reasonably easy.</p> <p>For 2., some example ML technology categories and instances to that we are considering compatibility with include:</p> <ul> <li>ML frameworks: PyTorch, TensorFlow, HuggingFace</li> <li>ML pipelines: Databricks, SageMaker</li> <li>Compute engines: Kubernetes, Spark, Ray</li> <li>Orchestrators: Airflow, Kubeflow, Pachyderm</li> <li>Scalability frameworks: PyTorch Lightning, Horovod</li> </ul> <p>Integration into these technologies, in may cases, is not necessarily deep. Since JATIC software is designed as python libraries, integration with a given technology may simply mean that they can easily be made to work together within python code, because of their expected formats, interface design, etc.</p> <p>To enable much of this integration within 2., 3., and 4., the JATIC team will develop a set of standardized protocols for models, datasets, metrics, and transformations. These protocols will make use of structural subtyping within python and will be heavily inspired by existing protocols within HuggingFace, Pytorch, and TensorFlow for maximimum compatability. In addition, our development shall place heavy emphasis on diligent API design in order to enable straightforward compatibility with a wide range of technologies.</p> <p>To enable 4., we will ensure consistent protocol expectations and version support across all of our tools.</p>"},{"location":"program-strategy/Capability%20Description/#productization","title":"Productization","text":"<ul> <li>Increase code quality, polish, speed, stability, scalability, and security of code. </li> <li>Resolve technical debt, hacked solutions, etc.</li> </ul>"},{"location":"program-strategy/Capability%20Description/#documentation-tutorials-and-usability","title":"Documentation, tutorials, and usability","text":"<ul> <li>Increase quality and quantity of useful end-user documentation.</li> <li>Create detailed tutorials and example workflows, especially aimed at those who may be less familiar with AI development.</li> <li>Increase user enjoyment out of software</li> <li>Increase usability of software.</li> </ul>"},{"location":"program-strategy/Capability%20Description/#intellectual-property","title":"Intellectual Property","text":"<p>Government will have full ability to access, re-distribute, and modify code.</p> <p>Developing vendors will retain the right to license their code with an open-source license and release it publicly (subject to some constraints on government exclusive features).</p>"},{"location":"program-strategy/Classification/","title":"Classification","text":""},{"location":"program-strategy/Classification/#jatic-mvp-classification","title":"JATIC MVP Classification","text":"<p>The JATIC Program will create software to enable the T&amp;E of AI models, primarily as python libraries.</p> <p>The Program Manger has determined that the software developed under the JATIC Program Minimum Viable Product (MVP) will be Unclassified, in order to most effectively meet the above capability need. The following constraints will be placed upon all software developed within the JATIC MVP:</p> <ol> <li>Shall not include information about any specific AI systems</li> <li>Shall not include any of the data that is required to test AI systems</li> <li>Shall not provide T&amp;E Master Plans, Test Plans, metrics thresholds, or metric objectives of the capabilities of an AI program, mission, or line of effort</li> <li>Shall not describe the tactics, techniques, or procedures used to evaluate the capabilities of an AI program, mission, or line of effort</li> <li>Shall not include details on the vulnerabilities, performance, accuracy, efficiency, reliability, other metrics, or reports of capabilities of an AI program, mission, or line of effort</li> <li>Shall not include techniques to test any AI model modalities, sensors, or systems that are DoD-unique (i.e., JATIC tools are applicable generically to research or commercial AI models)</li> </ol> <p>This classification is consistent with the following elements of DoD guidance:</p> <ol> <li>Joint AI Center (JAIC) Security Classification Guide (SCG), Section 3, Topics 44, 45, 50, and 113.</li> <li>DoD CIO Memo on Software Development and Open Source Software, Main memo &amp; Attachment 2</li> <li>Executive Order 13526, Section 1.4</li> <li>NIST White Paper, \"Mitigating the Risk of Software Vulnerabilities by Adopting a Secure Software Development Framework (SSDF)\"</li> </ol> <p>In addition, the design of MVP JATIC software to only include unclassified information is critical for fulfilling the above capability need. LOE 2.1.1 within the RAI S&amp;I Pathway mandates the AI T&amp;E Toolkit to be made widely available to DoD users, and 2.1.3 mandates a central repository for these tools. Currently, the DoD Centralized Artifact Repository and Source Code Repository are at IL2. There are no mechanisms to make CUI source code widely available to DoD users. Thus, MVP JATIC software will be developed in such a way that all materials within the program are Unclassified.</p>"},{"location":"program-strategy/Classification/#risk-management","title":"Risk Management","text":"<p>The JATIC program will employ the following processes and design practices in order to ensure that software developed within the MVP is appropriate for the level of classification.</p>"},{"location":"program-strategy/Classification/#review-process","title":"Review Process","text":"<p>Within the initial planning process each development team will confirm and vet their lines of development, functionalities, and capabilities with the JATIC program management team. Periodic manual reviews (e.g., at development \u201csprint\u201d planning meetings) will keep the work within bounds.</p> <p>Software quality will be monitored and enhanced by applying software engineering best practices. A \u201clinter\u201d will be applied to flag common programming concerns and enforce consistent source code style. Continuous integration (CI) and continuous delivery (CD) workflows will be implemented to automatically build and test the software on a regular basis. The current state of the repository will be indicated with \u201cbadges\u201d for properties such as build status, unit test results, documentation, and code coverage. In addition to automated quality control measures, code will be reviewed for quality and regularly tested by independent government testers who will control the quality and security of deliverables.</p>"},{"location":"program-strategy/Classification/#modular-open-systems-approach","title":"Modular Open Systems Approach","text":"<p>In accordance with the DoD CIO Memo on Software Development, the risk of code developed for DoD systems potentially benefitting adversaries will be managed through adopting a Modular, Open-Systems Approach (MOSA), within all of JATIC software packages.</p> <p>Note that an Unclassified MVP of JATIC clearly does not preclude the use of JATIC for classified or sensitive work, or the creation of additional modules at higher levels of classification. The JATIC MVP will provide a software foundation upon which later years of the program may build modules that are more specially tailored to mission use cases. This approach provides the ability for a wide distribution of JATIC foundational software, while protecting critical, innovative components as separate modules.</p>"},{"location":"program-strategy/Classification/#system-or-program-specific-information","title":"System or Program Specific Information","text":"<p>JATIC will not reveal, contain, describe, or include information on specific AI models, AI-enabled systems, AI programs, missions, or lines of effort. It will include neither data, nor models, nor metrics, nor vulnerabilities, nor results, nor specific tactics, techniques, or procedures used, nor thresholds or objectives to achieve.</p> <p>In addition, the JATIC MVP will only include techniques which are applicable to AI model frameworks and modalities that are commonplace within research and industry. In particular, the JATIC MVP will include tools for evaluation of CV Object Detection and Classification, two of the most common tasks found within AI research. JATIC will not include tools to test DoD-unique technology categories, such as DoD-unique sensors or DoD-unique AI-enabled systems. Testing software of this sort must be extremely tailored to the system, and may be appropriate for classification relative to the system that they are tailored to.</p>"},{"location":"program-strategy/Classification/#datasets-models-and-third-party-software","title":"Datasets, Models, and Third-Party Software","text":"<p>Development and testing will be driven by open-source datasets (such as CIFAR-10 and ImageNet, which are common benchmarks used in computer vision research) and open-source AI models. No new data sources will be distributed within JATIC. Any examples, data samples, or pre-trained models that are used as examples within JATIC documentation will be derived from open datasets or publicly available sources.</p> <p>We do not plan to redistribute any embedded third-party software. However, in software description, we will list all core dependencies required to be present in the environment for our software to be able to run.</p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/","title":"Contracting, Acquisition","text":""},{"location":"program-strategy/Contracting%20and%20Acquisition/#competition-strategy-modular-contracting-approach","title":"Competition strategy (Modular contracting approach)","text":"<p>JATIC will follow a modular contracting approach, heavily leveraging the Agile software development contracting strategy within DoD 5000.87. Discrete needs of JATIC will be be contracted separately in smaller increments, rather than a single contract that will provide all needs. The main categories of work within the JATIC effort will be:</p> <ol> <li>Development of JATIC software, including creation, productization, and hardening</li> <li>Cloud environments for software integration and test, including creation of DevSecOps environment</li> <li>Personnel for system administration, capability test, capability integration, and other activities</li> </ol> <p>Of the three, the large majority of funding will go towards (1) Development of JATIC software. The development of JATIC software will be further divided and awarded within smaller contracts, split by technical dimensions of AI T&amp;E, which are discussed within the Capability description. Each of these dimensions will become discrete software capabilities of the overall JATIC solutions.</p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#innovation-evolution","title":"Innovation &amp; evolution","text":"<p>The JATIC program will collaborate with organizations in the DoD doing Science &amp; Technology (S&amp;T) research, including DARPA, DIU, TRMC, and the Service Labs, in order to transition prototype capabilities. In addition, because of the JATIC team's close interactions and feedback loop with AI T&amp;E tool users, the team will inform S&amp;T organizations on critical research directions in order to best meet capability needs.</p> <p>JATIC will transition prototypes from TRL 5-6 to TRL 8-10 to be made available as mature products across the DoD.</p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#contracting-strategy","title":"Contracting strategy","text":""},{"location":"program-strategy/Contracting%20and%20Acquisition/#contract-vehicle","title":"Contract vehicle","text":"<p>JATIC will use the Tradewind OTA as its primary contracting solution, but will also use other contracting solutions if appropriate for the need, including CDAO's Test and Evaluation Services Blanket Purchase Agreement (T&amp;E BPA). </p> <p>JATIC contracts for the development of software will primarily be Firm Fixed Price (FFP) contracts for the development of software products. For a small number of activities, such as the system administration of the DevSecOps environment, JATIC will leverage service contracts.</p> <p>Contract periods of performance (PoPs) will be short (e.g., 12 months) to align with release and deployment cycles, with extensions to continue development support as needed. Since each of the software capabilities that JATIC will be acquiring is relatively small, CDAO T&amp;E does not plan to use a challenge-based acquisition approach where multiple contracts are awarded contracts to demonstrate competing capabilities towards the same requirements and need. Instead, JATIC will utilize short PoPs, options, and agile reprioritization to continuously allow resource allocation to most effective solutions and pressing capability needs.</p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#selection-approach","title":"Selection approach","text":"<p>JATIC will use a phased approach to selection in order to accelerate the timeframe of capability delivery.</p> <ol> <li>Release RFIs and receive responses from vendors.</li> <li>Perform market research and information obtained through RFI demos</li> <li>Refine requirements and release Call to Industry</li> </ol> <p>CDAO T&amp;E will utilize Oral Presentations of technical proposals, as well as Technical Product Demonstrations as the primary mechanisms for performing vendor evaluation. </p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#agile-software-contracts","title":"Agile software contracts","text":"<p>JATIC vendors will leverage an agile software development approach, which is characterized by continuous change and reprioritization of requirements. Thus, JATIC requires flexible contracts that allow the scope and prioritized capability needs to change in order to deliver functional capability. The government, in consultation with government sponsors through the User Agreements, will have full control of prioritizing the product backlog and managing the development process to ensure individual development contracts align with architecture, vision, and other development activities. Contract modifications will not be not necessary to reprioritize, add, or delete Agile requirements from the product backlog.</p> <p>In order to utilize Agile development approaches along with FFP contracts, the contract will fix award price and period of performance, but not strictly fix project scope (fix time and cost, flexible and continuously updated scope). Contract price will be negotiated by setting a level-of-effort that the contractor should provide over the PoP, using specified hours in specified labor categories. PoP will be short (e.g., 12 months) to align with release and deployment cycles.</p> <p>Project scope and requirements will be defined in terms of threshold / objective achievement which allow room for flexible reprioritization of requirements while still allowing criteria for clear and unbiased vendor performance evaluation.</p>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#vendor-deliverables","title":"Vendor deliverables","text":"<p>JATIC contracts will include a deliverables list, but will strive to maintain simplicity rather than excess in extensive deliverable requirements.</p> <p>In order to avoid excessive deliverables documentation, JATIC will:</p> <ul> <li>Distinguish between high-value documents and those that simply \u201ccheck the box\u201d or do not contribute to the value or quality of the product being delivered.</li> <li>Streamline the format of the required documents as appropriate.</li> <li>Work with the decision authorities to align on an incremental and iterative approach to document completion.</li> <li>Utilize automated tools and their artifacts as deliverables, including automated test reports of code coverage and security.</li> </ul>"},{"location":"program-strategy/Contracting%20and%20Acquisition/#cost-estimates","title":"Cost estimates","text":"<p>The software that is developed under JATIC will follow an agile development strategy which fixes contract price and time, while reprioritizing scope dynamically to best meet capability needs, and to build on successful lines of efforts while cutting unsuccessful efforts. The cost estimate for contract prices will be determined by setting a level-of-effort that the contractor should provide over the PoP, using specified hours in specified labor categories. Afterwards, the cost for contract prices will be negotiated with vendors, using the independent cost estimate and labor categories as a basis.</p>"},{"location":"program-strategy/Cost%20and%20Funding/","title":"Cost and Funding","text":""},{"location":"program-strategy/Cost%20and%20Funding/#funding-appropriation-categories","title":"Funding appropriation categories","text":"<p>JATIC funding is grouped into two primary appropriation categories: Research, Development, Test, and Evaluation (RDT&amp;E) and Operations &amp; Maintenance (O&amp;M). JATIC software products will be developed primarily with RDT&amp;E funding, while services such as enterprise cloud hosting, DevSecOps services, and system administration will be funding with O&amp;M funding. Product support of JATIC capabilities, after the primary development phase, will also be funded with O&amp;M funding.</p>"},{"location":"program-strategy/Cost%20and%20Funding/#five-year-funding-plan","title":"Five-year funding plan","text":"<p>JATIC is funded from FY23 to FY28. The program funding levels by year are specified below, as well as their line items, as required by the CAPE Program Decision Memorandum.</p> Line Item Fund Type FY23 FY24 FY25 FY26 FY27 FY28 FY23-28 Scalable AI Test Harness RDT&amp;E 6,183 9,467 9,077 8,123 8,394 8,394 49,638 Adversarial AI T&amp;E RDT&amp;E 3,356 4,008 5,204 5,048 4,937 4,937 27,490 O&amp;M 152 156 159 162 166 166 961 Data Repo, Data Service Marketplace, Ontologies RDT&amp;E 2,445 2,650 11,707 11,547 11,306 11,306 50,961 O&amp;M 457 467 476 486 497 497 2,880 AI/ML Model Card Standards RDT&amp;E 743 1,956 1,768 2,844 1,572 1,572 10,455 O&amp;M 313 320 326 333 340 340 1,972 JATIC Management O&amp;M 1,175 3,060 5,400 5,224 5,072 5,072 25,003 Total 14,824 22,084 34,117 33,767 32,284 32,284 169,360 Fund Type FY23 FY24 FY25 FY26 FY27 FY28 FY24-28 O&amp;M 2,097 4,003 6,361 6,205 6,075 6,075 30,816 RDT&amp;E 12,727 18,081 27,756 27,562 26,209 26,209 138,544 Total 14,824 22,083 34,117 33,767 32,284 32,284 169,360"},{"location":"program-strategy/Datasets/","title":"Datasets","text":""},{"location":"program-strategy/Datasets/#background","title":"Background","text":"<p>Within FY23, the JATIC Program is focused on developing AI T&amp;E capability for CV Classification and Object Detection. In order to provide targets for the main uses cases on which our tools should apply, we've identified four mission use cases and corresponding datasets to focus on:</p> <ol> <li>Satellite Imagery - CV Object Detection</li> <li>Medical Imaging - CV Classification</li> <li>UAV - CV Object Detection</li> <li>Autonomous Driving - CV Object Detection</li> </ol> <p>These should not preclude application to other tasks, but serve as use cases to keep in mind during development. </p> <p>For most of the above use cases, we are working with DoD partners to apply our tools into T&amp;E workflows for these mission types. For each of the above use cases, we select an open-source dataset which \"represents\" the mission to serve as our working target. These selected datasets should be used in any realisitic demonstrations.</p> <p>Below, we provide further info on the chose open-source datasets.</p>"},{"location":"program-strategy/Datasets/#selected-datasets","title":"Selected datasets","text":""},{"location":"program-strategy/Datasets/#satellite-imagery","title":"Satellite imagery","text":""},{"location":"program-strategy/Datasets/#medical-imaging","title":"Medical Imaging","text":""},{"location":"program-strategy/Datasets/#uav","title":"UAV","text":"<p>The VisDrone dataset (Zhu et al., '20) was collected by the AISKYEYE team from Tianjin University, China. The dataset contains 288 video clips with 261,908 frames and 10,209 static images, captured by various drone-mounted cameras. These frames have been manually annotated with more than 2.6 million bounding boxes of targets of interest. Scenes vary in location (14 different cities in China), environment (urban and country), objects (pedestrians, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Challenges with the dataset include: 1) long-tailed distributions, 2) small objects, 3) visually similar classes, and 4) wide scale range of objects. The dataset can be used for five different tasks, although we focus primarily on object detection in images:</p> <p>Task 1. Object detection in images Task 2. Object detection in videos Task 3. Single-object tracking Task 4. Multi-object tracking Task 5. Crowd counting</p> <p>Paper Dataset</p>"},{"location":"program-strategy/Datasets/#autonomous-driving","title":"Autonomous driving","text":""},{"location":"program-strategy/Design%20Principles/","title":"Design Principles","text":""},{"location":"program-strategy/Design%20Principles/#introduction","title":"Introduction","text":"<p>This document traces the development of a number of design principles of JATIC software.</p>"},{"location":"program-strategy/Design%20Principles/#review-of-previous-capabilities","title":"Review of previous capabilities","text":""},{"location":"program-strategy/Design%20Principles/#limitations-in-previous-capabilities","title":"Limitations in previous capabilities","text":"<p>While AI T&amp;E capabilities developed by previous DoD AI Programs have been functional, none of them are sufficient as an enterprise-wide solution. Below, we provide our observations as to what shortcomings these AI T&amp;E capabilities had. To be fair, these capabilities were not designed to be enterprise-wide solutions, but instead were developed to solve the T&amp;E problems which were particular to their AI program. With that caveat in mind, here are several key factors which have limited previous capabilities from becoming widely used and distributed across the Department:</p> <ol> <li>Difficulties in deployment</li> <li>Capabilities were difficult to deploy to environments across the DoD. Even applications which were delivered as containers were extremely difficult to deploy and orchestrate in new environments, often requiring extensive effort from cloud engineers and the development team.</li> <li>Common errors included configurations in helm charts or docker-compose files, permissions issues, and incompatible changes between software versions.</li> <li>Difficulties in standardization</li> <li>Capabilities often did not support common AI/ML model formats (e.g., TensorFlow/Keras, PyTorch, MXNET) or data ground truth formats (e.g., csv, json, or serialized formats) that are used within industry and academia. This was often because they were developed with a specific AI application in mind. This demanded model and data wrangling into different standards to use different capabilities.</li> <li>This lack of support for the common standards used in industry and academia also made it difficult for the capabilities to evolve with developments in the state-of-the-art, such as new algorithms and techniques, which often built on existing formats.</li> <li>Difficulties in integration with platforms</li> <li>Capabilities did integrate easily with the main platforms and tools through which AI programs used to develop and manage their AI. For example, the CDAO ADVANA platform uses Databricks and MLFlow for its AI, and Army AI Integration Center uses the COEUS platform. For example,\u00a0capabilities often required users to leave the platform and use another application entirely.</li> <li>Often, applications had a completely different UI, different processes for setting up data and models, etc. For example, some tools may interface through a REST API, with all experiment variables configured through a .json file. Other may be configured through a GUI, with experiment variables configured through dropdowns.</li> <li>Integration with supporting T&amp;E functionality</li> <li>All T&amp;E capabilities required a foundation of key functions in order to be effective. These functions included<ol> <li>an automation pipeline to kick off T&amp;E experiments</li> <li>a dashboard to visualize results</li> <li>an experiment tracking system to associate metrics with model runs</li> <li>a object storage database</li> <li>a model inference server</li> </ol> </li> <li>T&amp;E capabilities often provided their own bespoke solutions to the above, including their own visualization dashboards, automation pipelines, etc. But, ML pipelines often had a great deal of these capabilities built in.</li> <li>Thus, using multiple capabilities within an ML pipeline often meant spinning up multiple object storage databases, dashboards, automation pipelines etc. which was confusing and inefficient.</li> <li>Lack of maturity, polish, cybersecurity, scalability, etc.</li> <li>While capabilities could often be made to work, they almost never had the level of maturity of fully developed and productized software.</li> <li>This showed in a number of areas, including the quality of the UI, the detail of the documentation, the level of cyber-hardening, and the performance of the capabilities when used at scale.</li> <li>Lack of discovery of the capability</li> <li>A program may have had great functionality but it was not easy or possible for other teams to discover it</li> </ol> <p>The above limitations did not operate in isolation, but rather, intersected with and compounded on themselves. A Program Office developing AI that wanted to use two such T&amp;E capabilities, perhaps one for automated metrics calculation and another for adversarial robustness testing, would have to engineer the deployment of both into their environment, integrate both with their current tools and platforms, create model and data format converters to work with both capabilities (often expecting different formats), and use separate programs to interact with each (perhaps a separate GUI for one, and a command-line interface for another).</p> <p>Because of the challenges with existing capabilities, many programs developing AI created their own bespoke capabilities to address their T&amp;E gaps, limited themselves to calculating basic T&amp;E metrics (e.g., precision, accuracy, recall, F1 score, etc.), or relied on reported metrics from the model vendors. None of these solutions were satisfactory - either the AI was not sufficiently tested, or funding was suboptimally spent on the development of another tool with limited applicability and similar functionality to existing tools.</p>"},{"location":"program-strategy/Design%20Principles/#maximizing-the-effectiveness-of-te","title":"Maximizing the effectiveness of T&amp;E","text":"<p>The examination of previous capabilities indicates that the deep integration of T&amp;E capabilities with other technologies within the AI/ML lifecycle creates powerful synergistic effects that increase the effectiveness of the whole. For example, it is extremely valuable for the T&amp;E metrics computed to feed into a centralized model registry, to be logged in the experiment tracker, to be able to be used as a target for the hyperparameter optimizer, and to be able to be visualized within a dashboard.</p> <p>This effect also holds for between different T&amp;E capabilities themselves: they work best when they are able to be used in combination with each other. For example, if there are three T&amp;E capabilities for 1) automated metrics calculation, 2) adversarial attacks, and 3) explainability and saliency maps, it may be extremely interesting for a user to understand an adversarial attack on an image by understanding the change in the corresponding saliency map after the attack is produced. It may also be extremely helpful to automate the computation of adversarial attacks and explainability techniques into the metrics calculation pipeline. T&amp;E capabilities which are unable to integrate with each other and other technologies disguise the true potential of T&amp;E both in itself and as an AI-enabler.</p> <p>Palantir's Data Management Platform demonstrates a successful approach to integration: to own the entire vertical, end-to-end pipeline. This approach is not one that is practical to adopt, since it calls for the development not of a testing capability, but rather a comprehensive platform for data and ML which includes testing functionality as one component.</p> <p>It cannot be the case that JATIC, which is designed to be an enterprise-wide capability for CV model T&amp;E, also dictates every other component of the data and AI process. First of all, JATIC is not funded to do this. Second, programs offices, agencies, and organizations throughout the Department will leverage many different platforms based on their mission need. JATIC should be able to deliver T&amp;E capability to support a wide range of these organizations, no matter what larger platform they choose to use for their data storage, labeling, model training, dashboarding, model monitoring, etc.</p> <p>From all of the above, we summarize the following principles:</p> <ol> <li>JATIC cannot become an end-to-end platform that includes all functionality required for the AI/ML lifecycle; JATIC's functionalities should be limited to T&amp;E itself.</li> <li>In order to deliver value across the DoD enterprise, JATIC must be able to deploy and operate in a wide variety of environments across the Department.</li> <li>In order to maximize the effectiveness of T&amp;E for a given program developing AI, JATIC must be able to integrate into other AI/ML platforms, frameworks, and technologies used by that program.</li> </ol>"},{"location":"program-strategy/Design%20Principles/#ml-platforms","title":"ML platforms","text":"<p>For many industry organizations developing and deploying AI in production, the foundational technology stack is the end-to-end ML platform. For creating AI models in production, the actual model weights are only one small piece of the puzzle. Automation, tracking, scalability, deployment, and monitoring all emerge as difficult problems when AI must work in a production environment. ML platforms often provide solutions for the range operations necessary for the AI/ML lifecycle, including:</p> <ol> <li>Data: Data cleansing, data ingestion, exploratory data analysis, data transformation, data validation, feature engineering, data splitting, data versioning</li> <li>Models: model training, training optimization, model validation, training at scale, model registration, model storage</li> <li>Deployment: model deployment, model serving, model monitoring, model logging, model retraining</li> <li>Support: Automation, workflow orchestration, dashboarding, visualization, collaboration</li> </ol> <p>There are numerous platforms available for this purpose. Many companies have developed their own in order to train and deploy their AI models at scale, including Uber's Michelangelo, Facebook's Learner, and Airbnb's Bighead. Major solutions offered as a service include AWS Sagemaker, Azure Machine Learning, Google Cloud AI Platform, and Databricks. A great article for more information is a tour of end-to-end ML platforms.</p>"},{"location":"program-strategy/Design%20Principles/#dods-ml-platforms","title":"DoD's ML platforms","text":"<p>Our observation is that program offices, agencies, and organizations developing AI across the DoD have recently recognized the need for ML platforms. Many organizations are developing their own, uncoordinated with others. The focus among these platforms varies based on the mission need: some are focused on data science, some on model development, some on post-deployment monitoring and model retraining. Some examples across the DoD include:</p> <ol> <li>DoD CDAO - ADVANA</li> <li>\"DoD's big data platform for advanced analytics\"</li> <li>\"Advana is a centralized data and analytics platform that provides DoD users with common business data, decision support analytics, and data tools\"</li> <li>ADVANA is also gaining MLOps capabilities to support machine learning development and testing workflows.</li> <li>Joint AI Center - Joint Common Foundation</li> <li>\"The DoD\u2019s Cloud-Based AI Development and Experimentation Platform\"</li> <li>\"The Joint Common Foundation (JCF) is a secure cloud-based AI development and experimentation environment that delivers critical tools and capabilities to support the DoD\u2019s pursuit of an AI-ready force.\"</li> <li>NIWC / DIU - Project AMMO</li> <li>An MLOps platform to support rapid retraining, deployment, and monitoring of AI models on unmanned underwater vehicles.</li> <li>Air Force - Platform One</li> <li>\"Platform One (P1) is a modern cloud-era platform that provides valuable tooling, hosts\u00a0CI/CD\u00a0DevSecOps\u00a0pipelines, and offers a secure Kubernetes platform for hosting microservices.\"</li> <li>Initially focused on DevSecOps, but now getting augmented with MLOps pipelines to support AI/ML development.</li> <li>AFRL - RedForce</li> <li>A set of cloud services and capabilities to enable rapid delivery of AI.</li> <li>Army Futures Command - Project COEUS</li> <li>\"'Open Community Data Science Platform' to support autonomy and artificial intelligence requirements\"</li> <li>\"The aim of COEUS is to enable the development and deployment of AI capabilities faster than previously possible. COEUS provides a way to collect usable data\u2014often in real time as it comes in from the field\u2014curate it, use it to effectively train AI models, and deploy them quickly as threats emerge.\"</li> <li>Army PM IS&amp;A - ArcaneFire, TITAN</li> <li>\"An AI/ML pipeline ...\u00a0for model training, management, and deployment of AI/ML models.\"</li> <li>Uses Databricks, MLFlow, Delta Lake, and rsync for its ML platform.</li> <li>Army PEO IEWS - Project Linchpin</li> <li>\"Next-Generation IEW&amp;S sensors will be ineffective without a pipeline to train and deliver AI/ML. AI/ML capabilities must be continuously integrated and continuously delivered (CI/CD). Data sets should be centrally held; data ontology, categories, and labels should be standardized.\"</li> <li>\"Project Linchpin delivers an AI/MLOps \u201cPipeline as a Product\u201d (PaaP) for IEW&amp;S sensors.\"</li> <li>Army PEO C3T - AI Pathfinder</li> <li>\"PEO C3T is executing an AI Pathfinder that aims to reduce the time to develop and deploy AI-based capabilities from years down to months.\"</li> <li>\"Standardize AI-enabling IT infrastructure and DevOps/DevSecOps processes to enable smoother transition of AI capabilities from labs to warfighters.\"</li> </ol> <p>Most of these platforms use an amalgamation of open-source and commercial-off-the-shelf solutions to create their pipeline. Other platforms, like ADVANA, adopt an end-to-end solution such as Databricks as the foundation, and fill in the gaps with other capabilities.</p> <p>We hypothesize that as more program offices begin to develop AI, there will be more such platforms that emerge across the Department. Furthermore, unless the data storage and compute infrastructure become more centralized across the Department, these emerging platforms will be highly varied in their AI technology stack.</p> <p>Thus we encounter another principle:</p> <p>In order to deliver value across the DoD enterprise, JATIC must be able to integrate into a wide variety of AI/ML platforms and technologies, including those that are currently used across the Department and those that will be adopted in the future.</p>"},{"location":"program-strategy/Design%20Principles/#ml-platforms-and-te-capability-integration","title":"ML Platforms and T&amp;E capability integration","text":"<p>The central functionalities which are provided by ML platforms which are critical for T&amp;E include automation, visualization, tracking, and orchestration at scale. This is worth noting.</p> <p>T&amp;E capabilities, including JATIC, must</p> <ol> <li>be able to be automated within a model delivery pipeline</li> <li>have results that can be visualized and interactively explored within a visualization dashboard</li> <li>have results that can be tracked to compare models with each other and over time</li> <li>be able to scale efficiently</li> </ol> <p>These functions are necessary for a successful T&amp;E pipeline, but T&amp;E capabilities should not necessarily seek to create their own competing implementations. Many ML platforms already provide such functionalities, and one would need to invest a great deal of money to create a competing platform that could convince another to move away from Databricks, Sagemaker, or the bespoke AI/ML solution which has been heavily invested in. Besides comprehensive ML platforms, other technologies also provide general functions for the above, such as Apache Airflow for workflow automation.</p> <p>Instead of recreating functionalities for automation, visualization, and tracking, JATIC should be designed to modularly interoperate with existing ML platforms and technologies that offer these critical supporting functions.</p> <p>Next emerge the questions of implementation: How is it that JATIC should design its capabilities such that they can integrate into a wide variety of the above ML platforms? Is such a design even feasible? What are the drawbacks and limitations to such a general design?</p> <p>We propose python libraries as a software form factor that allows wide integration with ML platforms and technologies. The following observations support provide support.</p> <ol> <li>A common interface throughout nearly all of the ML platforms is python code through various types of interactive python notebooks.</li> <li>Orchestration engines, which create the pipelines between modular components of an ML platform, widely support python code components.</li> <li>ML models and ML technologies are often interacted with through python code.</li> </ol> <p>In addition to software form factor, JATIC functions should be designed with intelligent interfaces to allow wide compatibility with the above technologies.</p> <p>JATIC software will be designed as python libraries with interfaces that are widely compatible with common ML platforms, model frameworks, data formats, and enabling technologies.</p>"},{"location":"program-strategy/Design%20Principles/#two-critical-dimensions-of-users","title":"Two critical dimensions of users","text":"<p>We identify two critical dimensions of JATIC users along which create greatly different requirements and methods of value delivery.</p> <ol> <li>Platform-locked vs Platform-free - This dimension refers to the level of commitment that a user has to a certain ML platform. A platform-locked user may be part of an organization that has already committed to using certain platforms for their data and ML. While we note above that many DoD organizations are adopting ML platforms, a large amount of the AI being developed and tested within the Department is still done ad-hoc, without a platform-level solution. These users may not be committed to any fixed technologies.</li> <li>Tech-savvy vs not tech-savvy - This dimension refers to the level of technical competence of a user. A tech-savvy user is one who is at least familiar with python, jupyter notebooks, and reading technical documentation.</li> </ol> <p>We believe that these two distinctions are some of the most important among potential users of not only JATIC software, but of AI/ML technology across the DoD. When wanting to use any given AI/ML technology out there (for example, take any technology on https://landscape.lfai.foundation/) the two preeminent questions are:</p> <ol> <li>Does the technology exist on the cloud platform or environment on which work is done? This is especially important in classified environments.</li> <li>Does the user have the technical experience and know-how to use the technology?</li> </ol> <p>Below, we present examples of a user persona within each quadrant of these two dimensions and some things they may care about within a T&amp;E capability, ranked from most important to least important.</p> <ul> <li>Savvy, Platform-locked</li> <li>Ex: Databricks ML developer</li> <li>Wants tools that usable seamlessly with their ML platform</li> <li>Wants tools that give deep new model insights, including complex and technical T&amp;E functions</li> <li>Wants tools that can integrate flexibly into a number of ML technologies</li> <li>Wants tools that greatly simplify previously tedious processes</li> <li>Non-savvy, Platform-locked</li> <li>Ex: Non-technical test engineer whose T&amp;E data lives on ADVANA must use ADVANA's Databricks pipeline</li> <li>Wants tools that are simple to setup and use</li> <li>Wants tools that usable seamlessly with their ML platform</li> <li>Wants tools that enable easy, automated, and documented T&amp;E</li> <li>Savvy, Platform-free</li> <li>Ex: Test engineer who is comfortable with python but has not used an enterprise ML platform</li> <li>Wants tools that give deep new model insights, including complex and technical T&amp;E functions</li> <li>Wants tools that can integrate flexibly into a number of ML technologies</li> <li>Wants tools that greatly simplify previously tedious processes</li> <li>Non-savvy, Platform-free</li> <li>Ex: Non-technical test engineer who's organization does not have an enterprise ML platform. T&amp;E data lives on S3 bucket and compute performed on an on-prem cloud</li> <li>Wants tools that are simple to setup and use</li> <li>Wants tools that enable easy, automated, and documented T&amp;E</li> </ul> <p>We hypothesize that the platform-locked groups make up most of our large customers, while many smaller organizations are platform-free. Successful delivery and support for these two groups will look different, since an integration with a large customer's platform (e.g., ADVANA) may expose our capability to many users at once and may require large agreements, while smaller groups without platforms will be more ad-hoc.</p>"},{"location":"program-strategy/Design%20Principles/#limitation-non-savvy-users-technical-complexity","title":"Limitation: Non-savvy users &amp; technical complexity","text":"<p>The most important consideration for non-savvy users is ease of setup and use. This is fundamentally in tension with the design of not only JATIC (as python libraries), but also with nearly all AI/ML technologies, which use code as the primary interface.</p> <p>While T&amp;E itself is not an activity that fundamentally requires code (one can perform T&amp;E through the selection of models, datasets, and parameters within dropdown menu GUIs), a code interface greatly enhances the flexibility, power, and integrations that T&amp;E capabilities can have. ML model development is similar - parameter configuration and model training could be accomplished in a GUI (select model architecture, weights, training step, etc. and train), but the python form factor adds much more flexibility. Lack of technical familiarity, at least with python, is a hurdle that the DoD will need surmount in order to increase its effectiveness in AI development, testing, and deployment.</p>"},{"location":"program-strategy/Design%20Principles/#guis","title":"GUIs","text":"<p>The natural response to support the non-savvy userbase is to create a comprehensive GUI-based application from which T&amp;E capabilities can be used. We think this is correct, but it is extremely important how the design of this GUI-based application is undertaken.</p>"},{"location":"program-strategy/Design%20Principles/#difficulties-in-developing-gui-based-applications","title":"Difficulties in developing GUI-based applications","text":"<p>It is easy to develop a GUI-based application which sacrifices all of the other principles and requirements noted in this document, such as integration with other technologies, ease of deployment, and automation. Additionally, developing a successful GUI is a deceptively difficult endeavor. There is little that is more off-putting than a badly designed GUI. The use of several tools, each with their own independent GUI design, may be incongruent and jarring. A GUI is much more difficult to keep polished and updated than code. A GUI which spans capability of multiple developers requires much more integration effort than code integration. A GUI should be developed in close coordination with the end-users, to ensure that it aligns with their workflow. Finally, GUI development is generally expensive.</p>"},{"location":"program-strategy/Design%20Principles/#advantages-of-interactive-interfaces-for-te","title":"Advantages of interactive interfaces for T&amp;E","text":"<p>On the other hand, many T&amp;E tools greatly increase in richness when paired with an interactive interface. For instance XAI saliency maps are much more useful as an interactive display, where the user can tweak parameters and see the changes to the heat map on an image, rather than a series of static images.</p> <p>A solution which provides an interactive interface while avoiding many of the difficulties above involves the usage of a library that can build GUIs in pure python and embed within python notebooks. This approach has the following advantages:</p> <ol> <li>allows the GUI to be usable within many environments such as Databricks directly within experimentation notebooks, without an additional web server</li> <li>provides a standardized library that all vendors can use, and therefore, a standardized GUI experience</li> <li>makes GUI creation and iteration extremely easy, even for those without any training in front-end development</li> <li>expands T&amp;E tools by providing interactivity and immediate feedback</li> </ol> <p>We propose the use of Gradio (https://www.gradio.app/). While many applications can be used to develop web-based UIs (see https://towardsdatascience.com/gradio-vs-streamlit-vs-dash-vs-flask-d3defb1209a2), Gradio's ease of UI development and ability to embed in Jupyter notebook outputs makes it most appropriate for our usage.</p> <p>Each T&amp;E capability will develop a series of widget in Gradio that allow for the usage of the capability's underlying functions in an interactive way. For example, an XAI saliency capability would develop a widget that would allow the user to provide choose a model, saliency technique, image, and parameters as inputs, and display the saliency map as an output. The inputs could be tweaked interactively, for instance, numerical inputs on a slider bar.</p> <p>We propose that a more extensive GUI-based application is developed later, after the foundational code capabilities have been developed and we have a concrete group of users.</p>"},{"location":"program-strategy/Design%20Principles/#limitation-platform-free-users-lack-of-functionality","title":"Limitation: Platform-free users &amp; lack of functionality","text":"<p>We noted above that automation, tracking, and visualization are critical components for any AI T&amp;E pipeline. We also noted that T&amp;E capabilities should not attempt to create this functionality, since it exists within many ML platforms or other ML technologies, and the functionality often spans beyond that of T&amp;E. However, since we propose that JATIC does not provide this functionality, the JATIC does not reach full potential for platform-free users until this functionality is obtained. This makes JATIC a difficult product to adopt for platform-free users, since the tools seem disparate (rather than a single T&amp;E solution) and lacking in core functionality such as experiment tracking. Giving users the ability to plug in their own solutions increases interoperability, but also increases complexity, especially for users who are unfamiliar with the space of solutions.\u00a0\u00a0</p>"},{"location":"program-strategy/Design%20Principles/#jatic-preferred-platform","title":"JATIC preferred platform","text":"<p>For this group, it may also be the case that a single, unified T&amp;E platform may greatly simplify and standardize ML testing and development. To these users, JATIC can be delivered as a bundle, with an industry standard automation, tracking, and artifact storage solution, as the JATIC preferred platform. This is the position the position that was previously fulfilled by standalone, GUI based T&amp;E tools, which we called \"Tier 2\" tools (as opposed to \"Tier 1\" tools, which were python libraries).</p> <p>This unified T&amp;E platform also works well in conjunction with the T&amp;E GUI as a single solution that can be easily adopted by non-savvy, platform-free users for comprehensive T&amp;E.</p>"},{"location":"program-strategy/Design%20Principles/#limitation-design-within-constraints-of-ml-platforms","title":"Limitation: Design within constraints of ML platforms","text":"<p>It is obvious that one has more control in capability design when one designs all components of the capability, rather than requiring integrating into existing capabilities. In outsourcing many critical T&amp;E functionalities to other platforms and technologies, JATIC takes on a large number of design constraints. A central challenge of JATIC will be to create powerful T&amp;E capabilities within this smaller design space.</p> <p>An example: ML platforms like MLFlow and Sagemaker provide dashboards for visualization of model metrics. Are these sufficiently flexible to allow visualization of explainable AI saliency maps? To visualize interactive performance curves? To explore examples of model misclassifications? If not, what is a solution that does not compromise ease of integration and deployment?</p>"},{"location":"program-strategy/Documented%20Need/","title":"Documented Need","text":"<p>The Joint AI T&amp;E Infrastructure Capability (JATIC) Program is developing an interoperable set of state-of-the-art software capabilities for AI algorithm T&amp;E. It aims to provide a provide a comprehensive suite of integrated testing tools which can be deployed widely across the enterprise to address key T&amp;E gaps while maintaining broad compatibility to common AI frameworks. JATIC will include capabilities for the T&amp;E of AI model performance, cybersecurity, adversarial resilience, robustness, explainability, and bias which will operate in the Computer Vision (CV) modality.</p> <p>The capability need and requirement for JATIC derives from the</p> <ol> <li>Responsible AI Strategy and Implementation Pathway (RAI S&amp;I Pathway), issued by Deputy Secretary of Defense Kathleen Hicks in June 2022</li> <li>The National Artificial Intelligence Test &amp; Evaluation Infrastructure Capability (NAITIC) Interim Report, issued by CDAO in July 2022</li> </ol>"},{"location":"program-strategy/Documented%20Need/#rai-si-pathway","title":"RAI S&amp;I Pathway","text":"<p>The RAI S&amp;I Pathway documents a series of specific near-term steps for the implementation of the Department's AI ethical principles and Responsible AI Foundational Tenets. Tenet 2 of the S&amp;I Pathway is Warfighter Trust and clearly defines the capability need that JATIC seeks to address. In particular, Tenet 2 charges CDAO T&amp;E, in coordination with OUSD(R&amp;E) and DOT&amp;E, to</p> <pre><code>LOE 2.1: BUILD A ROBUST TEVV ECOSYSTEM AND ACCOMPANYING INFRASTRUCTURE TO DEVELOP AND FIELD AI CAPABILITIES SAFELY AND SECURELY.\n\nLOE 2.1.1: Develop a TEW framework to articulate how test and evaluation (T&amp;E) should be intertwined across an Al capability's lifecycle and pathways for continuous testing and standards for documentation and reporting. Identify synergies between AI T&amp;E and traditional T&amp;E (e.g. effectiveness, suitability, security, safety) to empower programs to streamline T&amp;E efforts. Include guidance for operationalizing RAT principles into testable conjectures for common technologies, mission domains, and uses cases.\n\nLOE 2.1.2: Develop or acquire Al-related T&amp;E tools to be used as a resource for Al developers and testers. This AI T&amp;E Toolkit shall draw upon best practices and innovative research from industry and the academic community, as well as commercially available technology where appropriate. The Toolkit will be made widely available to DoD users and shall include:\n\n1. Tangible, concrete guidance for PMs, testers, and other relevant T&amp;E stakeholders about how to implement RAI T&amp;E throughout a capability's lifecycle;\n2. T&amp;E Master Plan template for AI and a set of templates for test plans;\n3. A library of T&amp;E metrics for AI systems, including metrics for trustworthiness and confidence; and\n4. Necessary tools and technologies required to detect both natural degradation of and adversarial attacks on Al, including those to detect various attacks on AI systems, and that can notify testers or operators when such attacks are occurring.\n\nLOE 2.1.3: Create a test range environment and central repository of tools for T&amp;E of Al, linking to existing and emerging equivalent DoD Component environments, that enables easy and continuous testing for DoD testers. Where appropriate, tools that are housed in this environment should comply with DoD Enterprise DevSecOps Reference Designs for portability across the Department.\n</code></pre>"},{"location":"program-strategy/Documented%20Need/#naitic-interim-report","title":"NAITIC Interim Report","text":"<p>The NAITIC Interim Report was developed in response to the FY23 Program Decision Memorandum, in which the Deputy Secretary of Defense and CAPE requested more information to support data-driven decisions about AI T&amp;E infrastructure investments at the DoD enterprise level.</p> <pre><code>Program Decision Memorandum Terms of Reference (TOR)\n\nAI T&amp;E capability is critical for responsible development and fielding of autonomous and artificial intelligence systems. DoD must target investments in this area at key gaps, leverage existing infrastructure in government, industry, and academia where possible, and ensure alignment of T&amp;E investments at all levels\u2014from the programs up to the enterprise\u2014with a coherent vision. This study is essential to inform the decision space for AI T&amp;E investments, and the TOR outlines criteria that will be used to evaluate the roadmap\u2019s utility in this regard\n</code></pre> <p>The NAITIC Interim report concluded that \"there is widespread interest for DoD enterprise-level T&amp;E infrastructure to address the novel and exacerbated challenges posed by the T&amp;E of AI. While programs are currently investing locally in T&amp;E resources\u2026 there is still a consistent desire across survey programs for DoD enterprise support.\"</p> <p>The JATIC program was specifically designed to meet the capability needs set forth in these documents, and other AI T&amp;E requirements throughout the Department.</p>"},{"location":"program-strategy/Intellectual%20Property%2C%20Open-source%2C%20Commercial/","title":"Intellectual Property, Open-source, Commercial","text":""},{"location":"program-strategy/Intellectual%20Property%2C%20Open-source%2C%20Commercial/#intellectual-property-strategy","title":"Intellectual property strategy","text":"<p>The Government shall have access to have ownership\u00a0of the source code created within JATIC contracts.\u00a0Exceptions to this policy can be made on a case by case basis and will be clearly spelled out in the contract award. At a minimum, the Government shall have access to all of the source code created within JATIC contracts to support future analysis and use.</p> <p>Contracts shall stipulate that proprietary software not be included in the delivery, or required for full functionality without Government approval. In the case that the software must integrate with proprietary software after Government approval, the software must be developed in a manner which preserves the modularity and independent functionality of the non-proprietary component.</p> <p>Software source code, along with all additional artifacts necessary to compile, test, secure, deploy, and operate the software shall be delivered on a regular basis, as agreed upon within the Performance Work Statement. Software also shall be delivered with appropriate documentation, including setup guides, reference architectures, interface control documents, and example use cases.\u00a0Additionally, deliveries shall specify technical information on the development, build, and operation of the software, including requirements versioning, dependencies, development environment, build automation software, etc. A precise list of required information with in each software delivery shall be listed in the Software Development Plan.</p>"},{"location":"program-strategy/Intellectual%20Property%2C%20Open-source%2C%20Commercial/#open-source-solutions","title":"Open-source solutions","text":"<p>Software designed for T&amp;E requires transparency of code implementation in order to foster trust in the computed results. Thus, the software solutions developed to support JATIC must have source code which is at least openly accessible by the Government.</p> <p>JATIC will leverage existing open-source solutions as a starting place for some of its components, especially when the open-source components are widely used and recognized. In these cases, JATIC will provide providing productization, maturity, transition, and deployment on top of the existing solution in order provide a mature product across the Department. JATIC will strive to make many of their capabilities publicly available.</p>"},{"location":"program-strategy/Intellectual%20Property%2C%20Open-source%2C%20Commercial/#commercial-solutions","title":"Commercial solutions","text":"<p>JATIC will leverage existing commercial solutions, insofar as the solutions meet the above capability requirement of code accessibility for the Government. JATIC software will generally avoid commercial solutions offering T&amp;E capacities \"as a service\" in order to maintain transparency of code.</p>"},{"location":"program-strategy/personas/","title":"Personas","text":""},{"location":"program-strategy/personas/#roles-target-user-types","title":"Roles - Target user types","text":"<p>The software developed within our program will support a number of different types of users within the AI Lifecycle. We call types of users, \"roles\". These are documented in the Roles.md document within this repo.</p>"},{"location":"program-strategy/personas/#roles-vs-personas","title":"Roles vs Personas","text":"<p>Personas are specific manifestations of the roles into concrete individuals.  Each persona should be associated with a role.</p> <p>Our personas, with their particular background, missions, and operating environments, help provide a clear perspective and unique voice for the different types of customers that we are supporting. </p> <p>Ideally, the overall target user base should be represented within our personas. That is, our personas span a broad range of the AI use cases, mission environments, and technical backgrounds which we would like our tools to support. </p>"},{"location":"program-strategy/personas/#organization-of-documents","title":"Organization of Documents","text":"<ul> <li>All roles desired for personas should be documented in the Roles.md document. Currently, these roles are fixed across the program. If it becomes necessary to have different definitions of roles for different teams, raise the issue for discussion.</li> <li>Program personas are found in the folder Program_Personas. These personas are owned and maintained by the program. These are held in Mural and linked to from Program_Personas/program_personas.md.</li> <li>Personas owned and maintained by product development teams can be found in the folder Product_Team_Personas. Each personas should have the name of the team owning and maintaining that persona in the name of the file.</li> </ul>"},{"location":"program-strategy/personas/Roles/","title":"Roles","text":"<p>The software developed within our program will support a number of different types of users within the AI Lifecycle. We call types of users, \"roles\".</p> <p>We identify the following as different roles we may support. Other roles may be identified. We will support roles by creating personas representing the role, creating workflows for the personas, and creating tools to support the persona and workflow. Roles should be defined in this document and then used in personas. </p> <ol> <li>AI T&amp;E Engineer</li> <li>ML Developer</li> <li>Data Analyst</li> <li>AI Red Teamer</li> </ol>"},{"location":"program-strategy/personas/Roles/#1-ai-te-engineer","title":"1. AI T&amp;E Engineer","text":"<ul> <li>Measure performance of models within various settings and contexts</li> <li>Compare performance of models</li> <li>Verification of requirements satisfaction and validation of evaluation results</li> <li>Quantitative evaluation of risks</li> <li>Explore potential unknown risks</li> <li>Develop summary report of findings</li> <li>Not a developer of the system</li> </ul>"},{"location":"program-strategy/personas/Roles/#2-ml-developer","title":"2. ML Developer","text":"<ul> <li>Exploratory data and model analysis</li> <li>Statistical analysis</li> <li>Model experimentation</li> <li>Feature engineering</li> <li>Model training and continuous fine-tuning</li> <li>Uses MLOps, end-to-end model pipelines</li> </ul> <p>See Data Scientist and ML Engineer in [1], and Data Scientist in [2] and [3] for references and a general job description. We're calling this ML Developer, since data scientist is easily confused with the following role. </p>"},{"location":"program-strategy/personas/Roles/#3-data-analyst","title":"3. Data Analyst","text":"<ul> <li>Data visualization and analysis</li> <li>Statistical analysis of dataset properties</li> <li>Data set selection, curation, and validation</li> <li>Data preparation and cleaning</li> </ul> <p>See Data Analyst in [3] and [4] for references and a general job description. Note that there is the potential for confusion with IC-background people, since analyst often means something different in that context.</p>"},{"location":"program-strategy/personas/Roles/#4-ai-red-teamer","title":"4. AI Red Teamer","text":"<ul> <li>Perform adversarial assessments of AI models</li> <li>Perform threat analysis </li> <li>Find system failure models</li> <li>Build proxy models to approximate real model performance</li> <li>Often possess limited model information</li> </ul>"},{"location":"program-strategy/personas/Roles/#references","title":"References","text":"<ol> <li>AWS Whitepapers ML Roles</li> <li>Red Hat ML Roles</li> <li>Domino ML Roles</li> <li>AWS Well-Architected ML Roles</li> </ol>"},{"location":"program-strategy/personas/User%20Engagement/","title":"User Engagement","text":""},{"location":"program-strategy/personas/User%20Engagement/#introduction","title":"Introduction","text":"<p>Frequent user engagement helps our program understand the mission tasks, needs, and interests of the wider DoD community - enabling us to better build products that meet their needs.</p> <p>This document covers the following topics: - The program's high-level strategy for user engagement - Key user-engagement categories and mechanisms for engaging different categories of users - Key responsibilities for user engagement</p>"},{"location":"program-strategy/personas/User%20Engagement/#high-level-strategy-for-user-engagement","title":"High-level Strategy for User engagement","text":"<p>Our user engagement strategy has the following objectives:</p> <ul> <li>Identificication of priority AI domains, mission use cases, and T&amp;E requirements across the DoD</li> <li>Identificication of the above for key JATIC user groups</li> <li>Promote clear traceability of these requirements from different users and organizations in our capability development process.</li> <li>Supporting active collaboration between tool users &amp; the JATIC team to create a consistent feedback loop that allows us to ensure our delivered capabilities address priority needs across the DoD. </li> <li>Developing, maintaining, and utilizing long-term partnerships with key JATIC users.</li> </ul> <p>Each engagement will broadly involve these tasks: 1.  Sourcing and connecting with users (current or potential). 2.  Sharing the high-level vision for the JATIC program. 3.  Identifying T&amp;E problems users are facing now or that they\u2019re anticipating as they look to increase their adoption of AI.  4.  Describing/pitching the JATIC tool, tools, or entire toolbox that might solve those problems that have been identified. 5.  Supporting users as they adopt our tools &amp; capabilities. 6.  Continued conversations with users to receive input on other features they desire that can help shape development for existing tools or provide demand signals for additional tools that should be built into the JATIC program.</p>"},{"location":"program-strategy/personas/User%20Engagement/#key-user-engagement-categories","title":"Key User Engagement Categories","text":"<p>The JATIC program is targeting a diverse range of key users for engagement that includes stakeholders across the government, as well as non-government entities in academia and industry given the open-source nature of our tools. Below, please find a short description of these different categories of users and general recommendations for engaging with these audiences.</p> <p>Specific goals for targeting priority users or user engagement will be identified each increment and shared as part of the increment planning process.</p> Government: DoD &amp; their contractors  Government employees and contractors for the Department of Defense include many categories of users that it will be valuable for JATIC to engage with throughout the program.  Engaging with these users throughout the lifecycle of JATIC capability development &amp; deployment directly feeds into our mission of producing software to accelerate and enable AI T&amp;E for DoD testers and increase the safety, effectiveness, and robustness of the DoD\u2019s AI-enabled systems.  DoD government user types we target for engagement might include, but are not limited to: - AI Test &amp; Evaluation Engineers   - Persona 1: Alice: Basic AI T&amp;E Engineer doing T&amp;E (e.g. for unmanned ground vehicles)   - Persona 2: Jay: Advanced AI T&amp;E Engineer doing T&amp;E (e.g. on satellite imagery)   - The Alice &amp; Jay [personas](https://app.mural.co/t/mitresandbox0478/m/mitresandbox0478/1693165466374/3484fce186f84fee231e1e584555cdcf7bc2f552?sender=c82d7e46-ee27-40a3-9bec-1ebf55573b17) can expand to include a multitude of AI tasks and domains requiring T&amp;E.   - Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or the identification of other priority AI tasks/domains relevant to the DoD that we should consider for future development.  - ML Developers/Researchers     - [Persona 3](https://app.mural.co/t/mitresandbox0478/m/mitresandbox0478/1693165466374/3484fce186f84fee231e1e584555cdcf7bc2f552?sender=c82d7e46-ee27-40a3-9bec-1ebf55573b17) \u2013 Fred: Manual ML Developer who needs to verify and validate models before starting field tests, create tests, and create reports on the best performing models.   - Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or identifying other priority AI tasks/domains relevant to the DoD that we should consider for future development.  - Leadership/Program Managers that are currently employing AI   - These individuals will be looking for AI T&amp;E capabilities that they can incorporate into existing programs. Unlikely to be users themselves, this audience will be searching for accessible tools and programs that will be easy to integrate with their technical teams.   - Engagement with individuals in this group should focus on identifying problems they/their team are facing and presenting or developing our tools as solutions to those problems.  - Leadership/Program Managers that are interested in employing AI in the future   -These individuals are looking to improve their situational awareness of AI T&amp;E problems and solutions that might apply to future AI-enabled systems they intend to use.   - Engagement with individuals in this group should focus on identifying problems they &amp; their team are facing with regards to AI adoption and sharing how our tools can help.  Mechanisms for engagement:  - Try to gather information about the attendees and their experience level before you meet with these users when possible. Be prepared to present the JATIC overview information ([Distribution C deck](https://gitlab.jatic.net/jatic/docs/presentations/-/blob/master/briefs/JATIC_Overview.pptx?ref_type=heads)) to teams without previous knowledge of JATIC. Be prepared to speak to tools &amp; capabilities within JATIC that might be of particular interest given the user needs. This might include inviting others from the program to join your meeting or setting up follow-up meetings with specific JATIC teams including additional technical folks from your tool development team, other tool product owners, or members of the JATIC Program Management team. - Invite users to stay engaged with the program by joining the [GitLab](https://gitlab.jatic.net/) using their government/FFRDC/UARC email account, attending future quarterly demonstrations or weekly deep dives, filling out the [intake form](https://forms.osi.apps.mil/pages/responsepage.aspx?id=kQEtEK7uYUexyxqD6G70RffprThOa3hKghVjeesZps5UN1hXMDUxN1dXM0c3SUJZVE1FMU1PTDAyQi4u) to join our email list, or attend future events hosted by CDAO JATIC. - Some engagements might progress to the point of needing official government-to-government agreements between CDAO &amp; the DoD office for specific partnership on a specific goal/mission (e.g. signing an MOU with an organization that we will provide their T&amp;E tools). Keeping the JATIC team apprised of outreach and engagements you have with DoD organizations using the [Partnerships Group in GitLab](https://gitlab.jatic.net/cdao/partnerships) is essential to ensure those select needs are identified early. - Ask these users if they have upcoming working group meetings, conferences, or other events that JATIC could attend and present at.   Government: Non-DoD &amp; their contractors  Employees and contractors who work within the government, but outside of DoD, include many similar users that can be found within the DoD category above.  Engaging with these non-DoD users allows us to expand our awareness of mission use cases, unique environments, and challenges that our capabilities can address in the use of AI-enabled systems across the US government. Input and feedback generated from these engagements can assist with refining our capabilities for maximum impact and expanded reach, as well as allow us to source demand signals for future capabilities that will also impact the DoD.   Non-DoD government user types we target for engagement might include, but are not limited to:  - AI Test &amp; Evaluation Engineers   - Persona 1: Alice: Basic AI T&amp;E Engineer doing T&amp;E (e.g. for unmanned ground vehicles)   - Persona 2: Jay: Advanced AI T&amp;E Engineer doing T&amp;E (e.g. on satellite imagery)   - The Alice &amp; Jay [personas](https://app.mural.co/t/mitresandbox0478/m/mitresandbox0478/1693165466374/3484fce186f84fee231e1e584555cdcf7bc2f552?sender=c82d7e46-ee27-40a3-9bec-1ebf55573b17) can expand to include a multitude of AI tasks and domains requiring T&amp;E.   - Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or the identification of other priority AI asks/domains relevant to the DoD that we should consider for future development.  - ML Developers/Researchers     - [Persona 3](https://app.mural.co/t/mitresandbox0478/m/mitresandbox0478/1693165466374/3484fce186f84fee231e1e584555cdcf7bc2f552?sender=c82d7e46-ee27-40a3-9bec-1ebf55573b17) \u2013 Fred: Manual ML Developer who needs to verify and validate models before starting field tests, create tests, and create reports on the best performing models.   - Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or the identification of other priority AI asks/domains relevant to the DoD that we should consider for future development.  - Leadership/Program Managers that are currently employing AI   - These individuals will be looking for AI T&amp;E capabilities that they can incorporate into existing programs. Unlikely to be users themselves, this audience will be searching for accessible tools and programs that will be easy to integrate with their technical teams.   - Engagement with individuals in this group should focus on identifying problems they/their team are facing and presenting or developing our tools as solutions to those problems that are also relevant to the DoD.  - Leadership/Program Managers that are interested in employing AI in the future   -These individuals are looking to improve their situational awareness of AI T&amp;E problems and solutions that might apply to future AI-enabled systems they intend to use.   - Engagement with individuals in this group should focus on identifying problems they &amp; their team are facing with regards to AI adoption and sharing how our tools can help.  Mechanisms for engagement:  - Try to gather information about the audience and their experience level before you meet with these users when possible. Be prepared to present the JATIC overview information ([Distribution C deck](https://gitlab.jatic.net/jatic/docs/presentations/-/blob/master/briefs/JATIC_Overview.pptx?ref_type=heads) to teams without previous knowledge of JATIC. Be prepared to speak to tools &amp; capabilities within JATIC that might be of particular interest given the user needs. This might include inviting others from the program to join your meeting or setting up follow-up meetings that include additional technical folks from your tool development team, other tool product owners, or members of the JATIC Program Management team.  - Invite users to stay engaged with the program by joining the [GitLab](https://gitlab.jatic.net/) using their government/FFRDC/UARC email account, attending future quarterly demonstrations or weekly deep dives, filling out the [intake form](https://forms.osi.apps.mil/pages/responsepage.aspx?id=kQEtEK7uYUexyxqD6G70RffprThOa3hKghVjeesZps5UN1hXMDUxN1dXM0c3SUJZVE1FMU1PTDAyQi4u) to join our email list, or attend future events hosted by CDAO JATIC. - Some engagements might progress to the point of needing official government-to-government agreements between CDAO &amp; the DoD office for specific partnership on a specific goal/mission (e.g. signing an MOU with an organization that we will provide their T&amp;E tools). Keeping the JATIC team apprised of outreach and engagements you have with government organizations using the [Partnerships Group in GitLab](https://gitlab.jatic.net/cdao/partnerships) is essential to ensure those select needs are identified early. - Ask these users if they have upcoming working group meetings, conferences, or other events that JATIC could attend and present at.  Non-Government: Academia  The JATIC program is designed to ensure that best practices and innovative research from the academic community are being brought into the DoD\u2019s AI T&amp;E capabilities.  Engagement with the academic community throughout the development and deployment of AI T&amp;E capabilities is important to ensure the JATIC program continues incorporating cutting edge research for our government clients. In addition to sourcing feedback and new research directions from academic partners, we also want to provide access and tooling that will benefit their communities and research through our open-source releases.   Academic user types we target for engagement might include, but are not limited to:  - AI Researchers - ML Developers - Institute Directors/program Managers - Professors teaching the new generation of AI/T&amp;E experts and the students themselves  Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or assist with the identification of other priority AI asks/domains relevant to the DoD that we should consider for future development.  Mechanisms for engagement:  - Try to gather information about the audience and their research focus before you meet with academic users if possible. Be prepared to present the JATIC overview information ([Distribution A deck](https://gitlab.jatic.net/home/frameworks/-/blob/main/CDAO_JATIC_Overview.pdf?ref_type=heads)) to provide a high-level overview of the program and its mission. Be prepared to speak to tools &amp; capabilities within JATIC that might be of particular interest given their interests, especially highlighting those tools that are already open-sourced to the public. This might include inviting others from the program to join your meeting or setting up follow-up meetings that include additional technical folks from your tool development team, other tool product owners, or members of the JATIC Program Management team. - Share our open-source tools with these audience members and encourage them to provide feedback through open-source platforms or directly to your teams. - Ask these users if they have upcoming working group meetings, conferences, or other events that JATIC could attend and present at. - Please note any engagements with academic organizations using the [Partnerships Group in GitLab](https://gitlab.jatic.net/cdao/partnerships).  Non-Government: Industry  The JATIC program is designed to ensure that best practices and innovative research from industry are also being brought into the DoD\u2019s AI T&amp;E capabilities, both through industry leadership within the program and continued engagement with the broader industry community.  Intentional engagement with industry will allow the JATIC program to fulfill its goal of transitioning best practices for T&amp;E to the DoD in order to increase maturity and useability. In addition to receiving feedback from industry partners, we also want to encourage the adoption and use of our capabilities and standards across the industry community as we continue to release our tools publicly.  Industry user types we target for engagement might include, but are not limited to:  - AI Researchers - ML Developers - AI Red Teaming Professionals  Engagement with individuals in this group should focus on getting these users to begin or maintain active involvement, provide feedback on existing tools, or assist with the identification of other priority AI asks/domains relevant to the DoD that we should consider for future development.  Mechanisms for engagement:  - Try to gather information about the audience and their research focus before you meet with academic users if possible. Be prepared to present the JATIC overview information ([Distribution A deck](https://gitlab.jatic.net/home/frameworks/-/blob/main/CDAO_JATIC_Overview.pdf?ref_type=heads)) to provide a high-level overview of the program and its mission. Be prepared to speak to tools &amp; capabilities within JATIC that might be of particular interest given their interests, especially highlighting those tools that are already open-sourced to the public. This might include inviting others from the program to join your meeting or setting up follow-up meetings that include additional technical folks from your tool development team, other tool product owners, or members of the JATIC Program Management team. - Share our open-source tools with these audience members and encourage them to provide feedback through open-source platforms or directly to your teams. - Ask these users if they have upcoming working group meetings, conferences, or other events that JATIC could attend and present at. - Please note any engagements with academic organizations using the [Partnerships Group in GitLab](https://gitlab.jatic.net/cdao/partnerships)."},{"location":"program-strategy/personas/User%20Engagement/#key-responsibilities-for-user-engagement-across-the-program","title":"Key Responsibilities for User Engagement Across the Program","text":""},{"location":"program-strategy/personas/User%20Engagement/#product-owners","title":"Product Owners","text":"<p>Product owners serve a unique role in the user engagement process for both their development team and the overall JATIC program (see \"Agile Roles &amp; Responsibilities\"). Product owner responsibilities when it comes to user engagement include, but are not limited to:</p> <ul> <li> <p>Understanding the program level vision for JATIC &amp; being able to successfully present that vision to external audiences (using the JATIC Overview Distribution A or Distribution C deck.)</p> </li> <li> <p>Advocating for your specific tool/capability to a variety of users by presenting briefs, demos, and answering questions on it.</p> </li> <li> <p>Engaging with a broad range of potential users across the DoD, other government agencies, FFRDC/UARCs, academic, and industry organizations to determine future needs and tasks that your capability can serve. This includes documentation of these external engagements in the Partnerships Group in GitLab.</p> </li> <li> <p>Leading the execution of the user feedback strategy for your team by providing guidance and expertise about the users and their operational needs to the wider development team. This includes documentation of end-user requirements and feedback for your team and the JATIC program to incorporate.</p> </li> <li> <p>Presenting the public demonstration of your tool at the end of each increment.</p> </li> <li> <p>Overseeing the creation and maintenance of partnerships and feedback loops with priority AI tasks or exemplar users for the JATIC program.</p> </li> </ul> <p>Program Management Team</p> <p>The JATIC Program Management Team\u2019s responsibilities for user engagement include, but are not limited to:</p> <ul> <li> <p>Promoting the JATIC program across key user bases &amp; identifying strategic opportunities for engagement. </p> </li> <li> <p>Lead higher-level \"organization to organization\" user engagements with key partners to establish MOUs or user agreements.</p> </li> <li> <p>Developing acquisition strategies for new JATIC capabilities based on demand signals provided by external users.</p> </li> <li> <p>Providing high-level guidance to JATIC teams about the priority use cases, exemplar users &amp; personas that should be targeted on an annual &amp; per increment basis.</p> </li> <li> <p>Identifying Product Owners who can support each tool/capability within the Program.</p> </li> <li> <p>Working with Product Owners to ensure user communities are informed in a timely manner about the program &amp; its capabilities. This includes overseeing outreach activities such as quarterly capability demos, weekly deep dives, program partnerships, etc.</p> </li> <li> <p>Maintaining awareness of external engagements with key user groups to lead next-level engagements that require more formal commitments such as MOUs or user agreements.</p> </li> <li> <p>Providing frequent summary of capabilities to DoD/CDAO leadership.</p> </li> </ul> <p>Development Teams (led by Team Lead/Scrum Master) The Development Team\u2019s responsibilities for user engagement include, but are not limited to:</p> <ul> <li> <p>Establishing the team environment, including best processes for working as a team to ensure capability delivery to a broad user base.</p> </li> <li> <p>Supporting product owners &amp; program management in external outreach conversations or events where appropriate.</p> </li> <li> <p>Reporting requests for external engagement to their product owner to ensure proper documentation.</p> </li> <li> <p>Working with the product owner to incorporate feedback regarding priority AI tasks or exemplar users for the JATIC program.</p> </li> </ul>"},{"location":"program-strategy/personas/kitware_Data_Analyst/","title":"Persona: Data Analyst","text":""},{"location":"program-strategy/personas/kitware_Data_Analyst/#role-and-background","title":"Role and Background","text":"<ul> <li>Name: Maxwell</li> <li>Role: DoD Contractor</li> <li>Education: B.S. in Biomedical Engineering</li> <li>Experience: Works primarily on medical imaging problems</li> </ul>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#mission-and-goals","title":"Mission and Goals","text":"<p>Maxwell is working on a digital pathology problem, where he is helping create a \"super\" dataset for training models on whole slide images. His hypothesis is that including multiple datasets and standardizing their formats and labels will improve model performance by increasing the size and diversity of training samples. He needs to survey existing datasets, assess their quality and applicability to the given task, and create a report on the statistics of each dataset. He may also assist in creating functions for loading in the final dataset (e.g. a Pytorch DataLoader utility).</p>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#preferences-and-challenges","title":"Preferences and Challenges","text":"<p>Maxwell needs methods for efficiently handling large volumes of data and standardizing their formats. This might include a central data store or workflows for efficiently pulling down data and visualizing dataset examples. From a data quality perspective, he also needs to ensure that there are no duplicates in the data (and across datasets), which could result in data contamination and bias model evaluation results. Finally, he may need to assess different dataset licenses (sometimes in consultation with his compliance team) to ensure that they can be used on his project. </p>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#working-environment","title":"Working Environment","text":"<p>Maxwell has a prototypical data scientist background, so he works primarily in Python using Jupyter notebooks for development, which could come from other providers (e.g. Databricks or Amazon Sagemaker). He is familiar with standard visualization libraries (e.g. matplotlib and seaborn), as well as tools for querying and analyzing data (e.g. SQL and pandas). He is cognizant of potential protected health information (PHI) constraints, although he currently plans to only work with de-identified data.</p>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#relevant-jatic-tools","title":"Relevant JATIC Tools","text":"<ul> <li>Dataset Cards</li> <li>Label Error Analysis</li> <li>Sufficiency Analysis</li> <li>Bias and Explainability</li> <li>Dataset Utilities (e.g. splitting, merging, etc.)</li> <li>Visualization</li> </ul>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#workflow","title":"Workflow","text":"<p>Link to workflow in Mural</p>"},{"location":"program-strategy/personas/kitware_Data_Analyst/#references","title":"References","text":""},{"location":"program-strategy/personas/kitware_ML_Developer/","title":"Persona: ML Developer","text":""},{"location":"program-strategy/personas/kitware_ML_Developer/#role-and-background","title":"Role and Background","text":"<ul> <li>Name: Luna</li> <li>Role: DoD Contractor</li> <li>Education: Ph.D. in Computer Science</li> <li>Experience: Extensive computer vision research background</li> </ul>"},{"location":"program-strategy/personas/kitware_ML_Developer/#mission-and-goals","title":"Mission and Goals","text":"<p>Luna is working on aircraft object detection in satellite imagery. She plans to do large-scale model training and comparison, and wants to create a model that is generalizable to operational data which may be slightly different than her training data. Some of the challenges she faces include handling imbalanced object classes (e.g. few examples of rare classes of aircraft) and a requirement to assess both the adversarial and natural robustness of her models.</p>"},{"location":"program-strategy/personas/kitware_ML_Developer/#preferences-and-challenges","title":"Preferences and Challenges","text":"<p>Luna needs access to GPU clusers where she can remotely run and monitor her experiments in a scalable and parallelizable manner. Importantly, she does not want to have to create a custom model training and evaluation framework, instead preferring to reuse existing tools or infrastructure for this (e.g. MLflow for experiment tracking). She sees value in both command line and more interactive tools that provide a UI. She also needs tools to help her with more fine-grained model assessment, e.g. discovering hidden data clusters that might be affecting model performance.</p>"},{"location":"program-strategy/personas/kitware_ML_Developer/#working-environment","title":"Working Environment","text":"<p>Luna prefers the Linux OS with tools such as VSCode for coding in Python. She is also familiar with other deep learning frameworks such as Tensorflow, as well as Jupyter notebooks for development. She works primarily with unclassified data (and sometimes CUI data).</p>"},{"location":"program-strategy/personas/kitware_ML_Developer/#relevant-jatic-tools","title":"Relevant JATIC Tools","text":"<ul> <li>ML T&amp;E Platform</li> <li>Model Analysis (e.g. saliency maps)</li> <li>Model Cards</li> <li>Metrics Computation</li> </ul>"},{"location":"program-strategy/personas/kitware_ML_Developer/#workflow","title":"Workflow","text":"<p>Link to workflow in Mural</p>"},{"location":"program-strategy/personas/kitware_ML_Developer/#references","title":"References","text":""},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/","title":"Persona: Platform Engineer","text":""},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#name-pax","title":"Name: Pax","text":""},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#quote","title":"Quote","text":"<p>I like to work with users to make sure their software and pipelines are deployed easily. I'm always keeping an eye on security and efficiency. I always look for ways to automate and simplify processes. Helping others deploy their projects can be challenging but fun.</p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#demographic-information","title":"Demographic information","text":"<ul> <li>Experience: 8 years in IT</li> <li>Education: BS in Computer Science</li> <li>Job Title: Platform Engineer</li> </ul>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#goals-and-motivations","title":"Goals and motivations","text":"<p>The platform engineer is focused on infrastructure and access. </p> <p>Regarding the platform itself, the platform engineer is in charge of maintaining the deploying the platform. This includes maintaining the GitOps deployment pipeline and the underlying infrastructure, ensuring access to the appropriate cloud resources for deployment, and ensuring continuous availability. The platform engineer also oversees security and the creation of new users and granting/maintaining their access to different aspects of the platform and infrastructure. </p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#challenges-and-obstacles","title":"Challenges and obstacles","text":"<p>One of the biggest challenges of the Platform Engineer is knowing and understanding the needs of the end users. It is a challenge to ensure that resources are made available when it is not clear what resources will be required. Another big challenge is ensuring uninterrupted availability of the platform. The GitOps approach is a great help here, but over time there will always be infrastructure maintenance challenges.</p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#behaviors-and-preferences","title":"Behaviors and preferences","text":"<p>Pax comfortable with a high degree of autonomy. They appreciate tools and technologies that allow for efficiency, automation, and scalability.</p> <p>They are comfortable working in a wide variety of environments. They understand bash scripts, python, terraform, and helm to name a few. They typically work in Linux systems but are flexible. </p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#background-and-experience","title":"Background and experience","text":"<p>Pax has a strong background in IT and systems engineering. They have deployed various platforms on cloud and local infrastructure. They have a strong Dev Ops background and are comfortabe with scalable compute such as kubernetes. They are also proficient in several programming languages, and have a deep knowledge of cloud technologies.</p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#user-story","title":"User story:","text":"<p>Pax has been tasked with deploying a user-facing Jupyterhub-based platform on which AI engineers will develop and launch models. They need to ensure that the infrastructure is set up correctly, that models can be deployed seamlessly, and that the deployment pipeline is secure and efficient. They use their knowledge of bash scripts, python, terraform, and helm to set up the environment, configure the deployment pipeline, and troubleshoot any issues that arise during the deployment process. Pax is also responsible for adding/removing userss and setting appropriate permission levels for users. </p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#working-environment","title":"Working Environment","text":"<p>Pax is a remote employee that spends much of their day logged into remote machines via their Linux laptop. They work primarily AWS GovCloud resources but are also familiar with other cloud services.</p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#relevant-jatic-tools","title":"Relevant JATIC Tools","text":"<p>Pax doesn't use JATIC tools directly but rather ensures that end users have the infrastructure required. </p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#workflow","title":"Workflow","text":"<p>The Platform Engineer workflow for deploying a Nebari platform: * Set up credentials and access to the platform cloud resources (or local as needed) * Set up the initial git repo following the guided init CLI * [Optional] modify <code>nebari-config.yml</code> as needed to add any additional resources, etc.  * Enable GitOps based on <code>nebari-config.yml</code> to ensure that auto redeploy and reproducibility are in place. </p>"},{"location":"program-strategy/personas/metrostar_quansight_Platform_Engineer/#references","title":"References","text":""},{"location":"program-strategy/personas/program_personas/","title":"Program Personas","text":"<p>Program personas are kept in Mural. Currently these are located here.</p>"},{"location":"program-strategy/personas/template/","title":"Persona: [Persona]","text":""},{"location":"program-strategy/personas/template/#name-name","title":"Name: [Name]","text":""},{"location":"program-strategy/personas/template/#quote","title":"Quote","text":"<p>I want to ...</p>"},{"location":"program-strategy/personas/template/#demographic-information","title":"Demographic information","text":"<ul> <li>Experience: [N] years in [?]</li> <li>Education: [degree in what?]</li> <li>Job Title: </li> </ul>"},{"location":"program-strategy/personas/template/#goals-and-motivations","title":"Goals and motivations","text":""},{"location":"program-strategy/personas/template/#challenges-and-obstacles","title":"Challenges and obstacles","text":""},{"location":"program-strategy/personas/template/#behaviors-and-preferences","title":"Behaviors and preferences","text":""},{"location":"program-strategy/personas/template/#background-and-experience","title":"Background and experience","text":""},{"location":"program-strategy/personas/template/#user-story","title":"User story:","text":""},{"location":"program-strategy/personas/template/#working-environment","title":"Working Environment","text":""},{"location":"program-strategy/personas/template/#relevant-jatic-tools","title":"Relevant JATIC Tools","text":""},{"location":"program-strategy/personas/template/#workflow","title":"Workflow","text":""},{"location":"program-strategy/personas/template/#references","title":"References","text":""},{"location":"sdp/","title":"Software Development Plan","text":"<p>View this repo in Gitlab Pages</p> <p>The Software Development Plan (SDP) describes the approach, standards, and expectations for the development of software within the JATIC program.</p> <p>The SDP is designed with three parties in mind: the JATIC project organizers, the developers of JATIC technologies, and the end-users of the JATIC technologies.</p> <p>For the project organizers, this document aims to to reduce the variability between JATIC projects in terms of their structure, design, quality, and their ability to inter-operate among one another. It also enables the stakeholders to update and automate the enforcement of requirements in a way that is both systematic and scalable.</p> <p>For developers, this document is meant to simplify the process of creating high-quality Python projects, and to eliminate the need to choose from the many different approaches to packaging, testing, documentation, and the like. Additionally, it will minimize the need for vendors to employ test engineers and CI/CD specialists, by providing guidance for leveraging standardized open source tools for developing robust automated test suites. Lastly, this SDP aims to make transparent and reproducible the quality-assurance and security testing procedures that the stakeholders will be employing such that these requirements can be used to directly drive the development process.</p> <p>For end-users, this document ensures that JATIC projects are maintained and tested to high standards, and are\u00a0 interoperable with one another. An end-user that becomes proficient in the APIs and documentation of one JATIC project should find it easier to become proficient with other JATIC projects. Furthermore, end-users should find that JATIC capabilities compose together elegantly in terms of both code ergonomics and in terms of performance.</p> <p>POC: @akapusta</p>"},{"location":"sdp/branch-merge-release/","title":"Branch, Merge, Release Strategy","text":"<p>JATIC software libraries will be developed and maintained as projects on GitLab. The following outlines the recommended strategy for creating branches, merging, and releasing versions of individual JATIC libraries via GitLab. For more general guidance on using GitLab within the Agile project management process, see the Gitlab for Agile page.</p> <p>Project maintainers are ultimately responsible for defining the branch, merge, and release procedures for their project. The Gitlab for Agile page describes roles within the project.  While each project should generally follow the guidance below, the procedure can be modified as needed to support the unique needs of the project. Each project's strategy should be clearly documented in the Contributing section of the project's README.</p>"},{"location":"sdp/branch-merge-release/#branch-strategy","title":"Branch Strategy","text":"<ul> <li> <p>Development on the project will be defined, assigned, and tracked using GitLab's Issues. Issues may include new features, bug fixes, updates to documentation, or development of demonstrations. Developers who have been assigned Issues will implement their updates to the code by creating and pushing commits to feature branches. All feature branches must have an associated Issue in Gitlab.</p> </li> <li> <p>JATIC projects should follow one of three common Git branching strategies: Git Flow, GitHub Flow, or GitLab Flow. Their selected strategy should be documented in the Contributing section of the project's README. Any differences from the official branching strategies should also be noted in that section.</p> </li> <li> <p>Feature branches should have an intuitive name, with the Issue number as the prefix, followed by the name of the Issue or a description of the feature (e.g., %{issue id}-%{feature}). See GitLab's guidance on branch names for more information on how to connect Issues to their corresponding branch and merge request.</p> </li> </ul>"},{"location":"sdp/branch-merge-release/#merge-strategy","title":"Merge Strategy","text":"<ul> <li> <p>Updates to the main/dev branch should only be made through GitLab's Merge Requests. When ready, the developer will open a Merge Request (MR) to merge their feature branch into the main/dev branch. If the developer would like feedback on their work before they have finished completing their changes, they should mark the MR as a draft.</p> </li> <li> <p>When creating the MR, the developer will also assign reviewers who may be other developers or repository maintainers on the project. While developers can perform review, a project maintainer must provide final approval before the MR can be merged. Explicit rules on the minimum number of reviewers and who can perform reviews can be specified on a per-project basis and enforced via GitLab settings for the project. Product Owners can make note of any suggested or required reviewers when creating the original Issue.</p> </li> <li> <p>The feature branch should not be merged until the branch is passing all tests and a project maintainer has approved the MR. Once the MR has been approved, the developer who opened the MR should merge the feature branch into the main/dev branch and the feature branch should be deleted. If the original Issue number is included as the prefix of the branch name, GitLab will automatically close the Issue when the MR is closed.</p> </li> </ul>"},{"location":"sdp/branch-merge-release/#release-strategy","title":"Release Strategy","text":"<ul> <li>Releases should be created and timed according to the desires of the team handling the repo (which includes the product owner), to keep the release version functional.</li> <li> <p>GitLab's Releases will be used to indicate the most stable, up-to-date, and thoroughly tested version of the code. Prior to a new release, tests should be added for all new functionality and bug fixes (e.g., strive for 100% coverage), and all tests should be passing. The library's documentation should also be updated to reflect any changes in the new release. At a minimum, all functions added since the last release should be properly referenced in the documentation's \"reference\" section. \"Howtos\" and \"tutorials\" should also be added to the documentation for any new functionality, when appropriate.</p> </li> <li> <p>Upcoming releases should be announced in the JATIC Slack's \"announcements\" channel.</p> </li> <li> <p>All JATIC integration tests (integration tests at the JATIC level, whichever exist at the time) should pass, in addition to your repo's internal tests. In the case of breaking integration tests, you should communicate closely with the components that break from your change to coordinate a joint release. Coordinated releases should release all necessary updates within an hour of each other.</p> </li> <li> <p>Once ready, releases should be created from the main branch if following the Git Flow or GitHub Flow strategy, or from a release branch if following GitLab Flow. Releases should be named and tagged using semantic versioning, (i.e., \\&lt;Major&gt;.\\.\\&lt;Patch&gt;). Patch is incremented to indicate bug fixes, Minor is incremented to indicate new functionality that does not change the core API, and Major is incremented to indicate compatibility-breaking API changes. <li> <p>All changes associated with each release should be documented and communicated to the broader JATIC team via changelogs, release notes, and announcements in JATIC Slack's \"announcements\" channel. One announcement for the upcoming release, one for the completed release. Individual projects can decide how to structure and maintain their project's release notes (e.g., via a \"pending release\" notes document, with bullets added from each merge request and automated CI checks), and this process should be documented in the Contributing section of their README.</p> </li> <li> <p>JATIC users or other project teams that are also building JATIC T&amp;E tools should use the latest release of any given JATIC library, and keep their code up to date with each subsequent release.</p> </li> <li> <p>Currently, the definition of \"deployment\" within JATIC can be described as uploading an official release of a JATIC library to publicly available code repositories. Once an official release has been created within GitLab, JATIC libraries can be \"deployed\" to the following initial approved target environments:</p> <ul> <li>GitHub</li> <li>PyPi</li> <li>Conda-forge </li> </ul> <p>The procedures and implementation of continuous deployment to these environments via GitLab CI is currently in progress.</p> </li>"},{"location":"sdp/coding-guidelines/","title":"Python Coding Guidelines","text":""},{"location":"sdp/coding-guidelines/#purpose","title":"Purpose","text":"<p>This document specifies the coding guidelines for coding in Python for the project. Guidelines help improve consistency across the tools created as part of the JATIC program.</p> <p>They should be: - Flexible: exceptions to adherence to the guidelines are permitted, particularly if the guideline prevents implementation of a desired feature. - Evolving: Feel free to suggest additions or changes to the guidelines.</p>"},{"location":"sdp/coding-guidelines/#python-version","title":"Python Version","text":"<p>We use the currently supported major version of Python per the python version graph.  The exact version of Python or library dependencies is not authoritatively captured in non-executable documents but captured in executable configuration or scripts.  In other words, we will declare the versions of Python that this project is targetting but rely on configuration files running in CI to enforce the list.</p> <p>According to the Python version graph, each version of Python has three distinct phases:</p> <pre><code>1. Active: During this phase, the version of Python receives regular bugfixes, security patches, and feature enhancements.\n1. Maintenance: Once a version reaches maintenance mode, it no longer receives new features or bugfixes but only critical security patches.\n1. End-of-Life: When a version reaches End-of-Life (EOL), it is no longer supported.\n</code></pre> <p>A good place to look for the current versions of Python that the project is considered Active is in the <code>pyproject.toml</code> file in a repository's root folder.  There is a <code>python</code> version listed in that file that might be considered the main version but this project considers many Python versions for compatibility.  The list of current Python versions that are considered is captured in the matrix configuration of <code>tox</code>.  Tox has a list of Python versions that our tests and builds iterate through, checking each one.</p> <p>In plain text, the list of Python version is:     - 3.8     - 3.9     - 3.10     - 3.11 where the patch level version of each of those Python versions is up to date with high and critical vulnerability notices from CVE websites such as MITRE's or NIST's.</p> <p>To determine the supported versions of Python that we should use, we generally follow the timeline and guidelines provided by the Python development community. The Python version graph (https://devguide.python.org/versions/) shows a sliding support window for different Python versions.  Scientific Python has a sliding support window graph that features many libraries.</p> <p>Our goal is to remain within the Active or Maintenance phases for all the Python versions and dependencies.  Although a strict time period or version range has not yet been selected, it should be an ongoing and reasonable security practice that is recognizable in the context of the projects and software we are using.  The exact versions will change over time and those details should not be captured in Markdown but in things that are executing in the CI/CD pipeline.</p> <p>\u2139\ufe0f There is more detail in Software Requirements</p>"},{"location":"sdp/coding-guidelines/#supportedrequired-dependencies-and-tool-versions","title":"Supported/Required Dependencies and Tool Versions","text":"<p>The general dependency strategy is the same as the Python Version section.  Dependencies have support lifecycles, sometimes no support but an open-source community and signals that a version should not be used.  Dependencies have dependencies and there is little value in documenting this graph as human readable markdown.</p> <p>The specific versions of dependencies (e.g., Python version) supported/required by your repo should be specified at the top of the repo's top-level README.md. For example, like this.</p> <p>Per requirements to use poetry to manage Python dependencies, the dependency versions should be verified by poetry by inclusion in the repo's <code>pyproject.toml</code> file. They should also be contained in a <code>.tools-version</code> to be verified by asdf in the CI/CD pipeline.</p>"},{"location":"sdp/coding-guidelines/#pep8","title":"PEP8","text":"<p>We adhere to PEP8 with two exceptions:</p> <ul> <li> <p>PEP8 suggests using a line length limit of 79, but JATIC projects have a range within which they should stay: 79-150. Based on\u00a0PEP guidance on this subject it makes sense to limit width. Also, line length should be limited consistently within a repo. Line length must be no longer than 140. We suggest something smaller like 99 or 120 as a good compromise between narrowness for side-by-side and the readability of fewer continuations.</p> </li> <li> <p>We define the <code>__all__</code> variable in each Python module. The contents of <code>__all__</code> should only be content that you want exported from the package. The <code>__init__.py</code> file should then include import wildcards. To be clear, leading underscores should be used for private variables and functions that should not be used outside the file. Objects without leading underscore and not within <code>__all__</code> are intended to be usable within other files in the package, but not exported outside the package.</p> </li> <li>Your Python modules should never use a wildcard import to import all from a module.</li> <li>Aside from that, a leading underscore is sufficient to indicate private variables, functions or methods:</li> </ul> <pre><code># Bad:\n# Contents of `my_package/second_module.py`\nfrom my_package.my_module import *  # Do not import \"*\"\n\n# Good:\n# Contents of `my_package/my_module.py`\n__all__ = [public_function]\ndef public_function():\n\u00a0\u00a0  pass\n\ndef private_function_for_package():\n\u00a0\u00a0  pass\n\ndef _private_function_within_module():\n    pass\n\n# Contents of `my_package/second_module.py`\nfrom my_package.my_module import private_function_for_package, public_function\n\n# Contents of `__init__.py`\nfrom my_package.my_module import *\n</code></pre>"},{"location":"sdp/coding-guidelines/#in-addition-to-pep8","title":"In Addition to PEP8","text":"<p>PEP8 leaves many things open. These are additional guides or choices/opinions to choose from options suggested by PEP8. python</p>"},{"location":"sdp/coding-guidelines/#types","title":"Types","text":"<p>A JATIC Python project's interfaces should leverage consistent type annotations across interfaces, which serve as concise, legible, and verifiable documentation. Publicly-facing interfaces should prioritize the use of standard-library types (e.g. <code>int</code>\u00a0 or <code>dict</code>), common third party data types (e.g. <code>torch.Tensor</code>), or structural subtypes (a.k.a. protocols as specified in PEP 544) over the introduction of custom types. An additional document that motivates use of typed interfaces\u00a0for JATIC projects will be provided. Resources for best practices for writing and maintaining type annotations can be found here and here.</p>"},{"location":"sdp/coding-guidelines/#documentation-and-docstrings","title":"Documentation and Docstrings","text":"<p>We use docstrings of widely-adopted styles that allow direct generation of documentation via Sphinx. Allowed styles are, for example, NumPy Documentation Style or Google-style docstrings. Examples in docstrings must follow the Doctest format. These docstrings must be used to autogenerate documentation with Sphinx. Internal functions do not require docstrings. Functions part of the public API require docstrings.</p> <p>Items in the <code>Parameters</code> section should include types. For a tensor-like parameter with constraints on its shape, its shape pattern should be included alongside its type information. E.g. a batch of images may appear in the <code>Parameters</code> section as: <code>batch: torch.Tensor, shape-(N, C, H, W)</code> where <code>N</code> is the batch size and <code>(C, H, W)</code> is the shape of a single image. Interfaces (functions, classes, etc.) that are part of a code base's public API should include an <code>Examples</code> section. Here is a link to example docstring.</p> <p>The numpydoc Sphinx extension can be used to generate reference documentation for the project for Numpy-style docstrings. Napoleon Sphinx extension can work for Google-style or Numpy-style. This enables Sphinx to parse and render NumPy-style documentation strings.</p> <p>Much code should be self-documented by its writing and naming, making docstrings unnecessary. For non-public-API functions, docstrings are up to the discretion of the writer.</p>"},{"location":"sdp/coding-guidelines/#quotes","title":"Quotes","text":"<p>Use double quotes \" , not single quotes '. This choice adheres to things like PEP257.</p>"},{"location":"sdp/coding-guidelines/#naming-conventions","title":"Naming Conventions","text":"<p>PEP8 has much content on naming conventions. Follow those conventions. Some notable ones for ease of reference: - Functions: <code>snake_case</code>     - Names that are visible to the user as public parts of the API should follow conventions that reflect usage rather than implementation.     - See content above on <code>__all__</code> definition on handling objects that should be public or available within the package, but not public (neither should have leading underscore; public objects should be in <code>__all__</code> and available within the package should not).     - single_leading_underscore: For internal variables or functions that should be used only within the file. - Classes: <code>CapWords</code> - Types: <code>CapWords</code> - Constants: <code>ALL_CAPS</code> - Function: <code>snake_case</code> - Packages/module names: <code>snake_case</code> - Variables: <code>snake_case</code>     - df prefix for Pandas data frames     - _file_path suffix for file path strings     - Unit suffixes for variables with non-standard units, and only for those. In particular, radians are the standard unit for angles.     - The use of explicit, descriptive data structures \u2013 e.g., named tuples, dataclasses, and xarrays \u2013 should be used to store heterogeneous data. For example, accessing latitude &amp; longitude coordinates by name \u2013 <code>coord.lat</code>\u00a0 and <code>coord.lon</code>\u00a0 \u2013 is preferred to accessing them by index as <code>coord[0]</code>\u00a0 and <code>coord[1]</code>. - Modules:   Care must be taken to distinguish those parts of a JATIC project that comprise its public Python API from those that do not. This is chiefly accomplished by leveraging a leading underscore in the names of private modules and packages. It can be helpful for developers to use an <code>_internals</code> subpackage that contains the majority of the package's implementations, and to populate public namespaces purely via imports from parts of <code>_internals</code>.</p> <pre><code># Bad\ndistance_meters = 4.0\nangle_limit = 89.0\n\n# Good\ndistance = 4.0\nangle_limit_degrees = 89.0\n</code></pre>"},{"location":"sdp/coding-guidelines/#passing-arguments-by-keyword","title":"Passing Arguments by Keyword","text":"<p>Our default behavior is to pass function arguments by keyword rather than by position, for safety, maintainability, and readability. More specifically, the following guideline applies:</p> <ul> <li>Function takes one argument: It\u2019s up to you whether to pass by keyword or by position.</li> <li>Function takes two or more arguments: Pass by keyword.</li> <li>Exceptions: No need to pass by keyword if both of the following conditions are true:<ul> <li>The respective arguments and their positions are common knowledge (e.g. the start, stop, and step arguments, but you should pass by keyword for any of its additional arguments).</li> <li>The signature of the function is very unlikely to ever change (e.g. Python built-ins like range, standard library packages, mature libraries like NumPy, but not immature libraries like TensorFlow 0.8)</li> </ul> </li> <li>Note that both of the requirements for the exception case above will rarely be met by functions from your own code base, i.e. you can most often not assume that the arguments to your function are common knowledge and that its signature is not going to change.</li> </ul>"},{"location":"sdp/coding-guidelines/#example","title":"Example:","text":"<pre><code>def foo(yaw, pitch, roll, sausages, mode):\n    pass\n# Bad:\nfoo(1.57, -0.36, -1.57, 2.0, 'eggs')  # Let's pray that the order of yaw, pitch, roll will never change!\n\n# Good:\nfoo(yaw=1.57, pitch=-0.36, roll=-1.57, sausages=2.0, mode='eggs')\n</code></pre>"},{"location":"sdp/coding-guidelines/#instance-attribute-definition","title":"Instance Attribute Definition","text":"<p>Instance attributes should all be defined in init() so that it is easy to see what instance attributes a class has:</p> <pre><code># Bad:\nclass foo:\n    def __init__(bar):\n        self._bar = bar\n\n    def func():\n        self._baz = 42  # Instance attribute defined outside __init__\n\n# Good:\nclass foo:\n    def __init__(bar):\n        self._bar = bar\n        self._baz = None\n\n    def func():\n        self._baz = 42\n</code></pre>"},{"location":"sdp/coding-guidelines/#output-strings","title":"Output Strings","text":"<p>When you need to insert values of variables into strings, don\u2019t add strings together. In Python 3, use f-strings.</p>"},{"location":"sdp/coding-guidelines/#example_1","title":"Example:","text":"<pre><code>foo = 4\nbar = math.pi\n\n# Bad:\nstring = str(foo) + \" apples and \" + str(bar) + \" radians\"\nstring2 = \"\".join([str(foo),\" apples and \", str(bar), \" radians\"])\n\n# Good (Python 3):\nstring = f\"{foo} apples and {bar:.2f} radians\"\nprint(f\"I like to eat {foo} apples\")\n</code></pre>"},{"location":"sdp/coding-guidelines/#keyword-strings-enums","title":"Keyword Strings (Enums)","text":"<p>Keyword strings should be defined as enums whenever it makes sense (and it very often makes sense). It always makes sense if the respective keyword is used more than once. Doing this prevents errors, makes the code a lot more maintainable and traceable, makes possible editor features like \u201cgo to definition\u201d, code completion, smart refactoring, etc.</p> <pre><code># Bad:\n# In foo.py\nMyFunction.add(name=\"SET_INIT_MODE\",  # Ok if only used once in code\n               SetMode(key_name=\"mode\",  # Bad because used repeatedly\n                       value=\"init\"))  # Bad because used repeatedly\n\n# Good:\n# In foo.py\nfrom enum import Enum\n\naction_params = Enum('ActionParams',\n                     names={MODE: 'mode',\n                            STATE: 'state'},\n                     type=str)\n\nmode_params = Enum('ModeParams',\n                            names={INIT: 'init',\n                                   ERROR: 'error'},\n                            type=str)\n\nMyFunction.add(name=\"SET_INIT_MODE\", # Ok if only used once in code\n                 SetMode(key_name=action_params.MODE,\n                         value=mode_params.INIT))\n</code></pre>"},{"location":"sdp/coding-guidelines/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>Argparse (tutorial) is the standard for defining command line interfaces (CLI). Other packages for handling CLI can be used that make handling CLI easier, such as hydra-zen.</p>"},{"location":"sdp/coding-guidelines/#type-hints","title":"Type Hints","text":"<p>Always use type hints both for parameters and returns. They make the code a lot easier to understand and, more importantly, enable type checking. PEP484 is well worth a skim to get a deeper understanding of type hints.</p> <p>Type checking must be added in the CICD pipeline for your repo. PyRight is an example of type checker you can use.</p> <p>For more examples and details, see here.</p>"},{"location":"sdp/coding-guidelines/#example_2","title":"Example:","text":"<pre><code># Bad:\ndef foo_bar(data, groupby, bounds, baz, crazy_param = 42):  # Nobody knows what type these are\n    pass\n\n# Good:\nimport pandas as pd\nfrom typing import Any, Mapping, Sequence, Tuple, TypeVar, Union\n\nBoundsDict = Mapping[str, Tuple[float, float]]  # We can define type aliases\nT = TypeVar('T', pd.DataFrame, str, float, None)  # We can define arbitrary types\n\ndef foo_bar(\n        data: pd.DataFrame,\n        groupby: str,\n        bounds: BoundsDict,\n        baz: Union[int, Sequence[int]],\n        crazy_param: Any = 42,\n) -&gt; None:\n    pass\n</code></pre> <p>Some notes:</p> <ul> <li>The typing module provides support for type hints.</li> <li>The typing extensions module provides support for older Python protocols and version.</li> <li>Type hints support dynamic programming with types such as Union and Any.</li> <li>Type checking is consistent with Python\u2019s numeric hierarchy. A particularly important example of this is that int is a sub-type of float, therefore, if you want e.g. a parameter to accept both int and float, it\u2019s sufficient to annotate the parameter with the parent type float instead of Union[int, float].</li> <li>There is a difference between TypeVar and Union.</li> </ul>"},{"location":"sdp/coding-guidelines/#vectorization-numpy","title":"Vectorization / NumPy","text":"<p>Prefer vectorized operations (i.e. NumPy) over loops wherever possible. Loops in Python are very slow and therefore to be avoided for stuff like arithmetic operations if possible. Note that the <code>numpy.vectorize()</code> function itself is slow and really just a loop. See Notes section here.:</p> <pre><code># Bad:\nsum = 0\nfor i in long_list:\n    sum += i\n\n# Good:\nimport numpy as np\n\narray = np.array(long_list)\nsum = np.sum(array)  # Implemented in C and much, much faster\n</code></pre>"},{"location":"sdp/coding-guidelines/#parentheses-in-class-definitions","title":"Parentheses in Class Definitions","text":"<p>Do not add obsolete empty parentheses to class definitions if you don\u2019t inherit from anything:</p> <pre><code># Bad:\nclass MyClass():\n\n# Good:\nclass MyClass:\n</code></pre>"},{"location":"sdp/coding-guidelines/#default-argument-values","title":"Default Argument Values","text":"<p>A function signature's default values should not reflect narrow, application-specific choices. For example, it would be inappropriate for a function for training a neural network to specify the following arbitrary, unjustified default values</p> <p><code>def train(batch_size: int = 10, num_train_steps: int = 100): ...</code></p> <p>Such defaults can bias users to overlook important settings that they should determine for their application. Instead, <code>batch_size</code> and <code>num_train_steps</code> should not specify any default values.</p>"},{"location":"sdp/coding-guidelines/#mutable-default-arguments","title":"Mutable Default Arguments","text":"<p>Do not use mutable default arguments. In particular, lists and dictionaries in Python are mutable, so do not use those as default arguments. Reason: Default arguments in Python are initialized only once during the execution of the function definition code and then never again. Therefore, if a mutable default argument gets modified inside the function body, it will be modified for all future calls of that function. This short article explains the problem nicely. Two examples, the second of which is from the linked article:</p> <pre><code># Bad:\ndef foo(cookie_orders=[3, 3, 3]):\n    # Let's hope nobody modifies `cookie_orders` inside the function\n    pass\n\n# Good:\ndef foo(cookie_orders=(3, 3, 3)):\n    # No risk here, tuples are immutable\n    pass\n\n# Bad:\ndef append_to(element, to=[]):\n    to.append(element)\n    return to\n\n# Good:\ndef append_to(element, to=None):\n    if to is None:\n        to = []\n    to.append(element)\n    return to\n</code></pre>"},{"location":"sdp/coding-guidelines/#initialization-of-common-data-structures","title":"Initialization of Common Data Structures","text":"<p>When creating a new common data structure such as a list, dictionary, or tuple, use the explicitly named constructor function instead of pairs of brackets. Preference is strong when the structure is created empty. When initializing a filled list, dict, or tuple, it is better and easier to do with brackets/parentheses/braces.</p> <pre><code># Least Ok:\nnew_list = []\nnew_dictionary = {}\nnew_tuple = ()\n\n# Ok\nnew_filled_dictionary = dict(a_key=1, another_key=2)\n# Lists and tuples can also be initialized with their function names.\n\n# Good:\nnew_list = list()\nnew_filled_list = [[1, 2, 3], [4, 5, 6]]\nnew_dictionary = dict()\nnew_filled_dictionary = {\"a_key\": 1, \"another_key\": 2}\nnew_tuple = tuple()\nnew_filled_tuple = ([1, 2, 3], [4, 5, 6], [7, 8, 9])\n</code></pre>"},{"location":"sdp/coding-guidelines/#class-decorators","title":"Class Decorators","text":"<p>Class decorators should be used to identify static and class functions, but they can be used for other purposes as well. Guidance on how to use class decorators can be found online.</p>"},{"location":"sdp/design-process/","title":"Design Process Recommendations","text":"<p>Design is one component of the product development lifecycle and arguably the most important. Good design balances the needs of the user against the resources available for development and maintenance of the product. This document endeavors to list those practices that lead to good design.</p> <p>Callouts, formatted like this one, below point out the salient details and recommendations, and in some cases, firm requirements when preceded by \"Requirement\".</p>"},{"location":"sdp/design-process/#agile-vs-waterfall","title":"Agile vs. Waterfall","text":"<p>In a typical waterfall development, all planning and design happens before any building begins and typically not by those doing the work. This can be a viable strategy when essentially all requirements and expectations are known a priori but this is seldom the case. Feedback from stakeholders, unforeseen use cases, and technical advances, among other things, can all lead to new requirements and expectations.</p> <p>Agile development embraces change -- pivoting without mercy or guilt when new information becomes available. To adapt and respond to this change, the Agile development process is an iterative one. Each iteration cycle typically involves some degree of planning, design, development, testing, and review, potentially followed by deployment. Developers are directly involved in the planning and design which is done closer to the work. Feedback from one cycle affects the next, influencing the direction and priorities of the development.</p> <p>Requirement: For all the reasons mentioned, vendors working on the JATIC program will follow Agile development practices.</p>"},{"location":"sdp/design-process/#design-documentation","title":"Design Documentation","text":"<p>Given that vendors will be using Agile methods to develop their products, the need for full product specifications to be drawn up prior to implementation becomes unnecessary. However, some amount of up-front design must be considered, documented, and reviewed to ensure value to the sponsor, ease of integration, and system cohesiveness. This section describes the minimum set of recommended design documentation.</p> <p>Also note, that while these documents should be pretty fleshed out at the start of development, they are living documents and should be updated accordingly during each Agile iteration.</p> <p>Requirement: To prevent \"doc rot\", design documentation will be kept close to the source code in the product repository and will be written in Markdown.</p> <p>NOTE: I'm avoiding the more common terms \"Functional Specification\", \"High-Level Design Specification\", etc., the word \"specification\" in general, and the connotations that go along with it.</p>"},{"location":"sdp/design-process/#functional-description","title":"Functional Description","text":"<p>The functional description is not a design document per se, but provides a foundation and source of truth about the product from which all design flows.</p> <p>The functional description describes the what, why and how of the product from the user's perspective, providing a concise readout of what it offers. </p> <p>The functional description may be considered a subset of a \"concept of operations\" document. However, the operation and maintenance aspects which will be common to all vendors are not specified in the functional description.</p> <p>The functional description should answer at least the following questions:</p> <ol> <li>What is the product being built? The answer should include a general product description. For example, \"the product is a software tool for evaluating the efficacy of an adversarial pattern placed atop each image of a dataset.\"</li> <li>What are the goals and objectives of the product? In other words, what is the purpose of the product? For example, \"the product allows the user to study the affects of adversarial patterns on machine learning models trained for object detection.\"</li> <li>What is the value for the user? The answer may address these additional questions: why would the user choose this product over another? and what can this product do that other cannot?</li> <li>What are the deliverables? \"Deliverables include a Python software library which provides adversarial pattern placement and evaluation capabilities, and sample Jupyter Notebooks that demonstrate its use.\"</li> <li>How will the user interact with the product? The answer should be at a high level and not cover details of programming interfaces, such as \"the user will typically modify the contents of a provided sample Jupyter Notebook to utilize their own datasets and models. Alternatively, the user may incorporate the library into their own code and directly use its public API to apply adversarial patterns and evaluate performance of object detection models.\"</li> <li>What are the high-level requirements of the product? The requirements should be from the user's perspective. In the Agile view, these may serve as Epics or Feature Stories. \"The product must provide access to both standard models and datasets available remotely as well as local, user-provided ones.\" \"The product must let the user collect and compare results across a range of models and datasets.\"</li> <li>Are there any known, specific up-front requirements? These may be detailed requirements that are well-defined when the project starts, such as specific required features or performance metrics that must be met. Many times, these types of requirements can be associated with Agile stories as acceptance criteria.</li> </ol>"},{"location":"sdp/design-process/#high-level-design","title":"High-Level Design","text":"<p>Once a well thought out and documented functional description is available, the most important design document to be created is the high-level design.</p> <p>The high-level design describes the software architecture of the product.</p> <p>Typically the software architecture can be represented as a single diagram (with enough descriptive text) that provides a reference for anyone building the product or trying to understand how it works. Supporting diagrams such as information flows may also be appropriate, if not obvious from the architecture.</p> <p>Requirement: To facilitate change tracking, pictorial representations of the software architecture will be represented by a diagram that can be textually described and included in the high-level design document.</p> <p>Several text-based diagramming tools are available with integrated rendering in VSCode and GitLab. Popular alternatives include Mermaid and PlantUML. NOTE: Mermaid appears to be enabled in GitLab by default and may be a good choice. PlantUML is more feature-rich, but requires a separate server (or use of a public server). Here is a simple example of how a diagram would render in GitLab using Mermaid:</p> <pre><code>classDiagram\n    AdversarialAttackLib..DataSet\n    AdversarialAttackLib..Model\n    AdversarialAttackLib: patches[]\n    AdversarialAttackLib: apply()\n    AdversarialAttackLib: evaluate()\n</code></pre> <p>Besides the software architecture diagram, the high-level design should include:</p> <ol> <li>A brief description of the expected software components that comprise the product. This includes both vendor developed and third-party dependencies. For example, if the product is a Python package, the component would be a list of modules to be developed and dependent packages (NumPy, PyTorch, etc.).</li> <li>The platforms or target environment where the product will run.</li> <li>Identification of risks. This may include risks in choice of technology or due to assumptions made in the design.</li> <li>Anything else?</li> </ol>"},{"location":"sdp/design-process/#interface-design","title":"Interface Design","text":"<p>Before implementation, thought needs to be given to the interfaces of the product.</p> <p>The interface design document describes the capabilities of the user interface and any other available interfaces such as API, data providers, REST endpoints and the like.</p> <p>The interface design document is not a replacement for API documentation that can easily be generated with documentation tools, nor is it a screen capture of command line option descriptions (though those things, or links to them, can be included if available). Rather, it should make clear to users how they will work with the product as well as instruct system integrators and testers how the product will interface with other products. Additionally, the document may spell out what is expected from other products that have not yet been defined.</p>"},{"location":"sdp/design-process/#detailed-design","title":"Detailed Design","text":"<p>A detailed design delves into the particulars of the software components and how they interact. As development proceeds and things change, this type of documentation typically is incomplete at best, and misinformative at worst. The source code is truth and well written source code should be well documented if not self-documenting. See the coding guidelines for code documentation practices. For these reasons:</p> <p>No detailed design documentation is required or necessary.</p> <p>That said, it is often useful to capture some design information within Agile stories and tasks where appropriate. It helps communicate intent to the developers working on the story. However, the amount of design information and what specifically to include, if any at all, is highly dependent on the story and the capabilities of the individuals performing the work. In essence, stories are a placeholder for a conversation.</p>"},{"location":"sdp/design-process/#definition-of-done","title":"Definition of Done","text":"<p>The definition of done for some effort, whether a single issue, a research spike, an epic or even a project is a checklist of items that need to be accomplished before the effort can me truly called done. This section details suggested checklist items for certain efforts.</p>"},{"location":"sdp/design-process/#user-stories-and-technical-issues","title":"User Stories and Technical Issues","text":"<p>For a user story or feature, or for a technical issue that is not user-facing, the definition of done may include the following items (note that this may be tailored on a per-story basis):</p> <ul> <li>[ ] Source code compiles without warnings or errors</li> <li>[ ] Code linted and formatted</li> <li>[ ] Code committed to version control system</li> <li>[ ] New and existing unit tests pass</li> <li>[ ] Code coverage of unit tests is the same or better</li> <li>[ ] Code review complete, merge request approved, and code merged to main/dev branch per the branch, merge, and release strategy.</li> <li>[ ] CI/CD pipeline stages assessing code quality, security, and accessibility pass</li> <li>[ ] Functionality demonstrated and accepted by Product Owner</li> <li>[ ] Design documentation updated (particularly architecture and interface design)</li> <li>[ ] Interface documentation generated and published</li> <li>[ ] New third-party dependencies have been vetted (both for security and legal)</li> <li>[ ] New issues created to capture defects or functionality yet to be implemented</li> </ul>"},{"location":"sdp/design-process/#spikes","title":"Spikes","text":"<p>Spikes are time-bound investigations of particular focus. They may include research to learn about a new technology, prototyping to prove out a proposed solution, or conversations with other vendors or service providers. For a spike, a definition of done is much more limited:</p> <ul> <li>[ ] Findings and any decisions are documented using markdown in an appropriate Gitlab repo and disseminated</li> <li>[ ] Follow-up stories are added to the backlog</li> </ul> <p>Occasionally a spike may not have and reasonable output once the time limit is reached. In this case, the story can be re-pointed, duplicated, or abandoned as necessary.</p>"},{"location":"sdp/design-process/#epics","title":"Epics","text":"<p>An epic is a large feature, a set of features or a capability usually comprising a number of user stories. The definition of done for an epic is simply that all its constituent stories are done.</p> <p>Often integration tests or system tests should accompany an epic. Because these tests include more effort than a typical unit test, it's recommended to make them technical issues as part of the epic.</p> <p>More information about epics can before in the Agile processes document here.</p>"},{"location":"sdp/devsecops/","title":"DevSecOps Testing Requirements","text":"<ul> <li>DevSecOps Testing Requirements</li> <li>Expectations</li> <li>Capabilities<ul> <li>Code Coverage</li> <li>Unit tests</li> <li>Integration tests</li> <li>Regression tests</li> <li>Static Application Security Testing (SAST)</li> <li>Dependency Scanning</li> </ul> </li> </ul>"},{"location":"sdp/devsecops/#expectations","title":"Expectations","text":"<p>The following requirements will be enforced on all vendor projects running within JATIC GitLab CI/CD. Failure to pass the following test critera will result in failed CI builds, and the inability to progress forward in the JATIC DevSecOps pipeline.</p>"},{"location":"sdp/devsecops/#capabilities","title":"Capabilities","text":"<p>The project's test capabilities should abide by guidance in the Testing Plan section of the SDP and will be enforced via CI/CD.</p>"},{"location":"sdp/devsecops/#code-coverage","title":"Code Coverage","text":"<p>:star: Requirements: :star:</p> <ul> <li>Code coverage reported by pytest's code coverage report must be \u2265 90%.</li> </ul>"},{"location":"sdp/devsecops/#unit-tests","title":"Unit tests","text":"<p>Unit tests should be small and fast, therefore they should not interact with components that are slow or external to the application, such as databases. Calls to slow or external components should be mocked, stubbed, or avoided when writing unit tests.</p> <p>It is important to identify and test the unit's critical execution path, however it is usually impossible to test every corner case. It is recommended that a vendor use its coverage report to verify that the unit's critical execution paths have been thoroughly tested, and to identify areas of code that are not being tested.</p> <p>:star: Requirements: :star:</p> <ul> <li>Unit tests must be run in the CI/CD pipeline.</li> <li>pytest must be used to run tests of every function. </li> </ul> <p>Recommendations:</p> <ul> <li>Tests should cover inputs that span edges, beyond expected ranges, at limits, near limits, wrong types, too many inputs, across the span of potential issues, etc.</li> <li>Since unit tests are relatively inexpensive, we recommend writing many tests of inputs to ensure coverage of potential behavior of the code.</li> <li>Unit tests should be completed prior to and separately from integration tests.</li> </ul> <p>Code Example:</p> <pre><code># content of conftest.py\n\n# we define a fixture function below and it will be \"used\" by\n# referencing its name from tests\n\nimport pytest\n\n\n@pytest.fixture(scope=\"class\")\ndef db_class(request):\n    class DummyDB:\n        pass\n\n    # set a class attribute on the invoking test context\n    request.cls.db = DummyDB()\n</code></pre> <pre><code># content of test_unittest_db.py\n\nimport unittest\nimport pytest\n\n\n@pytest.mark.usefixtures(\"db_class\")\nclass MyTest(unittest.TestCase):\n    def test_method1(self):\n        assert hasattr(self, \"db\")\n        assert 0, self.db  # fail for demo purposes\n\n    def test_method2(self):\n        assert 0, self.db  # fail for demo purposes\n</code></pre> <p>Example reference</p> <p>Back to the top</p>"},{"location":"sdp/devsecops/#integration-tests","title":"Integration tests","text":"<p>Integration tests should typically focus on behavior over implementation: verifying the correctness of a function's outputs under diverse inputs is more important than using techniques like mocks to check how a function arrives at its results.</p> <p>Unlike unit tests, integration tests don't need to be small and fast. They may involve testing a component's integration with a database or external service. For such tests, a vendor should ensure that the database or external service is started and stopped automatically, either as part of the test code or in the tox configuration.</p> <p>:star: Requirements: :star:</p> <ul> <li>Integration tests must be written incrementally using a Top Down strategy.</li> <li>Integration tests must be run in the CI/CD pipeline.</li> </ul> <p>Recommendations:</p> <ul> <li>Business logic requirements/inconsistencies should be tested for prior to integration tests.</li> <li>Enable verbose test logging configurations to allow for maximum insight into integration testing failures between modules.</li> <li>Define a custom pytest marker to decorate integration tests.</li> <li>Integration tests should be completed prior to and separately from system tests.</li> </ul> <p>Code Example:</p> <pre><code># contents of pytest.ini\n\n[pytest]\nmarkers =\n    integration: marks tests as integration (deselect with '-m \"not integration\"')\n    serial\n</code></pre> <p>pytest markers can also be registered in a pytest_configure hook:</p> <pre><code>def pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\", \"integration: mark tests as integration\"\n    )\n</code></pre> <p>Example reference</p> <p>Back to the top</p>"},{"location":"sdp/devsecops/#regression-tests","title":"Regression tests","text":"<p>Considering the negative domino effect that can occur to a system whenever introducing new code (specifically when modifying core features), regression testing allows developers to pinpoint exactly where new bugs may have been introduced, helping to close the feedback loop inherent with agile software development. Test cases for new code should be written before the code can be considered complete.</p> <p>:star: Requirements: :star:</p> <ul> <li>All new code introduced into the codebase must be tested.</li> <li>All bugs introduced by new code should be reported and tracked within GitLab.</li> </ul> <p>Recommendations:</p> <ul> <li>Vendor teams should define a regression testing strategy that includes the most important test cases and frequency of testing.</li> <li>Previously executed test results and test plants should be available to be used as a \"known working state\" becnhmark for regression tests.</li> <li>Utilize pytest plugins such as regtest to compare output against previously executed tests, after new code has been introduced.</li> </ul> <p>Code Example:</p> <p>The pytest-regtest plugin provides a fixture named regtest which can be used as a file handle for recording data:</p> <pre><code>    def test_squares_up_to_ten(regtest):\n\n        result = [i*i for i in range(10)]\n\n        # one way to record output:\n        print(result, file=regtest)\n\n        # alternative method to record output:\n        regtest.write(\"done\")\n\n        # or using a context manager:\n        with regtest:\n            print(\"this will be recorded\")\n</code></pre> <p>If you run this test script with pytest the first time there is no recorded output for this test function so far and thus the test will fail with a message including a diff:</p> <pre><code>$ py.test\n...\n\nregression test output differences for test_demo.py::test_squares_up_to_ten:\n\n&gt;   --- current\n&gt;   +++ tobe\n&gt;   @@ -1,2 +1 @@\n&gt;   -[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n&gt;   -done\n&gt;   +\n</code></pre> <p>The output tells us what the current output is, and that the \"tobe\" output is still empty.</p> <p>For accepting this output, we run pytest with the --reset-regtest flag:</p> <pre><code>$ py.test --regtest-reset\n</code></pre> <p>Now the next execution of py.test will succeed and we can break the test by modifying the code under test to compute the first eleven square numbers:</p> <pre><code>    from __future__ import print_function\n\n    def test_squares_up_to_ten(regtest):\n\n        result = [i*i for i in range(11)]  # changed !\n\n        # one way to record output:\n        print(result, file=regtest)\n\n        # alternative method to record output:\n        regtest.write(\"done\")\n</code></pre> <p>The next run of pytest delivers a nice diff of the current and expected output from this test function:</p> <pre><code>$ py.test\n\n...\n&gt;   --- current\n&gt;   +++ tobe\n&gt;   @@ -1,2 +1,2 @@\n&gt;   -[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n&gt;   +[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n&gt;    done\n</code></pre> <p>Example reference</p> <p>Back to the top</p>"},{"location":"sdp/devsecops/#static-application-security-testing-sast","title":"Static Application Security Testing (SAST)","text":"<p>Static Application Security Testing is used to check source code for known security vulnerabilities and should be treated with equal, if not more importance than other types of software tests. GitLab offers built in SAST scanning functionality via GitLab CI/CD using the semgrep analyzer.</p> <p>:star: Requirements: :star:</p> <ul> <li>SAST scans must be run in the CI/CD pipeline.</li> <li>SAST scan results must contain no MEDIUM, HIGH, or CRITICAL vulnerabilities.</li> <li>False positives must be marked and dismissed in the project's vulnerability report, with proper justification provided in the \"description\" field.</li> </ul> <p>Recommendations:</p> <ul> <li>Integrate mitigating SAST scan results as part of development workflow.</li> <li>Check SAST scan results during development during every build to identify false positives early.</li> </ul> <p>Code Example:</p> <pre><code># Basic gitlab-ci.yml file including SAST scanning and report generation\n\nstages:\n- security-scan\n\ninclude: Jobs/SAST.gitlab-ci.yml\n\nsast:\n  stage: security-scan\n  artifacts:\n    paths: \n      - gl-sast-report.json\n    reports:\n      sast: gl-sast-report.json\n</code></pre> <p>Back to the top</p>"},{"location":"sdp/devsecops/#dependency-scanning","title":"Dependency Scanning","text":"<p>Dependency scanning analyzes project dependencies for knows security vulnerabilities. All dependencies utilized in a project are scanned, including transitive (nested) dependencies. GitLab offers built in dependency scanning functionality via GitLab CI/CD using the gemnasium-python analyzer, supporting Pip, Pipenv and Poetry.</p> <p>:star: Requirements: :star:</p> <ul> <li>Dependency scans must be run in the CI/CD pipeline.</li> <li>Dependency scan results must contain no MEDIUM, HIGH, or CRITICAL vulnerabilities.</li> </ul> <p>Recommendations:</p> <ul> <li>Address dependency vulnerabilities as soon as possible and plan for time to test dependency verisioning compatibility. Changing the version of a dependency to address a vulnerability can often break compatibility across reliant dependencies and this may not be noticable until runtime.</li> </ul> <p>Code Example:</p> <pre><code># Basic gitlab-ci.yml file including dependency scanning and report generation\n\nstages: security-scan\n\ninclude:\n- template: Jobs/Dependency-Scanning.gitlab-ci.yml\n\ndependency_scanning:\n  stage: security-scan\n  artifacts:\n    paths:\n    - gl-dependency-scanning-report.json\n    reports:\n      dependency_scanning:\n        - gl_dependency-scanning-report.json\n</code></pre> <p>Back to the top</p>"},{"location":"sdp/exception-handling/","title":"Exception Handling Guidelines","text":"<p>Software exceptions can and do occur in all types of software products. Often they are unexpected and if not handled properly, at best they are a nuisance to the user and at worst can cause catastrophic system failures and data loss. This guide provides recommendations for how to use and handle exceptions in Python for the JATIC project.</p> <p>In general, exceptions occur for exceptional reasons -- that is, unexpected behaviors that are not part of normal control flow of the software. Exception handling typically comes with a performance cost and should be kept out of the happy path. Additionally, readability is reduced with more indented code blocks and reasoning about how the code operates suffers when exceptions are used to skip through multiple call levels. Following are some examples of when to use and not use exceptions.</p>"},{"location":"sdp/exception-handling/#try-except-and-finally","title":"Try, Except, and Finally","text":"<p>The Python <code>try</code>, <code>except</code>, and <code>finally</code> blocks comprise the main mechanism for catching and handling exceptions. Use the <code>try</code> block to encapsulate code that may raise one or more types of exceptions. Use the <code>except</code> block to handle the exception. Use the <code>finally</code> block to execute required code regardless of whether an exception was raised or not (e.g. to close a file or respond to an API). See the Python documentation on errors and exceptions for details on syntax.</p>"},{"location":"sdp/exception-handling/#logging-exceptions","title":"Logging Exceptions","text":"<p>All unexpected exceptions should be logged using the applicable logging facility. (TBD Define the method of logging and link to it here.) Whether the program can continue to run normally or not, logging the exception is vital for debugging. Given a logging facility that provides log levels, exceptions should be logged with a <code>warning</code> level if execution can continue or an <code>error</code> level if it cannot.</p> <p>Repeated logging of the same exception should be avoided to prevent filling up log files. For example, the same exception might be raised for many images of a dataset. In this case, a flag might be used to log the exception only once. The logging facility may provide an mechanism to throttle message and, if so, should be configured to do so.</p>"},{"location":"sdp/exception-handling/#use-exceptions-for-unexpected-behavior","title":"Use Exceptions for Unexpected Behavior","text":"<p>Exceptions are considered atypical. Only use exceptions to convey when something unexpected happened. For example, when a method is not implemented or when a case is not supported, like so:</p> <pre><code>class MyClass:\n    def foo(self):\n        raise NotImplemented()\n\n    def provide_service(self, type):\n        if type == \"service 1\":\n            return self.service1()\n        elif type == \"service 2\":\n            return self.service2()\n        raise ServiceNotProvided()\n</code></pre>"},{"location":"sdp/exception-handling/#dont-use-exceptions-for-control-flow","title":"Don't Use Exceptions for Control Flow","text":"<p>Using exceptions in place of good control flow is discouraged. However, one allowable case where the use of an exception to affect flow control is the handling of keyboard interrupts or other asynchronous events that interrupt execution. For example:</p> <pre><code>def input_handler():\n    try:\n        while True:\n            wait_for_input()\n            process_input()\n    except KeyboardInterrupt:\n        print(\"Goodbye!\")\n</code></pre> <p>Sometimes deeply nested loops will raise an exception where <code>break</code> statements are ineffective. However, a better practice is to put the looping code in its own function and use a return statement. For example:</p> <pre><code># Bad:\ndef run_algorithm():\n    try:\n        for x in range(1000):\n            for y in range(1000):\n                for z in range(1000):\n                    if check(x, y, z):\n                        raise Exception()\n    except Exception:\n        print(\"found it!\")\n\n    # Do some more stuff...\n\n# Good:\ndef look_for_it():\n    for x in range(1000):\n        for y in range(1000):\n            for z in range(1000):\n                if check(x, y, z):\n                    return True\n    return False\n\ndef run_algorithm():\n    if look_for_it():\n        print(\"found it!\")\n\n    # Do some more stuff...\n</code></pre>"},{"location":"sdp/exception-handling/#return-status-from-functions-where-appropriate","title":"Return Status from Functions where Appropriate","text":"<p>In some cases, it might be helpful for the caller of a function to know if the call completed as planned, partially completed, or failed altogether. Or perhaps the called function is asynchronous and waiting on more data. In cases like these, returning status can be a better approach than raising an exception for performance and readability. Two questions to consider when determining the best approach would be: are multiple return statuses likely? And, can the caller handle the returned status and do something reasonable? If the caller can't or won't do anything with the status, it's not helpful to return it. If something bad or unrecoverable happens in the called function, then raising an exception (preferably a custom defined one as defined below) is preferred.</p>"},{"location":"sdp/exception-handling/#catch-specific-exceptions","title":"Catch Specific Exceptions","text":"<p>Never use an except block without an exception type. Doing so handles all exceptions -- not only those that may be expected but unexpected exceptions as well, potentially hiding errors in a misbehaving program. For example:</p> <pre><code># Bad:\ntry:\n    foo()\nexcept:\n    print(\"caught foo's exception\")\n\n# Good:\ntry:\n    bar()\nexcept SomeExpectedError:\n    print(\"this is the exception I'm looking for\")\n</code></pre>"},{"location":"sdp/exception-handling/#avoid-handling-exceptions-silently","title":"Avoid Handling Exceptions Silently","text":"<p>Handling an exception, but doing nothing with it is a code smell. If there really is no action to take, at a minimum use logging or some other output to indicate the exception occurred. In the rare event that even outputting a message is offensive, at least comment the code appropriately. For example:</p> <pre><code># Bad\ntry:\n    foo()\nexcept FooError:\n    pass\n\n# OK\ntry:\n    foo()\nexcept FooError:\n    # We can safely ignore this error because blah, blah, blah.\n    pass\n\n# Good\ntry:\n    foo()\nexcept FooError:\n    logging.info(\"Ignoring expected FooError\")\n</code></pre>"},{"location":"sdp/exception-handling/#dont-ignore-external-exceptions","title":"Don't Ignore External Exceptions","text":"<p>Review the calls you're making to system and third-party libraries to understand potential sources of exceptions. Handling them appropriately makes the application less likely to crash and hence more user friendly. For example, when using file I/O be certain to handle pertinent exceptions:</p> <pre><code>try:\n    with open(\"data.dat\", \"r\") as file:\n        process(file)\nexcept FileNotFoundError:\n    print(\"file not found\")\nexcept PermissionError:\n    print(\"permission denied\")\nexcept IOError:\n    print(\"unable to read from file\")\n</code></pre>"},{"location":"sdp/exception-handling/#top-level-exception-handling","title":"Top-level Exception Handling","text":"<p>Handling all unexpected exceptions at the top level can improve the user experience by providing meaningful information about and error rather than a cryptic stack trace, like so:</p> <pre><code>def main():\n    foo()\n\nif __name__ == \"__main__\":\n    try:\n        sys.exit(main())\n    except Exception as e:\n        if flags.dev:\n            raise e\n        print(f\"We're sorry, the following error occurred: {e}. Please contact your system administrator for help.\")\n        if flags.debug:\n            traceback.print_exc()\n</code></pre>"},{"location":"sdp/exception-handling/#handling-systemexit","title":"Handling SystemExit","text":"<p>Handling the <code>SystemExit</code> exception must be done with care. Typically <code>SystemExit</code> is handled to finish cleaning up before a program ends. Unit tests checking exit codes require that the exception be re-raised. However, to bypass the unit test altogether, use <code>os._exit()</code>. For example:</p> <pre><code># This code will be unit tested to check its exit code.\ntry:\n    sys.exit(main())\nexcept SystemExit as e:\n    if something_bad_happened:\n        os._exit(666)\n    else:\n        raise\n</code></pre>"},{"location":"sdp/exception-handling/#custom-exceptions","title":"Custom Exceptions","text":"<p>Use custom exception classes to raise and handle application specific errors. Use the built-in Python exception classes when the semantics make sense (e.g. <code>ValueError</code> is a good choice when a function or method receives an inappropriate argument value). Per PEP-8, derive exception classes from <code>Exception</code> and not <code>BaseException</code>. See PEP-8, \"Programming Recommendations\" for the rationale. For example:</p> <pre><code># Bad:\ndef foo(bar):\n    if not isinstance(bar, Bar):\n        raise Exception()\n\n# Good:\ndef foo(bar):\n    if not isinstance(bar, Bar):\n        raise ValueError()\n\n    if not bar.is_in_good_state():\n        raise BarError(\"Bar object not ready\")\n</code></pre> <p>See PEP-8, \"Exception Names\" for details on exception naming. In general, they should be names like classes with a suffix of <code>Error</code> for exceptions that are truly errors.</p>"},{"location":"sdp/open-source-strategy/","title":"Open-source Strategy","text":""},{"location":"sdp/open-source-strategy/#purpose","title":"Purpose","text":"<p>This document outlines a general policy towards public release, open-source, and open-collaboration for the JATIC program. It outlines the situations where open-source or public release of features are desirable, and the situations where they are not. </p> <p>Because of the numerous products within the program and their diverse development histories, this document does not provide a technical and detailed guide on the management of public and internal versions. Each product is expected to create a detailed release strategy for their product, which should detail management of public and internal features.</p> <p>More on details on release can be found within our Release Strategy. </p>"},{"location":"sdp/open-source-strategy/#definitions","title":"Definitions","text":"<p>For clarity, we define some related but distinct terms.</p> <ul> <li>We say that a project is publicly released when its software has been approved and released to the open internet.</li> <li>We say that a project is open-source when its software and its source code have been approved and released to the open internet with a license that allows any user to use, change, and re-distribute the source code without restriction. </li> <li>We say that a project is open-collaboration when its software and its source code have been approved and released to the open internet, and has a policy which allows any capable internet user to participate in its development. </li> </ul> <p>Note that a project can be public release without being open-source, for example, if it has a license that restricts commercial and military use. A project can also be open-source without being open-collaboration, if it does not, in practice, accept external contributions requests.</p>"},{"location":"sdp/open-source-strategy/#overall-objectives","title":"Overall objectives","text":"<p>The JATIC program aims to use public release, open-source, and open-collaboration to achieve the following objectives:</p> <ol> <li>get new features in front of as many users as quickly as possible</li> <li>allow products to be distributed as easily and as widely as possible</li> <li>maintain compatability and relevancy with other products</li> <li>reduce bugs by getting more eyes on the code</li> <li>increase trust and transparency of products</li> <li>increase adoption of open and common standards </li> <li>get feedback and contributions from the larger AI community</li> <li>establish positive DoD and CDAO image within the space</li> </ol> <p>On the other hand, there are situations where public release, open-source, and open-collaboration is not appropriate and may:</p> <ol> <li>reduce comparative national security advantage</li> <li>reduce industry competitive incentives</li> <li>provide potential for malicious contributions</li> </ol> <p>Case-by-case determinations of public release and open-source will attempt to strike a balance between these factors. Many products may need to maintain a publicly released version, as well as modules that are kept internal for DoD use. </p> <p>In general, we would like to publicly release, open-source, and allow open-collaboration on as much of what we develop within the program as possible. The most common instance where this is not possible will be when a feature is directly requested by a DoD organization and is uniquely relevant to their mission application. </p> <p>For example, if there are a set of adversarial techniques which have been developed within the DoD that they would like incorporated into a JATIC library, this may constitute a feature which cannot be publicly released. For cases like these, we would like to develop a project construct which allows us to still achieve the benefits of public release for the rest of the capability - for example, an extension mechanism which allows the main library to remain public - while allowing development on a non-public module which can extend the main library. The specific mechanisms for dealing with these situations will need to be determined case-by-case by the particular product teams.</p>"},{"location":"sdp/open-source-strategy/#choosing-a-license","title":"Choosing a license","text":"<p>For products that are publicly released, it is preferable that they use licenses which are conducive to the above objectives.</p> <p>If there is opportunity to choose a license for a new product (for instance, if the product began under the JATIC program, or previously has never been released externally), a discussion on the license should be held between the development team, the government, and the product owner.</p> <p>Permissive licenses such as Apache, MIT, or BSD are preferred. </p>"},{"location":"sdp/open-source-strategy/#public-release-open-source-process","title":"Public release / open-source process","text":"<p>There are provisions within many of the existing contracts for the developing companies to release code without any explicit approval from the government. Within these contracts, the government has the responsibility of explicitly denying the release of certain features if they would not like them to be publicly released. Within other contracts where the government has not given this approval to the developing companies, the code will be pushed through the government public release process.</p> <p>In both cases, it is the responsibility of the government and product owners to determine during increment planning if there are any particular features that will be developed during the increment which cannot be publicly released and open-sourced. For these features, a technical plan shall be created to ensure that these features can be developed in an extensible module, or that the branch with these features will otherwise still be able to pull from future developments in the upstream open-source branches.</p> <p>The government reserves the right to determine at any point if a feature cannot be publicly released or open-sourced. For instance, this may occur in the case where there are significant changes from the increment goals during execution. This should be a rare exception.</p>"},{"location":"sdp/open-source-strategy/#synchronization-with-upstream-open-source-branches","title":"Synchronization with upstream open-source branches","text":"<p>Many projects will have significant development external to the project's development within JATIC. This may be development by other parts of the company, or developments from the open internet.</p> <p>Please follow these two principles when synchronizing with other versions or branches.</p> <ul> <li>It is important to facilitate easy Synchronization with upstream developments, since this provides access to valuable developments from the larger community.</li> <li>Before accepting pull requests from the open internet, code should be tested for correct behavior and security.</li> </ul>"},{"location":"sdp/open-source-strategy/#project-specific-release-strategies","title":"Project-specific release strategies","text":"<p>This document does not provide specific guidance on the details, technical processes, or schedule to follow for public release or open-source. Each project must create a release strategy that documents the particular details for their project. This document must be approved by the program prior to any public releases of the project.</p> <p>At the very least, this document should discuss: - general strategy for public release, open-source, and open-collaboration - internal and public release cadence  - target platforms for public release, open-source, and open-collaboration - strategy for sychronization with upstream versions or branches - strategy for reviewing pull requests from internet - type of open-source license which the project will use</p> <p>Each project should probably also create a <code>CONTRIBUTING.md</code>, perhaps use git hooks to enforce style, set up security testing in CI, etc. Please briefly include these details in the release strategy.</p> <p>This release strategy should build upon the program's overall release strategy. It will likely look different between the various projects. For some, it may be a continuous deployment of the features into the public version, with the public and internal version looking identical with repo mirroring. For others, the public release may happen at a much slower release cadence.</p>"},{"location":"sdp/protocols/","title":"Common protocols, utilities, and tooling via jatic_toolbox","text":"<p>A Python package, <code>jatic_toolbox</code>, will be provided to vendors as a source of common types, protocols (a.k.a structural subtypes), utilities, and tooling to be leveraged by JATIC Python projects. It is designed to streamline the development of JATIC Python projects and to ensure that end-users enjoy seamless, synergistic workflows when composing multiple JATIC capabilities. These serve to eliminate redundancies that would otherwise be shared across \u2013 and burden \u2013\u00a0 the majority of JATIC projects. <code>jatic_toolbox</code>\u00a0 is designed to be a low-dependency, frequently-improved Python package that is installed by JATIC projects. The following is a brief overview of the current state of its submodules.</p>"},{"location":"sdp/protocols/#jatic_toolboxprotocols","title":"<code>jatic_toolbox.protocols</code>","text":"<p>Defines common types \u2013 such as an inference-mode object detector \u2013 factor to be leveraged across JATIC projects. These are specifically designed to be protocols, which support structural subtyping. As a result, developers and users can satisfy typed interfaces without having to explicitly subclass these custom types. These help to promote common interfaces across JATIC projects without introducing explicit inter-dependencies between them.</p>"},{"location":"sdp/protocols/#jatic_toolboxinterop","title":"<code>jatic_toolbox.interop</code>","text":"<p>Wrappers and functions that make JATIC protocols compatible with popular 3rd party libraries and frameworks. For example, this module can be used to wrap the object detectors provided by huggingface and timm so that they adhere to the JATIC protocols for object detectors.</p>"},{"location":"sdp/protocols/#jatic_toolboxutils","title":"<code>jatic_toolbox.utils</code>","text":"<p>Provides:</p> <ul> <li>Functions for validating the types and values of user arguments, with explicit and consistent user-error messages, that raise <code>jatic_toolbox</code>-customized exceptions.</li> <li>Specialized PyTorch utilities to help facilitate safe and ergonomic code patterns for manipulating stateful torch objects</li> <li>Other quality assurance and convenience functions that may be widely useful across projects</li> </ul>"},{"location":"sdp/protocols/#jatic_toolboxtesting","title":"<code>jatic_toolbox.testing</code>","text":"<p>Tools that help developers create a rigorous automated test suite for their JATIC project. These include:</p> <ul> <li>The quality assurance tests that the SDP stakeholders will be running as part of the JATIC-wide CI/CD pipeline, which can be run locally by project developers.</li> <li>pytest fixtures for initializing test functions with common models, datasets, and other inputs that are useful for testing machine learning code.</li> <li>Functions running static type checking tests using pyright in a pytest test suite, including scans of both source code and example documentation code blocks.</li> <li>Hypothesis strategies for driving property-based tests of interfaces that leverage JATIC protocols.</li> </ul>"},{"location":"sdp/software-requirements/","title":"Software Requirements","text":"<p>The following specifies standards and requirements for a Python project's: structure, code design and standards, code style, documentation style, test suite, and maintenance regimen.</p> <p>The rai-toolbox is an example of a project that adheres to these standards. Developers are encouraged to use this project as a reference in order to see the SDP's software requirements put into practice.</p>"},{"location":"sdp/software-requirements/#project-structure","title":"Project Structure","text":"<p>The following is a description of the basic structure for a Python project that adheres to the Python Packaging Authority's standards. This structure helps to ensure compatibility with 3rd party utilities and project analysis tools. This tutorial demonstrates the process for creating a project that adheres to this structure.</p> <ul> <li>Although not part of pypa's standard, we also suggest that modules and packages that are not part of a project's public API should have a leading underscore in its name, or it should reside in a directory whose name possesses the leading underscore.</li> <li>The project's Python source code should be isolated in a top-level directory named <code>src/</code>. This ensures that the Python package cannot be imported by the test suite unless the package has been installed locally.</li> <li>Project metadata should be specified in the <code>pyproject.toml</code> file or <code>setup.cfg</code> file when possible.</li> <li>A version string should be be accessible at runtime via the <code>__version__</code> attribute of the project's top-level Python module. See the section Versioning, Compatibility, and Maintenance Expectations for more details.</li> <li>Tools that have project-level configurations, such as <code>tox</code>, <code>coverage.py</code> and <code>isort</code>, should be configured using <code>pyproject.toml</code> when possible. Note that <code>flake8</code> and <code>pre-commit</code> do not support <code>pyproject.toml</code>. <code>flake518</code> is a small wrapper around <code>flake8</code> that provides support for <code>pyproject.toml</code>.</li> <li>The project's test suite should be in a <code>tests/</code> directory located at the top-level of the project.</li> <li>Public Python interfaces should include type annotations, and the project should include an empty <code>py.typed</code> file in its <code>src/</code> directory that is included as part of the project's package-data. This is to comply with PEP 561's specification for Python packages to distribute type information to users. <li>A project that provides users with custom types \u2013 that are predominantly meant for use as type annotations \u2013 should provide them to users through a submodule named <code>.typing</code>.</li> <p>The following is an example layout of such a project:</p> <pre><code># structure of a Python project named 'jatic_vision'\n\njatic_vision/\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 jatic_vision/\n\u2502       \u251c\u2500\u2500 py.typed\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 models.py\n\u2502       \u2514\u2500\u2500 _internals.py\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 conftest.py\n    \u2514\u2500\u2500 test_models.py\n</code></pre>"},{"location":"sdp/software-requirements/#dependency-management","title":"Dependency Management","text":"<p>A project's required dependencies should be kept to a minimum, in part, by isolating optional dependencies to appropriate sub-modules. For example, suppose that the project <code>my_lib</code> leverages <code>matplotlib</code> to provide visualization capabilities - it can make\u00a0<code>matplotlib</code> an optional dependency by restricting its usage to the <code>my_lib.viz</code> submodule, and by listing <code>matplotlib</code> under the <code>project.optional-dependencies</code> section of the project's <code>pyproject.toml</code> file. In this way, users can leverage <code>my_lib</code> without installing <code>matplotlib</code> unless they explicitly utilize features from <code>my_lib.viz</code>.</p> <p>A project's installation dependencies, both required and optional, must include minimum version numbers in the <code>pyproject.toml</code> file.</p>"},{"location":"sdp/software-requirements/#poetry","title":"Poetry","text":"<p>Poetry is required for internal development of software in the JATIC program. At the current moment of writing, poetry is the most complete dependency tool in the python ecosystem and has been used to find dependency problems already. </p> <p>Consumers of JATIC tools can use whatever dependency management their team thinks is best, but we strongly recommend Poetry. </p> <p>Teams should commit the lock file that is generated by poetry. CI and automation tools can install poetry and use it directly. If there is a need to transition slowly, poetry can generate a <code>requirements.txt</code> file, but ideally poetry should be used from development to test to production. </p> <p>The explanation for this requirement is as follows:</p> <p>Dependencies have dependencies known as transitive dependencies.  When a project is in use over a long enough time the minimum updates and complexity come from security updates.  In addition to security updates, packages may improve or be unmaintained so the package tree needs to change completely.  Ideally, this process would involve a high-quality dependency resolver which relies on semantic versioning (explained below) as well as tooling to make this process work well over time.  The python ecosystem has many tools that exist to manage dependencies but currently poetry solves this problem space very well.</p> <p>Dependencies need to work together and be repeatable.  If the project cannot run because something on the internet changed then the build is not repeatable.  Having many optional dependencies in groups causes the burden of permutations to move to the testing phase.  Eventually, all dependencies (whether optional or not) have to be combined together in a dependency tree.  This includes dependencies of dependencies.  It's too hard to solve by hand, poetry has a good resolver currently.</p> <p>When specifying what dependencies are needed, developers should try to be as loose with requirements as they can but then be strict when presented with a problem.  For example, let's suppose a project needs <code>numpy</code>.  In the <code>pyproject.toml</code> file the version would specified as loosely as possible <code>^1.24.2</code> instead of an exact <code>1.24.2</code>.  If a CVE for <code>numpy</code> required version 1.25.0 and above then the change would be made to use <code>^1.25.0</code> and only made <code>1.25.0</code> if other packages had issues resolving the dependency tree.</p> <p>Using poetry allows for repeatable builds and semantic versioning to be leveraged to make the software operate more smoothly over time.  Using pip or virtual environments does not solve the same sets of problems.</p>"},{"location":"sdp/software-requirements/#project-test-suite","title":"Project Test Suite","text":"<p>A JATIC Python project is to maintain an automated test suite. The primary distinction between a collection of manual tests versus an automated test suite is that, for the latter, a single command line call can collect, run, and report the results of all (or a subset of) a suite's tests. The project's automated test suite should leverage the following:</p> <ul> <li>The pytest framework for collecting, initializing, and running tests.</li> <li>The standard library's <code>unittest</code> module can be relied on for mocks, monkey patches, and doctests, all of which can be leveraged within a pytest-driven test suite. Otherwise, <code>unittest</code> should not be used as a test-runner and pytest-style test functions should be preferred over <code>unittest</code>-style test classes.</li> <li>The <code>nosetest</code> framework is no longer maintained and should not be used.</li> <li><code>tox</code> for automating the process of building isolated test environments, installing dependencies, and running tools (e.g. running tests in a Python 3.10 environment against the nightly build of PyTorch). <code>tox</code> helps normalize the process of running automated jobs across platforms (e.g. the same tox job-launch commands can be used on local machines as well as on CI/CD servers).</li> <li><code>tox</code> should be used to run a project's test suite across all supported Python versions.</li> <li>The <code>coverage.py</code> tool (with the pytest-cov plugin) to track the parts of the project's codebase that are and are not exercised by the test suite. Projects are free to specify their own code-coverage requirements (e.g. 85% code coverage), or to forego having a specific coverage requirement. That being said, the project's coverage metrics should be reported by a tox job.</li> <li>pyright is a fast type checker that performs incremental updates when files are modified.</li> </ul> <p>Teams are advised to read through this introductory tutorial to property based testing. Property-based testing is an effective approach to testing scientific software for which functions often cannot be tested against \"oracles\" (i.e. sources of known correct outputs for diverse inputs to functions).</p> <p>Explicit DevSecOps pipeline requirements that will be enforced via GitLab CI/CD can be found in the DevSecOps Testing Requirements document.</p>"},{"location":"sdp/software-requirements/#versioning-compatibility-and-maintenance-expectations","title":"Versioning, Compatibility, and Maintenance Expectations","text":"<p>A Python package's releases should utilize semantic versioning (i.e. the version number is structured as <code>MAJOR.MINOR.PATCH</code>).</p> <ul> <li>Projects like setuptools scm and versioneer can be used to extract a project's version string from version control metadata (e.g. a tag name from git). Using such a tool is recommended as it helps to eliminate from a project's release process the manual, error-prone steps of updating an embedded version string.</li> </ul> <p>A package's support for Python versions should be determined by the version support table provided by the Python developer's guide. JATIC projects should support (and test against) Python versions that are still receiving security updates. A project should add support for a new version of Python once it has been officially released and when the project's dependencies permit it. Dependencies that prevent compatibility with the full range of supported Python versions should be made optional if possible, or otherwise must be approved by JATIC project organizers.</p> <p>NumPy provides its own community policy standard, NEP 29, that calls for packages in the \"Scientific Python ecosystem\" to adopt a common time window-based approach to supporting Python and NumPy versions. This policy drops support for Python versions significantly earlier than the aforementioned timeline from the Python developer's guide. JATIC projects should abide by the more conservative version support schedule, but heed the fact that popular dependencies will adopt the NEP 29 timeline. As such, JATIC packages will need to maintain a sufficiently expansive test-job matrix to ensure that the minimum/maximum documented dependency versions are being tested, and that new project features do not inadvertently break compatibility.</p> <p>The project's tox jobs should exercise the minimum and maximum documented dependencies, and should include jobs that exclude optional requirements to ensure that they are not required for installation nor for exercising core functionality.</p>"},{"location":"sdp/software-requirements/#code-design-standards-and-guidelines","title":"Code Design, Standards, and Guidelines","text":"<p>Python code should comply with the JATIC Python Coding Guidelines. These should be followed for new code. Existing code can be permitted to ignore guidelines until time is allocated to revise and update the code.</p>"},{"location":"sdp/software-requirements/#code-style-and-formatting","title":"Code Style and Formatting","text":"<p>Adhering to a clear and consistent style is critical for writing maintainable code, especially when coordinating multi-organization collaborations. That being said, enforcing such standards can be tedious and labor-intensive. Fortunately, several powerful tools exist that can help automate consistent code styling. Compliance with the Python Coding Guidelines should help create consistent style. Additional style and formatting consistency should come by application of formatting tools.</p> <p>JATIC Python projects should leverage the following automated tools for formatting code and for enforcing style:</p> <ul> <li>black: is a PEP 8 compliant opinionated code formatter. It is designed to be run on a project's source code and tests in order to modify these text files in-place so that the resulting files have canonicalized formatting. black is limited in the ways that it can be configured; this is by-design so that teams do not spend time squabbling over things like white-space conventions. Note that black can be run on Jupyter notebooks as well.</li> <li>isort: sorts imports in alphabetical order and into sections (e.g. std-lib imports, third-party imports, and first-party imports). Vendors should configure isort to be compatible with black according to these instructions.</li> <li>flake8: analyzes your code to enforce the PEP8 standards and to catch bad code patterns, such as unused imports and variables. This is not an auto-formatter, rather when it is run from the command line it will generate a report of code style violations in a code base, which includes their locations and error codes. Vendors can configure flake8 to skip particular files (e.g. one may want to avoid scanning a <code>.py</code> file that was auto-generated by the Sphinx documentation framework).</li> </ul> <p>At its most basic level, our code style standards for Python-based projects are derived from the PEP 8 Style Guide, with additional formatting specifications that are best summarized as \"let auto-formatters do their thing\". It is recommended that these three tools be applied in an automated fashion. Pre-commit hooks make it simple to run these tools on a per-commit basis, and CI/CD processes should be designed to apply/enforce these tools before a branch can be merged into protected branches (e.g., <code>dev</code>, <code>main</code>). Here is an example of a pre-commit configuration file (and an accompanying isort config) that runs these three tools.</p>"},{"location":"sdp/software-requirements/#documentation","title":"Documentation","text":"<p>JATIC projects should utilize consistent conventions and styles in their documentation strings, documentation websites, and in the organization of tutorials, how-to guides, and other instructional materials. Guidance on documentation within code can be found in Python Coding Guidelines</p>"},{"location":"sdp/software-requirements/#project-documentation","title":"Project Documentation","text":"<p>Project documentation should leverage the Sphinx project to generate hierarchical documentation in HTML (or LaTeX for a printable version). Using the reStructuredText markup language for the source content recommended over markdown. The PyData Sphinx Theme is recommended for providing, e.g., the css styling, fonts for the HTML pages; it is popular, actively supported, and includes rich features like code-tabs and a dark-theme.</p> <p>Documentation should follow the guidance in Python Coding Guidelines.</p>"},{"location":"sdp/software-requirements/#documentation-organization","title":"Documentation Organization","text":"<p>It is recommended that projects leverage the Di\u00e1taxis framework for technical documentation authoring. This prescribes four \"modes\" of documentation \u2013 tutorials, how-to guides, explanations, and technical reference \u2013 for introducing users to a project. This can help to create a consistent learning experience for users who are leveraging multiple JATIC projects, and it helps to save JATIC vendors from \"reinventing the wheel\" when it comes to writing project documentation. A \"change log\" should also be maintained, detailing the release notes of the project's respective versions, as part of the documentation site.</p>"},{"location":"sdp/testing-plan/","title":"Testing Plan","text":""},{"location":"sdp/testing-plan/#expectations","title":"Expectations","text":"<p>As described in the Software Requirements section of the SDP, a vendor is expected to develop and maintain an automated test suite using the <code>pytest</code> framework. The test suite should be able to be run in a Python environment that has installed the vendor's package via the command <code>pytest tests/</code>. Additionally, a\u00a0<code>tox</code> configuration is to be maintained by the vendor, that supports the following:</p> <ul> <li>Running the tests that involve all of the required installation dependencies against each supported version of Python.</li> <li>Running the test suite in an environment that excludes all optional dependencies (i.e., confirming that optional dependencies are indeed optional).</li> <li>Measuring the test suite's code coverage.</li> <li>Running the tests that involve all of the package's optional installation dependencies.</li> <li>Running tests against the minimum dependency versions that the package is documented to support.</li> <li>Running tests against either the dependency versions that the package is documented to support, or, if no such version upper-bound is specified, against the nightly/master builds of the dependencies.</li> </ul> <p>The organizers of the SDP will run vendor-submitted code through tests that include:</p> <ul> <li>Verification that the project's structure adheres to the software requirements. This includes checks on the project's metadata and documentation as well.</li> <li>Verification that the project's code adheres to the style guidelines, as implemented by the automated tools that were specified by the software requirements.</li> <li>Exercising the project's test suite via its <code>tox</code> jobs.</li> <li>Running a spell checker on the project's documentation.</li> <li>Running security scans.</li> </ul>"},{"location":"sdp/testing-plan/#capabilities","title":"Capabilities","text":"<p>The project's test capabilities should abide by the following.</p>"},{"location":"sdp/testing-plan/#unit-tests","title":"Unit tests","text":"<p>A unit is the smallest piece of code to be tested in a test suite, and it is typically considered to be a single non-trivial function. Unit tests always use white-box testing techniques, where the inner workings of the unit being tested are known to the tester.</p>"},{"location":"sdp/testing-plan/#integration-tests","title":"Integration tests","text":"<p>Integration tests utilize a combination of white-box and black-box techniques to test a group of individual units as a single component. It is often simply testing the implementation of a single public interface. A project's test suite should prioritize integration tests of its public APIs over unit tests of internal functions.</p>"},{"location":"sdp/testing-plan/#regression-tests","title":"Regression tests","text":"<p>The purpose of regression testing is to ensure that modifications in software have not caused unintended adverse side effects. Regression testing should be conducted after any change to the codebase to prevent the introduction of new bugs. </p>"},{"location":"sdp/testing-plan/#system-tests","title":"System tests","text":"<p>A project's test suite should include a small number of system tests whereby the entire application is tested through an end-to-end process. This type of testing is very slow, so it should be kept to a bare minimum. System testing is limited to black-box techniques, where the tester has no knowledge of the underlying code. These tests may test the usability, scalability, compatibility, or performance of the application.</p>"},{"location":"sdp/testing-plan/#property-based-tests","title":"Property-based tests","text":"<p>Testing scientific software, such as neural networks, can be challenging as there is often no \"oracle\" \u2013 a source that will tell us what the expected output of a function will be \u2013 against which to test. For example, given an input to a neural network, it may not be possible to predict precisely what the network's outputs should be. In these cases it is useful to identify general properties of the function, e.g., a neural network is typically equivariant under batch-ordering, that can be tested under highly diverse inputs. Vendors are encouraged to utilize property-based testing in their test suites both in the context of unit tests and integration tests.</p>"},{"location":"sdp/testing-plan/#static-type-checking","title":"Static type checking","text":"<p>pyright is the static type checker that VSCode's Python extension is built off of. The <code>jatic_toolbox</code>\u00a0 provides a Python API for running pyright on both Python code and documentation that contains Python code examples. A vendor should leverage these capabilities to include in their test suite:</p> <ul> <li>Validate the type annotations of the project's public APIs, demonstrating that type checkers correctly flag \"true positives\" wherein users specify inputs of incorrect types, and that pyright returns a \"clean scan\" for correct usages of the interfaces.</li> <li>It is recommended that vendors include a pyright scan of the entirety of their internal code base, but this is not required.</li> <li>Runs pyright on the following parts of a project to verify that they do not contain unexpected errors:</li> <li>The <code>Examples</code> sections of the docstrings for all publicly-facing Python interfaces.</li> <li>All documentation files (i.e., restructured text files), to validate their Python code blocks.</li> <li>Jupyter notebooks that are included in the project (e.g., as demos).</li> </ul>"},{"location":"sdp/testing-plan/#test-results-reporting","title":"Test results reporting","text":"<p>A project's tox configuration should include jobs that can be run to report the following:</p> <ul> <li>The code coverage of the pytest test suite, via coverage.py. Refer to the Software Requirements for more details.</li> <li>Code coverage reported by pytest's code coverage report must be \u2265 90% explicitly mark code blocks that they do not intend to cover with their tests, and eventually enforce 100% code coverage once the project is sufficiently mature.</li> <li>A pyright type-completeness score, using the  <code>--ignoreexternal</code> flag. This should report 100% type completeness for the project's public API.</li> <li>A bandit security scan report.</li> </ul>"}]}